{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2375a36c-99ae-4852-91f3-8dbec09b1f36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 03:07:24.106469: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 03:07:24.817039: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cublas/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/:/usr/local/cudnn-112-81133/lib64:/usr/local/cuda-11.2/lib64:/lib/x86_64-linux-gnu:/opt/anaconda-3-2020.02/lib\n",
      "2023-09-19 03:07:24.817111: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cublas/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/:/usr/local/cudnn-112-81133/lib64:/usr/local/cuda-11.2/lib64:/lib/x86_64-linux-gnu:/opt/anaconda-3-2020.02/lib\n",
      "2023-09-19 03:07:24.817117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f63e18-7e08-4451-a21f-594b7f75f7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:06:32.675980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 16:06:33.445800: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cublas/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/:/usr/local/cudnn-112-81133/lib64:/usr/local/cuda-11.2/lib64:/lib/x86_64-linux-gnu:/opt/anaconda-3-2020.02/lib\n",
      "2023-09-22 16:06:33.445863: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cuda_nvrtc/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cublas/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/nvidia/cudnn/lib:/opt/anaconda-3-2020.02/envs/hskim/lib/:/usr/local/cudnn-112-81133/lib64:/usr/local/cuda-11.2/lib64:/lib/x86_64-linux-gnu:/opt/anaconda-3-2020.02/lib\n",
      "2023-09-22 16:06:33.445869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-09-22 16:06:34.362883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 16:06:34.903467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22325 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:c2:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GPU = 3\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= f\"{GPU}\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5539747e-7bde-4cd5-b26c-08cdf8d23ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:06:35.477667\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./.local/lib/python3.8/site-packages')\n",
    "sys.path.append('/usr/lib/python3/dist-packages')\n",
    "\n",
    "from keras import losses, metrics\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Dropout, Activation, Input\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D, concatenate, Add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats, interp\n",
    "import os, sys, pickle, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, datetime, time\n",
    "from sklearn.model_selection import KFold\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "\n",
    "# tensorflow 사용 시 seed 고정\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "SEED = 98\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada78df-cfa1-42dd-970d-94cded4648b2",
   "metadata": {},
   "source": [
    "# Input loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098091b-b9b8-4845-9cbc-b00c52e4f421",
   "metadata": {},
   "source": [
    "## signals only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8cae03-8acf-4aa2-828a-c72efb0f5f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (7022, 5000, 12)\n",
      "x test shape: (1756, 5000, 12)\n"
     ]
    }
   ],
   "source": [
    "SRATE = 500\n",
    "ECG_FILT = 'bandpass'\n",
    "TRAIN = 'train'\n",
    "ADULT = 'child'\n",
    "NORM = 'z-norm'\n",
    "GENDER = 'male'\n",
    "\n",
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{TRAIN}'\n",
    "input_path = f'dataset/{hyper_path}/'\n",
    "\n",
    "with np.load(input_path+'x.npz', allow_pickle=True) as f:\n",
    "    x_train = f['x_train']\n",
    "    x_test = f['x_test']\n",
    "        \n",
    "with np.load(input_path+'y.npz', allow_pickle=True) as f:\n",
    "    y_train = f['y_train']\n",
    "    y_test = f['y_test']\n",
    "    \n",
    "with np.load(input_path+'c.npz', allow_pickle=True) as f:\n",
    "    c_train = f['c_train']\n",
    "    c_test = f['c_test']  \n",
    "    \n",
    "with np.load(input_path+'g.npz', allow_pickle=True) as f:\n",
    "    g_train = f['g_train']\n",
    "    g_test = f['g_test']        \n",
    "    \n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5b50cf-87ef-488e-8665-38cb60599097",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_Y = 20\n",
    "y_train = y_train / SCALE_Y\n",
    "y_test = y_test / SCALE_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b642ace-b1dc-4ed8-b0be-7b651a6bc384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (3891, 5000, 12)\n",
      "x test shape: (963, 5000, 12)\n"
     ]
    }
   ],
   "source": [
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{GENDER}_{TRAIN}'\n",
    "\n",
    "train_mask = (g_train == 1) if GENDER == 'male' else (g_train == 0)\n",
    "test_mask = (g_test == 1) if GENDER == 'male' else (g_test == 0)\n",
    "\n",
    "x_train = x_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "c_train = c_train[train_mask]\n",
    "g_train = g_train[train_mask]\n",
    "\n",
    "x_test = x_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "c_test = c_test[test_mask]\n",
    "g_test = g_test[test_mask]\n",
    "\n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5cac09-b6df-49b3-9e47-cf45c7ed6756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.load(input_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "y_train = np.load(input_path+'y_train.npz')['arr_0']\n",
    "y_test = np.load(input_path+'y_test.npz')['arr_0']\n",
    "c_train = np.load(input_path+'c_train.npz')['arr_0']\n",
    "c_test = np.load(input_path+'c_test.npz')['arr_0']\n",
    "\n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbfbc0-4cce-4fb6-bb51-e40f2658c553",
   "metadata": {},
   "source": [
    "## signal+features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d690a99-a490-4ac6-a6a9-27edcde6274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (7021, 5000, 12)\n",
      "feat train shape: (7021, 55)\n",
      "x test shape: (1756, 5000, 12)\n",
      "feat test shape: (1756, 55)\n",
      "x train shape: (3890, 5000, 12)\n",
      "x test shape: (963, 5000, 12)\n",
      "train: 0.10179626941680908, test: 0.10746798664331436\n"
     ]
    }
   ],
   "source": [
    "SRATE = 500\n",
    "ECG_FILT = 'bandpass'\n",
    "TRAIN = 'train'\n",
    "ADULT = 'child'\n",
    "NORM = 'z-norm'\n",
    "GENDER = 'male'\n",
    "\n",
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{TRAIN}'\n",
    "input_path = f'dataset/{hyper_path}/signal+features/'\n",
    "\n",
    "with np.load(input_path+'x.npz', allow_pickle=True) as f:\n",
    "    x_train = f['x_train']\n",
    "    x_test = f['x_test']\n",
    "     \n",
    "with np.load(input_path+'feats.npz', allow_pickle=True) as f:\n",
    "    feat_train = f['feat_train']\n",
    "    feat_test = f['feat_test']\n",
    "        \n",
    "with np.load(input_path+'y.npz', allow_pickle=True) as f:\n",
    "    y_train = f['y_train']\n",
    "    y_test = f['y_test']\n",
    "    \n",
    "with np.load(input_path+'c.npz', allow_pickle=True) as f:\n",
    "    c_train = f['c_train']\n",
    "    c_test = f['c_test']  \n",
    "    \n",
    "with np.load(input_path+'g.npz', allow_pickle=True) as f:\n",
    "    g_train = f['g_train']\n",
    "    g_test = f['g_test']        \n",
    "    \n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'feat train shape: {feat_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')\n",
    "print(f'feat test shape: {feat_test.shape}')\n",
    "\n",
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{GENDER}_{TRAIN}'\n",
    "\n",
    "train_mask = (g_train == 1) if GENDER == 'male' else (g_train == 0)\n",
    "test_mask = (g_test == 1) if GENDER == 'male' else (g_test == 0)\n",
    "\n",
    "x_train = x_train[train_mask]\n",
    "feat_train = feat_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "c_train = c_train[train_mask]\n",
    "g_train = g_train[train_mask]\n",
    "\n",
    "x_test = x_test[test_mask]\n",
    "feat_test = feat_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "c_test = c_test[test_mask]\n",
    "g_test = g_test[test_mask]\n",
    "\n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')\n",
    "\n",
    "\n",
    "SCALE_Y = 100 if ADULT == 'adult' else 20\n",
    "y_train = y_train / SCALE_Y\n",
    "y_test = y_test / SCALE_Y\n",
    "\n",
    "print(f'train: {np.mean(y_train)}, test: {np.mean(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11cc08-5bca-4f96-b431-ca653dabb97a",
   "metadata": {},
   "source": [
    "## signal+features+aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84dfb294-79f8-400b-8146-9bac68adaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class lead_extractor:\n",
    "    \"\"\"\n",
    "    used to select specific leads or random choice of configurations\n",
    "\n",
    "    Twelve leads: I, II, III, V1, V2, V3, V4, V5, V6, aVR, aVL, aVF, \n",
    "    Six leads: I, II, V1, V3, V4, V6\n",
    "    Four leads: III, V1, V2, aVL # I, II, III, V2\n",
    "    Three leads: I, II, V2\n",
    "    Two leads: I, II\n",
    "\n",
    "    \"\"\"\n",
    "    L2 = np.array([1,1,0,0,0,0,0,0,0,0,0,0])\n",
    "    L3 = np.array([1,1,0,0,0,0,0,1,0,0,0,0])\n",
    "    L4 = np.array([0,0,1,1,1,0,0,0,0,0,1,0]) #([1,1,1,0,0,0,0,1,0,0,0,0])\n",
    "    L6 = np.array([1,1,0,1,0,1,1,0,1,0,0,0])\n",
    "    L8 = np.array([1,1,0,1,0,1,1,1,1,0,1,0])\n",
    "    L12 = np.array([1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "    @staticmethod\n",
    "    def get (x,num_leads):\n",
    "        if num_leads==None:\n",
    "            # random choice output\n",
    "            num_leads = random.choice([12,8,6,4,3,2])\n",
    "\n",
    "        if num_leads==12:\n",
    "            # Twelve leads: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6\n",
    "            return x\n",
    "\n",
    "        if num_leads==8:\n",
    "            # Six leads: I, II, III, aVL, aVR, aVF\n",
    "            x = x * lead_extractor.L8\n",
    "            return x\n",
    "\n",
    "        if num_leads==6:\n",
    "            # Six leads: I, II, III, aVL, aVR, aVF\n",
    "            x = x * lead_extractor.L6\n",
    "            return x\n",
    "\n",
    "        if num_leads==4:\n",
    "            # Six leads: I, II, III, V2\n",
    "            x = x * lead_extractor.L4\n",
    "            return x\n",
    "\n",
    "        if num_leads==3:\n",
    "            # Three leads: I, II, V2\n",
    "            x = x * lead_extractor.L3\n",
    "            return x\n",
    "\n",
    "        if num_leads==2:\n",
    "            # Two leads: II, V5\n",
    "            x = x * lead_extractor.L2\n",
    "            return x\n",
    "        raise Exception(\"invalid-leads-number\")\n",
    "\n",
    "def aug_generator(n_aug, x_train, feat_train, y_train, c_train, g_train):\n",
    "    from tqdm import tqdm\n",
    "    x_train_, feat_train_, y_train_, c_train_, g_train_ = [], [], [], [], []\n",
    "    for i in tqdm(range(x_train.shape[0])):\n",
    "        x_train_.append(x_train[i])\n",
    "        feat_train_.append(feat_train[i])\n",
    "        y_train_.append(y_train[i])\n",
    "        c_train_.append(c_train[i])\n",
    "        g_train_.append(g_train[i])\n",
    "\n",
    "        x_train_.append(lead_extractor.get(x_train[i], 8))\n",
    "        feat_train_.append(feat_train[i])\n",
    "        y_train_.append(y_train[i])\n",
    "        c_train_.append(c_train[i])\n",
    "        g_train_.append(g_train[i])    \n",
    "\n",
    "        if n_aug > 2:\n",
    "            x_train_.append(lead_extractor.get(x_train[i], 6))\n",
    "            feat_train_.append(feat_train[i])\n",
    "            y_train_.append(y_train[i])\n",
    "            c_train_.append(c_train[i])\n",
    "            g_train_.append(g_train[i])  \n",
    "\n",
    "        if n_aug > 3:\n",
    "            x_train_.append(lead_extractor.get(x_train[i], 4))\n",
    "            feat_train_.append(feat_train[i])\n",
    "            y_train_.append(y_train[i])\n",
    "            c_train_.append(c_train[i])\n",
    "            g_train_.append(g_train[i]) \n",
    "\n",
    "    x_train_ = np.array(x_train_, np.float32)\n",
    "    feat_train_ = np.array(feat_train_, np.float32)\n",
    "    y_train_ = np.array(y_train_, np.float32)\n",
    "    c_train_ = np.array(c_train_)\n",
    "    g_train_ = np.array(g_train_, np.float32)\n",
    "\n",
    "    print(f'x shape: {x_train_.shape}')\n",
    "    print(f'feat shape: {feat_train_.shape}')\n",
    "    print(f'y shape: {y_train_.shape}')\n",
    "    \n",
    "    return x_train_, feat_train_, y_train_, c_train_, g_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf8475f-c4f0-4ec0-a4ca-f6593afe9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (27824, 5000, 12)\n",
      "feat train shape: (27824, 55)\n",
      "x test shape: (6962, 5000, 12)\n",
      "feat test shape: (6962, 55)\n",
      "x train shape: (13358, 5000, 12)\n",
      "y train shape: (13358,)\n",
      "x test shape: (3387, 5000, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13358/13358 [00:03<00:00, 3800.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (26716, 5000, 12)\n",
      "feat shape: (26716, 55)\n",
      "y shape: (26716,)\n",
      "train: 0.6191248893737793, test: 0.6184705495834351\n"
     ]
    }
   ],
   "source": [
    "SRATE = 500\n",
    "ECG_FILT = 'bandpass'\n",
    "TRAIN = 'train'\n",
    "ADULT = 'adult'\n",
    "NORM = 'z-norm'\n",
    "GENDER = 'male'\n",
    "\n",
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{TRAIN}'\n",
    "input_path = f'dataset/{hyper_path}/signal+features/'\n",
    "\n",
    "with np.load(input_path+'x.npz', allow_pickle=True) as f:\n",
    "    x_train = f['x_train']\n",
    "    x_test = f['x_test']\n",
    "     \n",
    "with np.load(input_path+'feats.npz', allow_pickle=True) as f:\n",
    "    feat_train = f['feat_train']\n",
    "    feat_test = f['feat_test']\n",
    "        \n",
    "with np.load(input_path+'y.npz', allow_pickle=True) as f:\n",
    "    y_train = f['y_train']\n",
    "    y_test = f['y_test']\n",
    "    \n",
    "with np.load(input_path+'c.npz', allow_pickle=True) as f:\n",
    "    c_train = f['c_train']\n",
    "    c_test = f['c_test']  \n",
    "    \n",
    "with np.load(input_path+'g.npz', allow_pickle=True) as f:\n",
    "    g_train = f['g_train']\n",
    "    g_test = f['g_test']        \n",
    "    \n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'feat train shape: {feat_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')\n",
    "print(f'feat test shape: {feat_test.shape}')\n",
    "\n",
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{GENDER}_{TRAIN}'\n",
    "\n",
    "train_mask = (g_train == 1) if GENDER == 'male' else (g_train == 0)\n",
    "test_mask = (g_test == 1) if GENDER == 'male' else (g_test == 0)\n",
    "\n",
    "x_train = x_train[train_mask]\n",
    "feat_train = feat_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "c_train = c_train[train_mask]\n",
    "g_train = g_train[train_mask]\n",
    "\n",
    "x_test = x_test[test_mask]\n",
    "feat_test = feat_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "c_test = c_test[test_mask]\n",
    "g_test = g_test[test_mask]\n",
    "\n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'y train shape: {y_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')\n",
    "\n",
    "x_train, feat_train, y_train, c_train, g_train = aug_generator(2, x_train, feat_train, y_train, c_train, g_train)\n",
    "#x_test, feat_test, y_test, c_test, g_test = aug_generator(x_test, feat_test, y_test, c_test, g_test)\n",
    "\n",
    "SCALE_Y = 100 if ADULT == 'adult' else 20\n",
    "y_train = y_train / SCALE_Y\n",
    "y_test = y_test / SCALE_Y\n",
    "\n",
    "print(f'train: {np.mean(y_train)}, test: {np.mean(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b46f07-0d89-462f-abdb-3dcfdb0a8779",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91c1c53-fb8c-46ad-b077-3902581121f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n",
      "2023-09-21 11:07:17.735937\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 4  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"randomSearch/{hyper_path}/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_{nfold}fold_test{ntest}\"\n",
    "\n",
    "if not os.path.exists(f\"randomSearch/{hyper_path}\"):\n",
    "    os.mkdir(f\"randomSearch/{hyper_path}\")\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: 1D CNN 4 layers, regression')\n",
    "f.write(f'input: ECG of 10 second, output: age')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "layer_settings, test_settings = [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "globalpool_opts = ['max','ave']\n",
    "\n",
    "# hyperparamters pool\n",
    "filt_opts = [16, 32, 64, 128] # num of filters(kernel)\n",
    "stride_opts = [1,2,4]  # other opts: stride = (kernel-1)/2\n",
    "kernel_opts = range(7,20,2) # kernel size\n",
    "pool_size = 2\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [0, 16, 32, 64, 128]\n",
    "BATCH_SIZE = [32, 64, 128, 256, 512]\n",
    "lr_opts = [0.001, 0.002, 0.0005]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "nfilt, kernels, strides = [], [], []\n",
    "for i in range(5):\n",
    "    nfilt.append(0)\n",
    "    kernels.append(0)\n",
    "    strides.append(0)\n",
    "\n",
    "for nfilter in filt_opts:\n",
    "    for kernel in kernel_opts:\n",
    "        for stride in stride_opts:\n",
    "        #layer_settings.append([nfilter, kernel, int((kernel-1)/2)])       \n",
    "            layer_settings.append([nfilter, kernel, stride])\n",
    "    \n",
    "for dense_node in dense_opts:\n",
    "    for dropout_cnn in dropout_opts:\n",
    "        for dropout_fc in dropout_opts:\n",
    "            for batch_size in BATCH_SIZE:\n",
    "                for learning_rate in lr_opts:\n",
    "                    test_settings.append([dense_node, dropout_cnn, dropout_fc, batch_size, learning_rate])                                   \n",
    "\n",
    "                        \n",
    "print('done')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65166b0-a99d-4065-ae82-44ffce10f2c3",
   "metadata": {},
   "source": [
    "## 1d-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc66dd-af29-42b2-b7cf-535f6d80f375",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 11:09:19.136882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 11:09:19.621664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22324 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 11:09:22.853037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8904\n",
      "2023-09-21 11:09:23.313032: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-21 11:09:23.725512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-21 11:09:23.727835: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fccac0515f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-21 11:09:23.727882: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-09-21 11:09:23.736192: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-21 11:09:23.807048: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-21 11:09:23.865337: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/92 [============================>.] - ETA: 0s - loss: 0.1618 - mean_squared_error: 0.0618\n",
      "Epoch 1: val_loss improved from inf to 0.16156, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c128filt13str2,layer2:c16filt9str4,layer3:c64filt9str1,1conv,dropout0.1,dnodes64,dropout0/weights_0.hdf5\n",
      "92/92 [==============================] - 6s 16ms/step - loss: 0.1618 - mean_squared_error: 0.0618 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 2/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1612 - mean_squared_error: 0.0616\n",
      "Epoch 2: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 3/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0612\n",
      "Epoch 3: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 4/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0612\n",
      "Epoch 4: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1600 - mean_squared_error: 0.0610\n",
      "Epoch 1: val_loss improved from inf to 0.16526, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c128filt13str2,layer2:c16filt9str4,layer3:c64filt9str1,1conv,dropout0.1,dnodes64,dropout0/weights_1.hdf5\n",
      "92/92 [==============================] - 4s 21ms/step - loss: 0.1608 - mean_squared_error: 0.0613 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 2/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1592 - mean_squared_error: 0.0607\n",
      "Epoch 2: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 3/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1596 - mean_squared_error: 0.0609\n",
      "Epoch 3: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1593 - mean_squared_error: 0.0608\n",
      "Epoch 4: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1641 - mean_squared_error: 0.0630\n",
      "Epoch 1: val_loss improved from inf to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c128filt13str2,layer2:c16filt9str4,layer3:c64filt9str1,1conv,dropout0.1,dnodes64,dropout0/weights_2.hdf5\n",
      "92/92 [==============================] - 3s 15ms/step - loss: 0.1641 - mean_squared_error: 0.0629 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 2/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1629 - mean_squared_error: 0.0624\n",
      "Epoch 2: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 1s 11ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1555 - val_mean_squared_error: 0.0601\n",
      "Epoch 3/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1639 - mean_squared_error: 0.0634\n",
      "Epoch 3: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1643 - mean_squared_error: 0.0636 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 4/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1628 - mean_squared_error: 0.0624\n",
      "Epoch 4: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1617 - mean_squared_error: 0.0621\n",
      "Epoch 1: val_loss improved from inf to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c128filt13str2,layer2:c16filt9str4,layer3:c64filt9str1,1conv,dropout0.1,dnodes64,dropout0/weights_3.hdf5\n",
      "92/92 [==============================] - 3s 15ms/step - loss: 0.1615 - mean_squared_error: 0.0620 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0615\n",
      "Epoch 2: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 3/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0617\n",
      "Epoch 3: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1604 - mean_squared_error: 0.0614\n",
      "Epoch 4: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.17, val rmse 0.26###\n",
      "mae1.71+-0.00_rmse2.58+-0.00\n",
      "random search 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1579 - mean_squared_error: 0.0585\n",
      "Epoch 1: val_loss improved from inf to 0.18268, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 2s 36ms/step - loss: 0.1579 - mean_squared_error: 0.0585 - val_loss: 0.1827 - val_mean_squared_error: 0.0722\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1446 - mean_squared_error: 0.0510\n",
      "Epoch 2: val_loss improved from 0.18268 to 0.17449, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.1423 - mean_squared_error: 0.0497 - val_loss: 0.1745 - val_mean_squared_error: 0.0513\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1182 - mean_squared_error: 0.0319\n",
      "Epoch 3: val_loss improved from 0.17449 to 0.14829, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.1165 - mean_squared_error: 0.0311 - val_loss: 0.1483 - val_mean_squared_error: 0.0381\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0965 - mean_squared_error: 0.0228\n",
      "Epoch 4: val_loss did not improve from 0.14829\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0964 - mean_squared_error: 0.0228 - val_loss: 0.1841 - val_mean_squared_error: 0.0528\n",
      "Epoch 5/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0825 - mean_squared_error: 0.0169\n",
      "Epoch 5: val_loss improved from 0.14829 to 0.14017, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0828 - mean_squared_error: 0.0170 - val_loss: 0.1402 - val_mean_squared_error: 0.0345\n",
      "Epoch 6/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0749 - mean_squared_error: 0.0143\n",
      "Epoch 6: val_loss did not improve from 0.14017\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0745 - mean_squared_error: 0.0144 - val_loss: 0.1786 - val_mean_squared_error: 0.0535\n",
      "Epoch 7/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0695 - mean_squared_error: 0.0127\n",
      "Epoch 7: val_loss improved from 0.14017 to 0.11953, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0691 - mean_squared_error: 0.0126 - val_loss: 0.1195 - val_mean_squared_error: 0.0283\n",
      "Epoch 8/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0612 - mean_squared_error: 0.0103\n",
      "Epoch 8: val_loss did not improve from 0.11953\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0621 - mean_squared_error: 0.0106 - val_loss: 0.1402 - val_mean_squared_error: 0.0362\n",
      "Epoch 9/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0612 - mean_squared_error: 0.0105\n",
      "Epoch 9: val_loss improved from 0.11953 to 0.10473, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0606 - mean_squared_error: 0.0104 - val_loss: 0.1047 - val_mean_squared_error: 0.0235\n",
      "Epoch 10/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0576 - mean_squared_error: 0.0093\n",
      "Epoch 10: val_loss did not improve from 0.10473\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0570 - mean_squared_error: 0.0092 - val_loss: 0.1062 - val_mean_squared_error: 0.0241\n",
      "Epoch 11/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0549 - mean_squared_error: 0.0089\n",
      "Epoch 11: val_loss did not improve from 0.10473\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.0549 - mean_squared_error: 0.0087 - val_loss: 0.1071 - val_mean_squared_error: 0.0243\n",
      "Epoch 12/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0542 - mean_squared_error: 0.0084\n",
      "Epoch 12: val_loss improved from 0.10473 to 0.10103, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0540 - mean_squared_error: 0.0082 - val_loss: 0.1010 - val_mean_squared_error: 0.0223\n",
      "Epoch 13/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0079\n",
      "Epoch 13: val_loss improved from 0.10103 to 0.09901, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0504 - mean_squared_error: 0.0077 - val_loss: 0.0990 - val_mean_squared_error: 0.0220\n",
      "Epoch 14/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0502 - mean_squared_error: 0.0074\n",
      "Epoch 14: val_loss improved from 0.09901 to 0.09853, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0509 - mean_squared_error: 0.0077 - val_loss: 0.0985 - val_mean_squared_error: 0.0221\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0071\n",
      "Epoch 15: val_loss did not improve from 0.09853\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0481 - mean_squared_error: 0.0071 - val_loss: 0.1038 - val_mean_squared_error: 0.0230\n",
      "Epoch 16/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0066\n",
      "Epoch 16: val_loss improved from 0.09853 to 0.09350, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0473 - mean_squared_error: 0.0067 - val_loss: 0.0935 - val_mean_squared_error: 0.0218\n",
      "Epoch 17/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0065\n",
      "Epoch 17: val_loss did not improve from 0.09350\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.0460 - mean_squared_error: 0.0065 - val_loss: 0.1034 - val_mean_squared_error: 0.0230\n",
      "Epoch 18/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0062\n",
      "Epoch 18: val_loss did not improve from 0.09350\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0443 - mean_squared_error: 0.0062 - val_loss: 0.0976 - val_mean_squared_error: 0.0218\n",
      "Epoch 19/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0416 - mean_squared_error: 0.0057\n",
      "Epoch 19: val_loss did not improve from 0.09350\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0410 - mean_squared_error: 0.0056 - val_loss: 0.0971 - val_mean_squared_error: 0.0220\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.10, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1495 - mean_squared_error: 0.0532\n",
      "Epoch 1: val_loss improved from inf to 0.19984, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 2s 35ms/step - loss: 0.1495 - mean_squared_error: 0.0527 - val_loss: 0.1998 - val_mean_squared_error: 0.0617\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1182 - mean_squared_error: 0.0320\n",
      "Epoch 2: val_loss improved from 0.19984 to 0.17318, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1161 - mean_squared_error: 0.0311 - val_loss: 0.1732 - val_mean_squared_error: 0.0485\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0973 - mean_squared_error: 0.0224\n",
      "Epoch 3: val_loss did not improve from 0.17318\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.0965 - mean_squared_error: 0.0221 - val_loss: 0.2173 - val_mean_squared_error: 0.0671\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0800 - mean_squared_error: 0.0164\n",
      "Epoch 4: val_loss did not improve from 0.17318\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0800 - mean_squared_error: 0.0165 - val_loss: 0.1997 - val_mean_squared_error: 0.0584\n",
      "Epoch 5/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0741 - mean_squared_error: 0.0142\n",
      "Epoch 5: val_loss improved from 0.17318 to 0.16048, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0740 - mean_squared_error: 0.0141 - val_loss: 0.1605 - val_mean_squared_error: 0.0434\n",
      "Epoch 6/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0674 - mean_squared_error: 0.0123\n",
      "Epoch 6: val_loss improved from 0.16048 to 0.14201, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 0.0677 - mean_squared_error: 0.0124 - val_loss: 0.1420 - val_mean_squared_error: 0.0368\n",
      "Epoch 7/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0635 - mean_squared_error: 0.0113\n",
      "Epoch 7: val_loss improved from 0.14201 to 0.13410, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0632 - mean_squared_error: 0.0112 - val_loss: 0.1341 - val_mean_squared_error: 0.0342\n",
      "Epoch 8/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0591 - mean_squared_error: 0.0099\n",
      "Epoch 8: val_loss improved from 0.13410 to 0.11406, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0603 - mean_squared_error: 0.0105 - val_loss: 0.1141 - val_mean_squared_error: 0.0265\n",
      "Epoch 9/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0555 - mean_squared_error: 0.0091\n",
      "Epoch 9: val_loss did not improve from 0.11406\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.0555 - mean_squared_error: 0.0091 - val_loss: 0.1287 - val_mean_squared_error: 0.0321\n",
      "Epoch 10/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0530 - mean_squared_error: 0.0086\n",
      "Epoch 10: val_loss improved from 0.11406 to 0.10460, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0526 - mean_squared_error: 0.0085 - val_loss: 0.1046 - val_mean_squared_error: 0.0245\n",
      "Epoch 11/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0503 - mean_squared_error: 0.0082\n",
      "Epoch 11: val_loss did not improve from 0.10460\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0499 - mean_squared_error: 0.0079 - val_loss: 0.1087 - val_mean_squared_error: 0.0251\n",
      "Epoch 12/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0493 - mean_squared_error: 0.0077\n",
      "Epoch 12: val_loss did not improve from 0.10460\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.0491 - mean_squared_error: 0.0075 - val_loss: 0.1109 - val_mean_squared_error: 0.0260\n",
      "Epoch 13/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0489 - mean_squared_error: 0.0078\n",
      "Epoch 13: val_loss improved from 0.10460 to 0.10436, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0481 - mean_squared_error: 0.0075 - val_loss: 0.1044 - val_mean_squared_error: 0.0243\n",
      "Epoch 14/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0474 - mean_squared_error: 0.0071\n",
      "Epoch 14: val_loss did not improve from 0.10436\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0478 - mean_squared_error: 0.0073 - val_loss: 0.1048 - val_mean_squared_error: 0.0244\n",
      "Epoch 15/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0467 - mean_squared_error: 0.0069\n",
      "Epoch 15: val_loss did not improve from 0.10436\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 0.0466 - mean_squared_error: 0.0069 - val_loss: 0.1069 - val_mean_squared_error: 0.0247\n",
      "Epoch 16/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0438 - mean_squared_error: 0.0063\n",
      "Epoch 16: val_loss improved from 0.10436 to 0.09998, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0439 - mean_squared_error: 0.0064 - val_loss: 0.1000 - val_mean_squared_error: 0.0235\n",
      "Epoch 17/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0059\n",
      "Epoch 17: val_loss did not improve from 0.09998\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0419 - mean_squared_error: 0.0060 - val_loss: 0.1011 - val_mean_squared_error: 0.0238\n",
      "Epoch 18/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0062\n",
      "Epoch 18: val_loss did not improve from 0.09998\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0445 - mean_squared_error: 0.0063 - val_loss: 0.1037 - val_mean_squared_error: 0.0244\n",
      "Epoch 19/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0058\n",
      "Epoch 19: val_loss did not improve from 0.09998\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0417 - mean_squared_error: 0.0059 - val_loss: 0.1023 - val_mean_squared_error: 0.0257\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###1 fold : val mae 0.10, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1564 - mean_squared_error: 0.0572\n",
      "Epoch 1: val_loss improved from inf to 0.23560, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 3s 41ms/step - loss: 0.1563 - mean_squared_error: 0.0574 - val_loss: 0.2356 - val_mean_squared_error: 0.0996\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1343 - mean_squared_error: 0.0417\n",
      "Epoch 2: val_loss improved from 0.23560 to 0.20224, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1329 - mean_squared_error: 0.0408 - val_loss: 0.2022 - val_mean_squared_error: 0.0607\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1042 - mean_squared_error: 0.0261\n",
      "Epoch 3: val_loss improved from 0.20224 to 0.19659, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1036 - mean_squared_error: 0.0258 - val_loss: 0.1966 - val_mean_squared_error: 0.0603\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0874 - mean_squared_error: 0.0198\n",
      "Epoch 4: val_loss did not improve from 0.19659\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.0870 - mean_squared_error: 0.0195 - val_loss: 0.2013 - val_mean_squared_error: 0.0609\n",
      "Epoch 5/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.0154\n",
      "Epoch 5: val_loss improved from 0.19659 to 0.17742, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0763 - mean_squared_error: 0.0154 - val_loss: 0.1774 - val_mean_squared_error: 0.0510\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0714 - mean_squared_error: 0.0136\n",
      "Epoch 6: val_loss improved from 0.17742 to 0.15118, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0714 - mean_squared_error: 0.0136 - val_loss: 0.1512 - val_mean_squared_error: 0.0398\n",
      "Epoch 7/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0643 - mean_squared_error: 0.0116\n",
      "Epoch 7: val_loss improved from 0.15118 to 0.13919, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.0646 - mean_squared_error: 0.0117 - val_loss: 0.1392 - val_mean_squared_error: 0.0361\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0611 - mean_squared_error: 0.0106\n",
      "Epoch 8: val_loss improved from 0.13919 to 0.13014, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 0.0611 - mean_squared_error: 0.0106 - val_loss: 0.1301 - val_mean_squared_error: 0.0317\n",
      "Epoch 9/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0582 - mean_squared_error: 0.0099\n",
      "Epoch 9: val_loss improved from 0.13014 to 0.11551, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0581 - mean_squared_error: 0.0099 - val_loss: 0.1155 - val_mean_squared_error: 0.0277\n",
      "Epoch 10/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0550 - mean_squared_error: 0.0090\n",
      "Epoch 10: val_loss improved from 0.11551 to 0.10185, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0548 - mean_squared_error: 0.0089 - val_loss: 0.1019 - val_mean_squared_error: 0.0231\n",
      "Epoch 11/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0516 - mean_squared_error: 0.0081\n",
      "Epoch 11: val_loss did not improve from 0.10185\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0513 - mean_squared_error: 0.0080 - val_loss: 0.1201 - val_mean_squared_error: 0.0290\n",
      "Epoch 12/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0502 - mean_squared_error: 0.0079\n",
      "Epoch 12: val_loss did not improve from 0.10185\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0509 - mean_squared_error: 0.0079 - val_loss: 0.1228 - val_mean_squared_error: 0.0304\n",
      "Epoch 13/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0074\n",
      "Epoch 13: val_loss improved from 0.10185 to 0.09269, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.0481 - mean_squared_error: 0.0073 - val_loss: 0.0927 - val_mean_squared_error: 0.0211\n",
      "Epoch 14/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0068\n",
      "Epoch 14: val_loss did not improve from 0.09269\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0470 - mean_squared_error: 0.0069 - val_loss: 0.0999 - val_mean_squared_error: 0.0230\n",
      "Epoch 15/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0065\n",
      "Epoch 15: val_loss did not improve from 0.09269\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0465 - mean_squared_error: 0.0069 - val_loss: 0.0963 - val_mean_squared_error: 0.0217\n",
      "Epoch 16/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0067\n",
      "Epoch 16: val_loss did not improve from 0.09269\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0453 - mean_squared_error: 0.0066 - val_loss: 0.1036 - val_mean_squared_error: 0.0235\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###2 fold : val mae 0.10, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1581 - mean_squared_error: 0.0593\n",
      "Epoch 1: val_loss improved from inf to 0.15285, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_3.hdf5\n",
      "23/23 [==============================] - 2s 36ms/step - loss: 0.1581 - mean_squared_error: 0.0593 - val_loss: 0.1529 - val_mean_squared_error: 0.0548\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1436 - mean_squared_error: 0.0505\n",
      "Epoch 2: val_loss improved from 0.15285 to 0.14607, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt11str4,layer2:c16filt11str4,layer3:c128filt7str1,layer4:c32filt9str2,1conv,dropout0,dnodes64,dropout0/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.1423 - mean_squared_error: 0.0493 - val_loss: 0.1461 - val_mean_squared_error: 0.0382\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1142 - mean_squared_error: 0.0305\n",
      "Epoch 3: val_loss did not improve from 0.14607\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1134 - mean_squared_error: 0.0302 - val_loss: 0.1636 - val_mean_squared_error: 0.0454\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0988 - mean_squared_error: 0.0239\n",
      "Epoch 4: val_loss did not improve from 0.14607\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0975 - mean_squared_error: 0.0232 - val_loss: 0.2289 - val_mean_squared_error: 0.0738\n",
      "Epoch 5/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0866 - mean_squared_error: 0.0189\n",
      "Epoch 5: val_loss did not improve from 0.14607\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0856 - mean_squared_error: 0.0184 - val_loss: 0.1539 - val_mean_squared_error: 0.0403\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.16, val rmse 0.20###\n",
      "mae1.12+-0.25_rmse1.62+-0.25\n",
      "random search 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1643 - mean_squared_error: 0.0598\n",
      "Epoch 1: val_loss improved from inf to 0.23308, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_0.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1640 - mean_squared_error: 0.0594 - val_loss: 0.2331 - val_mean_squared_error: 0.0851\n",
      "Epoch 2/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1280 - mean_squared_error: 0.0385\n",
      "Epoch 2: val_loss improved from 0.23308 to 0.13149, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1277 - mean_squared_error: 0.0383 - val_loss: 0.1315 - val_mean_squared_error: 0.0340\n",
      "Epoch 3/100\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 0.1181 - mean_squared_error: 0.0321\n",
      "Epoch 3: val_loss improved from 0.13149 to 0.09544, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.1167 - mean_squared_error: 0.0315 - val_loss: 0.0954 - val_mean_squared_error: 0.0232\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1058 - mean_squared_error: 0.0276\n",
      "Epoch 4: val_loss did not improve from 0.09544\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1058 - mean_squared_error: 0.0276 - val_loss: 0.1031 - val_mean_squared_error: 0.0233\n",
      "Epoch 5/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.0979 - mean_squared_error: 0.0230\n",
      "Epoch 5: val_loss improved from 0.09544 to 0.09180, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0980 - mean_squared_error: 0.0231 - val_loss: 0.0918 - val_mean_squared_error: 0.0220\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0906 - mean_squared_error: 0.0203\n",
      "Epoch 6: val_loss did not improve from 0.09180\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0906 - mean_squared_error: 0.0203 - val_loss: 0.1059 - val_mean_squared_error: 0.0249\n",
      "Epoch 7/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0916 - mean_squared_error: 0.0214\n",
      "Epoch 7: val_loss improved from 0.09180 to 0.08296, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0916 - mean_squared_error: 0.0214 - val_loss: 0.0830 - val_mean_squared_error: 0.0186\n",
      "Epoch 8/100\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 0.0868 - mean_squared_error: 0.0196\n",
      "Epoch 8: val_loss did not improve from 0.08296\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0865 - mean_squared_error: 0.0196 - val_loss: 0.0892 - val_mean_squared_error: 0.0202\n",
      "Epoch 9/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.0845 - mean_squared_error: 0.0184\n",
      "Epoch 9: val_loss did not improve from 0.08296\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0847 - mean_squared_error: 0.0185 - val_loss: 0.1059 - val_mean_squared_error: 0.0291\n",
      "Epoch 10/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0844 - mean_squared_error: 0.0183\n",
      "Epoch 10: val_loss did not improve from 0.08296\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0841 - mean_squared_error: 0.0182 - val_loss: 0.0863 - val_mean_squared_error: 0.0184\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.09, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1568 - mean_squared_error: 0.0561\n",
      "Epoch 1: val_loss improved from inf to 0.21432, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1568 - mean_squared_error: 0.0561 - val_loss: 0.2143 - val_mean_squared_error: 0.0722\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1221 - mean_squared_error: 0.0356\n",
      "Epoch 2: val_loss improved from 0.21432 to 0.12848, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1221 - mean_squared_error: 0.0356 - val_loss: 0.1285 - val_mean_squared_error: 0.0349\n",
      "Epoch 3/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1103 - mean_squared_error: 0.0280\n",
      "Epoch 3: val_loss improved from 0.12848 to 0.09939, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.1097 - mean_squared_error: 0.0277 - val_loss: 0.0994 - val_mean_squared_error: 0.0256\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0250\n",
      "Epoch 4: val_loss did not improve from 0.09939\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1017 - mean_squared_error: 0.0250 - val_loss: 0.1015 - val_mean_squared_error: 0.0257\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0967 - mean_squared_error: 0.0231\n",
      "Epoch 5: val_loss did not improve from 0.09939\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0967 - mean_squared_error: 0.0231 - val_loss: 0.1031 - val_mean_squared_error: 0.0273\n",
      "Epoch 6/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.0921 - mean_squared_error: 0.0210\n",
      "Epoch 6: val_loss improved from 0.09939 to 0.09925, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0921 - mean_squared_error: 0.0211 - val_loss: 0.0993 - val_mean_squared_error: 0.0239\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0875 - mean_squared_error: 0.0197\n",
      "Epoch 7: val_loss improved from 0.09925 to 0.09894, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0875 - mean_squared_error: 0.0197 - val_loss: 0.0989 - val_mean_squared_error: 0.0247\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0879 - mean_squared_error: 0.0197\n",
      "Epoch 8: val_loss improved from 0.09894 to 0.09572, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0879 - mean_squared_error: 0.0197 - val_loss: 0.0957 - val_mean_squared_error: 0.0229\n",
      "Epoch 9/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0836 - mean_squared_error: 0.0182\n",
      "Epoch 9: val_loss improved from 0.09572 to 0.09360, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0837 - mean_squared_error: 0.0182 - val_loss: 0.0936 - val_mean_squared_error: 0.0237\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0797 - mean_squared_error: 0.0165\n",
      "Epoch 10: val_loss did not improve from 0.09360\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0797 - mean_squared_error: 0.0165 - val_loss: 0.1071 - val_mean_squared_error: 0.0265\n",
      "Epoch 11/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0806 - mean_squared_error: 0.0169\n",
      "Epoch 11: val_loss improved from 0.09360 to 0.09139, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.0806 - mean_squared_error: 0.0169 - val_loss: 0.0914 - val_mean_squared_error: 0.0213\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0791 - mean_squared_error: 0.0163\n",
      "Epoch 12: val_loss did not improve from 0.09139\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0791 - mean_squared_error: 0.0163 - val_loss: 0.0936 - val_mean_squared_error: 0.0236\n",
      "Epoch 13/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0754 - mean_squared_error: 0.0149\n",
      "Epoch 13: val_loss did not improve from 0.09139\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0751 - mean_squared_error: 0.0148 - val_loss: 0.0934 - val_mean_squared_error: 0.0230\n",
      "Epoch 14/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0743 - mean_squared_error: 0.0143\n",
      "Epoch 14: val_loss improved from 0.09139 to 0.08809, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0744 - mean_squared_error: 0.0144 - val_loss: 0.0881 - val_mean_squared_error: 0.0205\n",
      "Epoch 15/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0738 - mean_squared_error: 0.0144\n",
      "Epoch 15: val_loss did not improve from 0.08809\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0738 - mean_squared_error: 0.0144 - val_loss: 0.0916 - val_mean_squared_error: 0.0226\n",
      "Epoch 16/100\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 0.0703 - mean_squared_error: 0.0131\n",
      "Epoch 16: val_loss did not improve from 0.08809\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0705 - mean_squared_error: 0.0132 - val_loss: 0.0886 - val_mean_squared_error: 0.0211\n",
      "Epoch 17/100\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 0.0698 - mean_squared_error: 0.0126\n",
      "Epoch 17: val_loss improved from 0.08809 to 0.08796, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0708 - mean_squared_error: 0.0129 - val_loss: 0.0880 - val_mean_squared_error: 0.0202\n",
      "Epoch 18/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0692 - mean_squared_error: 0.0123\n",
      "Epoch 18: val_loss improved from 0.08796 to 0.08633, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0699 - mean_squared_error: 0.0126 - val_loss: 0.0863 - val_mean_squared_error: 0.0200\n",
      "Epoch 19/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.0698 - mean_squared_error: 0.0130\n",
      "Epoch 19: val_loss did not improve from 0.08633\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0695 - mean_squared_error: 0.0130 - val_loss: 0.0901 - val_mean_squared_error: 0.0218\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0693 - mean_squared_error: 0.0129\n",
      "Epoch 20: val_loss did not improve from 0.08633\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0693 - mean_squared_error: 0.0129 - val_loss: 0.0918 - val_mean_squared_error: 0.0220\n",
      "Epoch 21/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0690 - mean_squared_error: 0.0124\n",
      "Epoch 21: val_loss did not improve from 0.08633\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0693 - mean_squared_error: 0.0125 - val_loss: 0.0895 - val_mean_squared_error: 0.0199\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###1 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1822 - mean_squared_error: 0.0746\n",
      "Epoch 1: val_loss improved from inf to 0.14906, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_2.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1818 - mean_squared_error: 0.0743 - val_loss: 0.1491 - val_mean_squared_error: 0.0551\n",
      "Epoch 2/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1522 - mean_squared_error: 0.0530\n",
      "Epoch 2: val_loss improved from 0.14906 to 0.13902, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1515 - mean_squared_error: 0.0527 - val_loss: 0.1390 - val_mean_squared_error: 0.0484\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1258 - mean_squared_error: 0.0364\n",
      "Epoch 3: val_loss improved from 0.13902 to 0.10445, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1258 - mean_squared_error: 0.0364 - val_loss: 0.1044 - val_mean_squared_error: 0.0271\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1102 - mean_squared_error: 0.0290\n",
      "Epoch 4: val_loss did not improve from 0.10445\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1102 - mean_squared_error: 0.0290 - val_loss: 0.1499 - val_mean_squared_error: 0.0418\n",
      "Epoch 5/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1028 - mean_squared_error: 0.0259\n",
      "Epoch 5: val_loss did not improve from 0.10445\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.1028 - mean_squared_error: 0.0258 - val_loss: 0.1192 - val_mean_squared_error: 0.0357\n",
      "Epoch 6/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.0981 - mean_squared_error: 0.0238\n",
      "Epoch 6: val_loss improved from 0.10445 to 0.09864, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0980 - mean_squared_error: 0.0238 - val_loss: 0.0986 - val_mean_squared_error: 0.0229\n",
      "Epoch 7/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.0919 - mean_squared_error: 0.0214\n",
      "Epoch 7: val_loss improved from 0.09864 to 0.09423, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0920 - mean_squared_error: 0.0215 - val_loss: 0.0942 - val_mean_squared_error: 0.0210\n",
      "Epoch 8/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0863 - mean_squared_error: 0.0190\n",
      "Epoch 8: val_loss did not improve from 0.09423\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0863 - mean_squared_error: 0.0190 - val_loss: 0.1041 - val_mean_squared_error: 0.0246\n",
      "Epoch 9/100\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 0.0874 - mean_squared_error: 0.0191\n",
      "Epoch 9: val_loss did not improve from 0.09423\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0874 - mean_squared_error: 0.0192 - val_loss: 0.1105 - val_mean_squared_error: 0.0318\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0864 - mean_squared_error: 0.0187\n",
      "Epoch 10: val_loss improved from 0.09423 to 0.08312, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0864 - mean_squared_error: 0.0187 - val_loss: 0.0831 - val_mean_squared_error: 0.0186\n",
      "Epoch 11/100\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 0.0812 - mean_squared_error: 0.0169\n",
      "Epoch 11: val_loss did not improve from 0.08312\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0819 - mean_squared_error: 0.0172 - val_loss: 0.0968 - val_mean_squared_error: 0.0221\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0776 - mean_squared_error: 0.0158\n",
      "Epoch 12: val_loss did not improve from 0.08312\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0776 - mean_squared_error: 0.0158 - val_loss: 0.0875 - val_mean_squared_error: 0.0202\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0795 - mean_squared_error: 0.0166\n",
      "Epoch 13: val_loss did not improve from 0.08312\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0795 - mean_squared_error: 0.0166 - val_loss: 0.0987 - val_mean_squared_error: 0.0221\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1860 - mean_squared_error: 0.0786\n",
      "Epoch 1: val_loss improved from inf to 0.16251, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1860 - mean_squared_error: 0.0786 - val_loss: 0.1625 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1594 - mean_squared_error: 0.0604\n",
      "Epoch 2: val_loss improved from 0.16251 to 0.15512, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1590 - mean_squared_error: 0.0602 - val_loss: 0.1551 - val_mean_squared_error: 0.0559\n",
      "Epoch 3/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1481 - mean_squared_error: 0.0506\n",
      "Epoch 3: val_loss improved from 0.15512 to 0.15095, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1480 - mean_squared_error: 0.0506 - val_loss: 0.1509 - val_mean_squared_error: 0.0533\n",
      "Epoch 4/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1276 - mean_squared_error: 0.0383\n",
      "Epoch 4: val_loss did not improve from 0.15095\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1267 - mean_squared_error: 0.0378 - val_loss: 0.1660 - val_mean_squared_error: 0.0535\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1159 - mean_squared_error: 0.0310\n",
      "Epoch 5: val_loss improved from 0.15095 to 0.12384, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1159 - mean_squared_error: 0.0310 - val_loss: 0.1238 - val_mean_squared_error: 0.0317\n",
      "Epoch 6/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1035 - mean_squared_error: 0.0259\n",
      "Epoch 6: val_loss did not improve from 0.12384\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1034 - mean_squared_error: 0.0259 - val_loss: 0.1292 - val_mean_squared_error: 0.0333\n",
      "Epoch 7/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.1005 - mean_squared_error: 0.0250\n",
      "Epoch 7: val_loss improved from 0.12384 to 0.10775, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.1001 - mean_squared_error: 0.0247 - val_loss: 0.1078 - val_mean_squared_error: 0.0251\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0879 - mean_squared_error: 0.0199\n",
      "Epoch 8: val_loss improved from 0.10775 to 0.09804, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0879 - mean_squared_error: 0.0199 - val_loss: 0.0980 - val_mean_squared_error: 0.0227\n",
      "Epoch 9/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0885 - mean_squared_error: 0.0201\n",
      "Epoch 9: val_loss improved from 0.09804 to 0.09772, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0884 - mean_squared_error: 0.0201 - val_loss: 0.0977 - val_mean_squared_error: 0.0244\n",
      "Epoch 10/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.0844 - mean_squared_error: 0.0190\n",
      "Epoch 10: val_loss did not improve from 0.09772\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0843 - mean_squared_error: 0.0189 - val_loss: 0.0990 - val_mean_squared_error: 0.0229\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0849 - mean_squared_error: 0.0190\n",
      "Epoch 11: val_loss improved from 0.09772 to 0.09534, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0849 - mean_squared_error: 0.0190 - val_loss: 0.0953 - val_mean_squared_error: 0.0205\n",
      "Epoch 12/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.0792 - mean_squared_error: 0.0170\n",
      "Epoch 12: val_loss improved from 0.09534 to 0.08621, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c16filt7str1,layer2:c16filt7str1,layer3:c64filt11str2,1conv,dropout0.1,dnodes32,dropout0.5/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0791 - mean_squared_error: 0.0170 - val_loss: 0.0862 - val_mean_squared_error: 0.0188\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0787 - mean_squared_error: 0.0164\n",
      "Epoch 13: val_loss did not improve from 0.08621\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0787 - mean_squared_error: 0.0164 - val_loss: 0.0867 - val_mean_squared_error: 0.0181\n",
      "Epoch 14/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0755 - mean_squared_error: 0.0154\n",
      "Epoch 14: val_loss did not improve from 0.08621\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0755 - mean_squared_error: 0.0154 - val_loss: 0.0935 - val_mean_squared_error: 0.0209\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0730 - mean_squared_error: 0.0145\n",
      "Epoch 15: val_loss did not improve from 0.08621\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0730 - mean_squared_error: 0.0145 - val_loss: 0.0867 - val_mean_squared_error: 0.0182\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.09, val rmse 0.14###\n",
      "mae0.91+-0.02_rmse1.41+-0.04\n",
      "random search 3/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1623 - mean_squared_error: 0.0621\n",
      "Epoch 1: val_loss improved from inf to 0.16156, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str4,layer2:c32filt9str1,layer3:c128filt11str1,layer4:c32filt7str1,1conv,dropout0,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1623 - mean_squared_error: 0.0621 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 2/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1607 - mean_squared_error: 0.0614\n",
      "Epoch 2: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 3/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1612 - mean_squared_error: 0.0615\n",
      "Epoch 3: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.0613\n",
      "Epoch 4: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1643 - mean_squared_error: 0.0649\n",
      "Epoch 1: val_loss improved from inf to 0.16475, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str4,layer2:c32filt9str1,layer3:c128filt11str1,layer4:c32filt7str1,1conv,dropout0,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 4s 13ms/step - loss: 0.1643 - mean_squared_error: 0.0649 - val_loss: 0.1647 - val_mean_squared_error: 0.0635\n",
      "Epoch 2/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1594 - mean_squared_error: 0.0609\n",
      "Epoch 2: val_loss did not improve from 0.16475\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 3/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1597 - mean_squared_error: 0.0609\n",
      "Epoch 3: val_loss did not improve from 0.16475\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1593 - mean_squared_error: 0.0608\n",
      "Epoch 4: val_loss did not improve from 0.16475\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1640 - mean_squared_error: 0.0628\n",
      "Epoch 1: val_loss improved from inf to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str4,layer2:c32filt9str1,layer3:c128filt11str1,layer4:c32filt7str1,1conv,dropout0,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1645 - mean_squared_error: 0.0631 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0625\n",
      "Epoch 2: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 3/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1630 - mean_squared_error: 0.0625\n",
      "Epoch 3: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1632 - mean_squared_error: 0.0626\n",
      "Epoch 4: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1616 - mean_squared_error: 0.0620\n",
      "Epoch 1: val_loss improved from inf to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str4,layer2:c32filt9str1,layer3:c128filt11str1,layer4:c32filt7str1,1conv,dropout0,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1616 - mean_squared_error: 0.0620 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0617\n",
      "Epoch 2: val_loss improved from 0.16255 to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str4,layer2:c32filt9str1,layer3:c128filt11str1,layer4:c32filt7str1,1conv,dropout0,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1605 - mean_squared_error: 0.0617 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 3/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0614\n",
      "Epoch 3: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 0.1604 - mean_squared_error: 0.0615\n",
      "Epoch 4: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 5/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.0619\n",
      "Epoch 5: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###3 fold : val mae 0.17, val rmse 0.26###\n",
      "mae1.71+-0.00_rmse2.58+-0.00\n",
      "random search 4/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1738 - mean_squared_error: 0.0701\n",
      "Epoch 1: val_loss improved from inf to 0.16156, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt15str2,layer2:c128filt17str2,1conv,dropout0.5,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 4s 22ms/step - loss: 0.1736 - mean_squared_error: 0.0701 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 2/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1613 - mean_squared_error: 0.0617\n",
      "Epoch 2: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 3/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1612 - mean_squared_error: 0.0615\n",
      "Epoch 3: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0613\n",
      "Epoch 4: val_loss did not improve from 0.16156\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1678 - mean_squared_error: 0.0663\n",
      "Epoch 1: val_loss improved from inf to 0.16526, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt15str2,layer2:c128filt17str2,1conv,dropout0.5,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 3s 16ms/step - loss: 0.1683 - mean_squared_error: 0.0665 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 2/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1593 - mean_squared_error: 0.0608\n",
      "Epoch 2: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1593 - mean_squared_error: 0.0607 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1594 - mean_squared_error: 0.0608\n",
      "Epoch 3: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 4/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1582 - mean_squared_error: 0.0602\n",
      "Epoch 4: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1592 - mean_squared_error: 0.0607 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1719 - mean_squared_error: 0.0680\n",
      "Epoch 1: val_loss improved from inf to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt15str2,layer2:c128filt17str2,1conv,dropout0.5,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 3s 22ms/step - loss: 0.1717 - mean_squared_error: 0.0679 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 2/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1635 - mean_squared_error: 0.0627\n",
      "Epoch 2: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 3/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1632 - mean_squared_error: 0.0626\n",
      "Epoch 3: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1632 - mean_squared_error: 0.0626\n",
      "Epoch 4: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1712 - mean_squared_error: 0.0693\n",
      "Epoch 1: val_loss improved from inf to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt15str2,layer2:c128filt17str2,1conv,dropout0.5,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 4s 16ms/step - loss: 0.1712 - mean_squared_error: 0.0693 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0615\n",
      "Epoch 2: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 1s 12ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 3/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0616\n",
      "Epoch 3: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0614\n",
      "Epoch 4: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.17, val rmse 0.26###\n",
      "mae1.71+-0.00_rmse2.58+-0.00\n",
      "random search 5/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1671 - mean_squared_error: 0.0647\n",
      "Epoch 1: val_loss improved from inf to 0.15850, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_0.hdf5\n",
      "46/46 [==============================] - 4s 43ms/step - loss: 0.1671 - mean_squared_error: 0.0647 - val_loss: 0.1585 - val_mean_squared_error: 0.0605\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1613 - mean_squared_error: 0.0623\n",
      "Epoch 2: val_loss did not improve from 0.15850\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 0.1614 - mean_squared_error: 0.0623 - val_loss: 0.1602 - val_mean_squared_error: 0.0611\n",
      "Epoch 3/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.1583 - mean_squared_error: 0.0592\n",
      "Epoch 3: val_loss improved from 0.15850 to 0.15745, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1581 - mean_squared_error: 0.0593 - val_loss: 0.1574 - val_mean_squared_error: 0.0582\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1432 - mean_squared_error: 0.0468\n",
      "Epoch 4: val_loss improved from 0.15745 to 0.12768, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1432 - mean_squared_error: 0.0468 - val_loss: 0.1277 - val_mean_squared_error: 0.0387\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1213 - mean_squared_error: 0.0329\n",
      "Epoch 5: val_loss did not improve from 0.12768\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1213 - mean_squared_error: 0.0329 - val_loss: 0.3669 - val_mean_squared_error: 0.1978\n",
      "Epoch 6/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.1080 - mean_squared_error: 0.0264\n",
      "Epoch 6: val_loss did not improve from 0.12768\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1081 - mean_squared_error: 0.0266 - val_loss: 0.1838 - val_mean_squared_error: 0.0721\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1113 - mean_squared_error: 0.0279\n",
      "Epoch 7: val_loss improved from 0.12768 to 0.12750, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1113 - mean_squared_error: 0.0279 - val_loss: 0.1275 - val_mean_squared_error: 0.0419\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1030 - mean_squared_error: 0.0252\n",
      "Epoch 8: val_loss did not improve from 0.12750\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1030 - mean_squared_error: 0.0252 - val_loss: 0.1909 - val_mean_squared_error: 0.0675\n",
      "Epoch 9/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0974 - mean_squared_error: 0.0217\n",
      "Epoch 9: val_loss improved from 0.12750 to 0.10077, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0969 - mean_squared_error: 0.0216 - val_loss: 0.1008 - val_mean_squared_error: 0.0229\n",
      "Epoch 10/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0905 - mean_squared_error: 0.0200\n",
      "Epoch 10: val_loss improved from 0.10077 to 0.09632, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0902 - mean_squared_error: 0.0199 - val_loss: 0.0963 - val_mean_squared_error: 0.0223\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0920 - mean_squared_error: 0.0201\n",
      "Epoch 11: val_loss did not improve from 0.09632\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0920 - mean_squared_error: 0.0201 - val_loss: 0.1244 - val_mean_squared_error: 0.0313\n",
      "Epoch 12/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0854 - mean_squared_error: 0.0178\n",
      "Epoch 12: val_loss did not improve from 0.09632\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0852 - mean_squared_error: 0.0177 - val_loss: 0.0982 - val_mean_squared_error: 0.0240\n",
      "Epoch 13/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0826 - mean_squared_error: 0.0170\n",
      "Epoch 13: val_loss did not improve from 0.09632\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0828 - mean_squared_error: 0.0171 - val_loss: 0.0986 - val_mean_squared_error: 0.0216\n",
      "31/31 [==============================] - 0s 5ms/step\n",
      " ###0 fold : val mae 0.10, val rmse 0.16###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1662 - mean_squared_error: 0.0644\n",
      "Epoch 1: val_loss improved from inf to 0.16526, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_1.hdf5\n",
      "46/46 [==============================] - 4s 46ms/step - loss: 0.1662 - mean_squared_error: 0.0644 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1592 - mean_squared_error: 0.0607\n",
      "Epoch 2: val_loss did not improve from 0.16526\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1594 - mean_squared_error: 0.0608\n",
      "Epoch 3: val_loss did not improve from 0.16526\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1594 - mean_squared_error: 0.0608\n",
      "Epoch 4: val_loss did not improve from 0.16526\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "31/31 [==============================] - 0s 5ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1713 - mean_squared_error: 0.0671\n",
      "Epoch 1: val_loss improved from inf to 0.15499, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_2.hdf5\n",
      "46/46 [==============================] - 4s 47ms/step - loss: 0.1713 - mean_squared_error: 0.0671 - val_loss: 0.1550 - val_mean_squared_error: 0.0592\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1629 - mean_squared_error: 0.0623\n",
      "Epoch 2: val_loss did not improve from 0.15499\n",
      "46/46 [==============================] - 1s 33ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1612 - val_mean_squared_error: 0.0640\n",
      "Epoch 3/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1650 - mean_squared_error: 0.0642\n",
      "Epoch 3: val_loss improved from 0.15499 to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_2.hdf5\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1649 - mean_squared_error: 0.0641 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0625\n",
      "Epoch 4: val_loss did not improve from 0.15396\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 5/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1632 - mean_squared_error: 0.0625\n",
      "Epoch 5: val_loss did not improve from 0.15396\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1632 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0625\n",
      "Epoch 6: val_loss did not improve from 0.15396\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "31/31 [==============================] - 0s 5ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1696 - mean_squared_error: 0.0673\n",
      "Epoch 1: val_loss improved from inf to 0.16083, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt13str2,layer2:c128filt11str1,layer3:c16filt15str2,layer4:c128filt9str1,1conv,dropout0.1,dnodes16,dropout0.1/weights_3.hdf5\n",
      "46/46 [==============================] - 4s 45ms/step - loss: 0.1696 - mean_squared_error: 0.0673 - val_loss: 0.1608 - val_mean_squared_error: 0.0605\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1634 - mean_squared_error: 0.0649\n",
      "Epoch 2: val_loss did not improve from 0.16083\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1634 - mean_squared_error: 0.0649 - val_loss: 0.1630 - val_mean_squared_error: 0.0621\n",
      "Epoch 3/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.0619\n",
      "Epoch 3: val_loss did not improve from 0.16083\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1604 - mean_squared_error: 0.0617 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1602 - mean_squared_error: 0.0614\n",
      "Epoch 4: val_loss did not improve from 0.16083\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "31/31 [==============================] - 0s 5ms/step\n",
      " ###3 fold : val mae 0.17, val rmse 0.26###\n",
      "mae1.54+-0.29_rmse2.32+-0.44\n",
      "random search 6/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1662 - mean_squared_error: 0.0647\n",
      "Epoch 1: val_loss improved from inf to 0.15932, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "23/23 [==============================] - 2s 37ms/step - loss: 0.1662 - mean_squared_error: 0.0646 - val_loss: 0.1593 - val_mean_squared_error: 0.0619\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1653 - mean_squared_error: 0.0665\n",
      "Epoch 2: val_loss did not improve from 0.15932\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 0.1647 - mean_squared_error: 0.0659 - val_loss: 0.1970 - val_mean_squared_error: 0.0936\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1610 - mean_squared_error: 0.0611\n",
      "Epoch 3: val_loss did not improve from 0.15932\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1592 - mean_squared_error: 0.0605 - val_loss: 0.1806 - val_mean_squared_error: 0.0800\n",
      "Epoch 4/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1532 - mean_squared_error: 0.0580\n",
      "Epoch 4: val_loss did not improve from 0.15932\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.1524 - mean_squared_error: 0.0575 - val_loss: 0.2552 - val_mean_squared_error: 0.1382\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1584 - mean_squared_error: 0.0600\n",
      "Epoch 1: val_loss improved from inf to 0.16951, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "23/23 [==============================] - 2s 34ms/step - loss: 0.1594 - mean_squared_error: 0.0603 - val_loss: 0.1695 - val_mean_squared_error: 0.0671\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1501 - mean_squared_error: 0.0545\n",
      "Epoch 2: val_loss did not improve from 0.16951\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1489 - mean_squared_error: 0.0537 - val_loss: 0.3423 - val_mean_squared_error: 0.1983\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1211 - mean_squared_error: 0.0342\n",
      "Epoch 3: val_loss did not improve from 0.16951\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.1186 - mean_squared_error: 0.0331 - val_loss: 0.1819 - val_mean_squared_error: 0.0638\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0959 - mean_squared_error: 0.0218\n",
      "Epoch 4: val_loss did not improve from 0.16951\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0958 - mean_squared_error: 0.0219 - val_loss: 0.2328 - val_mean_squared_error: 0.0857\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.25###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1695 - mean_squared_error: 0.0666\n",
      "Epoch 1: val_loss improved from inf to 0.27231, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 3s 41ms/step - loss: 0.1691 - mean_squared_error: 0.0666 - val_loss: 0.2723 - val_mean_squared_error: 0.1487\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0611\n",
      "Epoch 2: val_loss improved from 0.27231 to 0.15182, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1612 - mean_squared_error: 0.0614 - val_loss: 0.1518 - val_mean_squared_error: 0.0565\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1586 - mean_squared_error: 0.0590\n",
      "Epoch 3: val_loss did not improve from 0.15182\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.1575 - mean_squared_error: 0.0582 - val_loss: 0.1857 - val_mean_squared_error: 0.0799\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1212 - mean_squared_error: 0.0344\n",
      "Epoch 4: val_loss did not improve from 0.15182\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1202 - mean_squared_error: 0.0337 - val_loss: 0.2502 - val_mean_squared_error: 0.1058\n",
      "Epoch 5/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0966 - mean_squared_error: 0.0220\n",
      "Epoch 5: val_loss did not improve from 0.15182\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0970 - mean_squared_error: 0.0222 - val_loss: 0.2992 - val_mean_squared_error: 0.1333\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1689 - mean_squared_error: 0.0673\n",
      "Epoch 1: val_loss improved from inf to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "23/23 [==============================] - 2s 35ms/step - loss: 0.1675 - mean_squared_error: 0.0665 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0617\n",
      "Epoch 2: val_loss improved from 0.16255 to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1610 - mean_squared_error: 0.0617\n",
      "Epoch 3: val_loss did not improve from 0.16255\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1604 - mean_squared_error: 0.0615\n",
      "Epoch 4: val_loss improved from 0.16255 to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c128filt7str1,layer2:c32filt7str2,layer3:c32filt17str2,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 5/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0618\n",
      "Epoch 5: val_loss did not improve from 0.16255\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 6/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1611 - mean_squared_error: 0.0618\n",
      "Epoch 6: val_loss did not improve from 0.16255\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 7/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0616\n",
      "Epoch 7: val_loss did not improve from 0.16255\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###3 fold : val mae 0.17, val rmse 0.26###\n",
      "mae1.70+-0.02_rmse2.56+-0.02\n",
      "random search 7/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1585 - mean_squared_error: 0.0589\n",
      "Epoch 1: val_loss improved from inf to 0.41361, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 20ms/step - loss: 0.1585 - mean_squared_error: 0.0589 - val_loss: 0.4136 - val_mean_squared_error: 0.2345\n",
      "Epoch 2/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1020 - mean_squared_error: 0.0244\n",
      "Epoch 2: val_loss did not improve from 0.41361\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.1025 - mean_squared_error: 0.0245 - val_loss: 0.4302 - val_mean_squared_error: 0.2335\n",
      "Epoch 3/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0924 - mean_squared_error: 0.0202\n",
      "Epoch 3: val_loss improved from 0.41361 to 0.12098, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0912 - mean_squared_error: 0.0198 - val_loss: 0.1210 - val_mean_squared_error: 0.0296\n",
      "Epoch 4/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0820 - mean_squared_error: 0.0167\n",
      "Epoch 4: val_loss improved from 0.12098 to 0.10114, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0835 - mean_squared_error: 0.0174 - val_loss: 0.1011 - val_mean_squared_error: 0.0233\n",
      "Epoch 5/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0753 - mean_squared_error: 0.0140\n",
      "Epoch 5: val_loss did not improve from 0.10114\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0758 - mean_squared_error: 0.0142 - val_loss: 0.1102 - val_mean_squared_error: 0.0253\n",
      "Epoch 6/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0683 - mean_squared_error: 0.0119\n",
      "Epoch 6: val_loss did not improve from 0.10114\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0680 - mean_squared_error: 0.0118 - val_loss: 0.1201 - val_mean_squared_error: 0.0303\n",
      "Epoch 7/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0682 - mean_squared_error: 0.0120\n",
      "Epoch 7: val_loss improved from 0.10114 to 0.09706, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0678 - mean_squared_error: 0.0118 - val_loss: 0.0971 - val_mean_squared_error: 0.0218\n",
      "Epoch 8/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0621 - mean_squared_error: 0.0102\n",
      "Epoch 8: val_loss improved from 0.09706 to 0.09428, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0621 - mean_squared_error: 0.0102 - val_loss: 0.0943 - val_mean_squared_error: 0.0207\n",
      "Epoch 9/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0598 - mean_squared_error: 0.0095\n",
      "Epoch 9: val_loss improved from 0.09428 to 0.09079, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0604 - mean_squared_error: 0.0096 - val_loss: 0.0908 - val_mean_squared_error: 0.0198\n",
      "Epoch 10/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0094\n",
      "Epoch 10: val_loss did not improve from 0.09079\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0597 - mean_squared_error: 0.0094 - val_loss: 0.1077 - val_mean_squared_error: 0.0258\n",
      "Epoch 11/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0549 - mean_squared_error: 0.0083\n",
      "Epoch 11: val_loss did not improve from 0.09079\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0551 - mean_squared_error: 0.0084 - val_loss: 0.0943 - val_mean_squared_error: 0.0207\n",
      "Epoch 12/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0080\n",
      "Epoch 12: val_loss did not improve from 0.09079\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0536 - mean_squared_error: 0.0079 - val_loss: 0.0932 - val_mean_squared_error: 0.0228\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0626\n",
      "Epoch 1: val_loss improved from inf to 0.17950, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 2s 20ms/step - loss: 0.1623 - mean_squared_error: 0.0618 - val_loss: 0.1795 - val_mean_squared_error: 0.0672\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1082 - mean_squared_error: 0.0278\n",
      "Epoch 2: val_loss did not improve from 0.17950\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.1082 - mean_squared_error: 0.0278 - val_loss: 0.2386 - val_mean_squared_error: 0.0881\n",
      "Epoch 3/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0899 - mean_squared_error: 0.0191\n",
      "Epoch 3: val_loss improved from 0.17950 to 0.16209, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0896 - mean_squared_error: 0.0191 - val_loss: 0.1621 - val_mean_squared_error: 0.0460\n",
      "Epoch 4/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0795 - mean_squared_error: 0.0154\n",
      "Epoch 4: val_loss improved from 0.16209 to 0.12107, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0804 - mean_squared_error: 0.0158 - val_loss: 0.1211 - val_mean_squared_error: 0.0299\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0743 - mean_squared_error: 0.0137\n",
      "Epoch 5: val_loss improved from 0.12107 to 0.10198, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0743 - mean_squared_error: 0.0137 - val_loss: 0.1020 - val_mean_squared_error: 0.0232\n",
      "Epoch 6/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0748 - mean_squared_error: 0.0143\n",
      "Epoch 6: val_loss did not improve from 0.10198\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.0751 - mean_squared_error: 0.0143 - val_loss: 0.1104 - val_mean_squared_error: 0.0263\n",
      "Epoch 7/100\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0734 - mean_squared_error: 0.0136\n",
      "Epoch 7: val_loss did not improve from 0.10198\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0728 - mean_squared_error: 0.0134 - val_loss: 0.1143 - val_mean_squared_error: 0.0267\n",
      "Epoch 8/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0665 - mean_squared_error: 0.0114\n",
      "Epoch 8: val_loss improved from 0.10198 to 0.09496, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0672 - mean_squared_error: 0.0116 - val_loss: 0.0950 - val_mean_squared_error: 0.0214\n",
      "Epoch 9/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0596 - mean_squared_error: 0.0093\n",
      "Epoch 9: val_loss did not improve from 0.09496\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0603 - mean_squared_error: 0.0095 - val_loss: 0.1000 - val_mean_squared_error: 0.0222\n",
      "Epoch 10/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0570 - mean_squared_error: 0.0089\n",
      "Epoch 10: val_loss improved from 0.09496 to 0.09409, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0570 - mean_squared_error: 0.0089 - val_loss: 0.0941 - val_mean_squared_error: 0.0213\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0561 - mean_squared_error: 0.0085\n",
      "Epoch 11: val_loss did not improve from 0.09409\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0561 - mean_squared_error: 0.0085 - val_loss: 0.1002 - val_mean_squared_error: 0.0230\n",
      "Epoch 12/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0080\n",
      "Epoch 12: val_loss did not improve from 0.09409\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0534 - mean_squared_error: 0.0079 - val_loss: 0.0966 - val_mean_squared_error: 0.0216\n",
      "Epoch 13/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0512 - mean_squared_error: 0.0074\n",
      "Epoch 13: val_loss improved from 0.09409 to 0.09295, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0508 - mean_squared_error: 0.0073 - val_loss: 0.0930 - val_mean_squared_error: 0.0203\n",
      "Epoch 14/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0467 - mean_squared_error: 0.0063\n",
      "Epoch 14: val_loss did not improve from 0.09295\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0474 - mean_squared_error: 0.0066 - val_loss: 0.0983 - val_mean_squared_error: 0.0213\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0489 - mean_squared_error: 0.0069\n",
      "Epoch 15: val_loss improved from 0.09295 to 0.09198, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0489 - mean_squared_error: 0.0069 - val_loss: 0.0920 - val_mean_squared_error: 0.0195\n",
      "Epoch 16/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0061\n",
      "Epoch 16: val_loss did not improve from 0.09198\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0450 - mean_squared_error: 0.0061 - val_loss: 0.0934 - val_mean_squared_error: 0.0198\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0430 - mean_squared_error: 0.0058\n",
      "Epoch 17: val_loss did not improve from 0.09198\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0430 - mean_squared_error: 0.0058 - val_loss: 0.0921 - val_mean_squared_error: 0.0199\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0421 - mean_squared_error: 0.0055\n",
      "Epoch 18: val_loss did not improve from 0.09198\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0421 - mean_squared_error: 0.0055 - val_loss: 0.0940 - val_mean_squared_error: 0.0219\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1765 - mean_squared_error: 0.0694\n",
      "Epoch 1: val_loss improved from inf to 0.21375, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 2s 20ms/step - loss: 0.1757 - mean_squared_error: 0.0690 - val_loss: 0.2137 - val_mean_squared_error: 0.0984\n",
      "Epoch 2/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1330 - mean_squared_error: 0.0421\n",
      "Epoch 2: val_loss did not improve from 0.21375\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.1316 - mean_squared_error: 0.0412 - val_loss: 0.2582 - val_mean_squared_error: 0.1054\n",
      "Epoch 3/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1026 - mean_squared_error: 0.0242\n",
      "Epoch 3: val_loss improved from 0.21375 to 0.10267, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.1031 - mean_squared_error: 0.0246 - val_loss: 0.1027 - val_mean_squared_error: 0.0242\n",
      "Epoch 4/100\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.0198\n",
      "Epoch 4: val_loss did not improve from 0.10267\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0900 - mean_squared_error: 0.0195 - val_loss: 0.1242 - val_mean_squared_error: 0.0318\n",
      "Epoch 5/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0809 - mean_squared_error: 0.0162\n",
      "Epoch 5: val_loss did not improve from 0.10267\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0816 - mean_squared_error: 0.0163 - val_loss: 0.1128 - val_mean_squared_error: 0.0270\n",
      "Epoch 6/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0747 - mean_squared_error: 0.0143\n",
      "Epoch 6: val_loss improved from 0.10267 to 0.09098, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0750 - mean_squared_error: 0.0143 - val_loss: 0.0910 - val_mean_squared_error: 0.0203\n",
      "Epoch 7/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0735 - mean_squared_error: 0.0137\n",
      "Epoch 7: val_loss did not improve from 0.09098\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0737 - mean_squared_error: 0.0138 - val_loss: 0.1070 - val_mean_squared_error: 0.0249\n",
      "Epoch 8/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0669 - mean_squared_error: 0.0118\n",
      "Epoch 8: val_loss did not improve from 0.09098\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0673 - mean_squared_error: 0.0120 - val_loss: 0.1201 - val_mean_squared_error: 0.0297\n",
      "Epoch 9/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0655 - mean_squared_error: 0.0113\n",
      "Epoch 9: val_loss did not improve from 0.09098\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0651 - mean_squared_error: 0.0112 - val_loss: 0.0972 - val_mean_squared_error: 0.0239\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.10, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1797 - mean_squared_error: 0.0727\n",
      "Epoch 1: val_loss improved from inf to 0.20370, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 2s 20ms/step - loss: 0.1797 - mean_squared_error: 0.0727 - val_loss: 0.2037 - val_mean_squared_error: 0.0956\n",
      "Epoch 2/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.1415 - mean_squared_error: 0.0499\n",
      "Epoch 2: val_loss improved from 0.20370 to 0.17902, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.1404 - mean_squared_error: 0.0489 - val_loss: 0.1790 - val_mean_squared_error: 0.0748\n",
      "Epoch 3/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1116 - mean_squared_error: 0.0296\n",
      "Epoch 3: val_loss improved from 0.17902 to 0.11344, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.1115 - mean_squared_error: 0.0295 - val_loss: 0.1134 - val_mean_squared_error: 0.0268\n",
      "Epoch 4/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0939 - mean_squared_error: 0.0217\n",
      "Epoch 4: val_loss did not improve from 0.11344\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0925 - mean_squared_error: 0.0210 - val_loss: 0.1142 - val_mean_squared_error: 0.0264\n",
      "Epoch 5/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0851 - mean_squared_error: 0.0179\n",
      "Epoch 5: val_loss did not improve from 0.11344\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0851 - mean_squared_error: 0.0176 - val_loss: 0.1325 - val_mean_squared_error: 0.0365\n",
      "Epoch 6/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0792 - mean_squared_error: 0.0162\n",
      "Epoch 6: val_loss improved from 0.11344 to 0.10106, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0788 - mean_squared_error: 0.0160 - val_loss: 0.1011 - val_mean_squared_error: 0.0230\n",
      "Epoch 7/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0736 - mean_squared_error: 0.0144\n",
      "Epoch 7: val_loss improved from 0.10106 to 0.09434, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0734 - mean_squared_error: 0.0144 - val_loss: 0.0943 - val_mean_squared_error: 0.0201\n",
      "Epoch 8/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0665 - mean_squared_error: 0.0122\n",
      "Epoch 8: val_loss improved from 0.09434 to 0.08764, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c64filt17str1,layer2:c64filt19str4,1conv,dropout0,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.0664 - mean_squared_error: 0.0121 - val_loss: 0.0876 - val_mean_squared_error: 0.0185\n",
      "Epoch 9/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0689 - mean_squared_error: 0.0124\n",
      "Epoch 9: val_loss did not improve from 0.08764\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0687 - mean_squared_error: 0.0123 - val_loss: 0.0924 - val_mean_squared_error: 0.0212\n",
      "Epoch 10/100\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0621 - mean_squared_error: 0.0107\n",
      "Epoch 10: val_loss did not improve from 0.08764\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0617 - mean_squared_error: 0.0106 - val_loss: 0.0930 - val_mean_squared_error: 0.0194\n",
      "Epoch 11/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0556 - mean_squared_error: 0.0089\n",
      "Epoch 11: val_loss did not improve from 0.08764\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0561 - mean_squared_error: 0.0091 - val_loss: 0.0898 - val_mean_squared_error: 0.0187\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.10, val rmse 0.15###\n",
      "mae0.95+-0.02_rmse1.45+-0.03\n",
      "random search 8/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2093 - mean_squared_error: 0.0769\n",
      "Epoch 1: val_loss improved from inf to 0.14767, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_0.hdf5\n",
      "6/6 [==============================] - 3s 213ms/step - loss: 0.2093 - mean_squared_error: 0.0769 - val_loss: 0.1477 - val_mean_squared_error: 0.0397\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1490 - mean_squared_error: 0.0517\n",
      "Epoch 2: val_loss improved from 0.14767 to 0.13243, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_0.hdf5\n",
      "6/6 [==============================] - 0s 88ms/step - loss: 0.1474 - mean_squared_error: 0.0510 - val_loss: 0.1324 - val_mean_squared_error: 0.0311\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1364 - mean_squared_error: 0.0450\n",
      "Epoch 3: val_loss did not improve from 0.13243\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.1333 - mean_squared_error: 0.0436 - val_loss: 0.2525 - val_mean_squared_error: 0.0915\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1145 - mean_squared_error: 0.0321\n",
      "Epoch 4: val_loss did not improve from 0.13243\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.1162 - mean_squared_error: 0.0328 - val_loss: 0.4779 - val_mean_squared_error: 0.2613\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1114 - mean_squared_error: 0.0305\n",
      "Epoch 5: val_loss did not improve from 0.13243\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 0.1110 - mean_squared_error: 0.0301 - val_loss: 0.5565 - val_mean_squared_error: 0.3397\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.14, val rmse 0.18###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2259 - mean_squared_error: 0.0936\n",
      "Epoch 1: val_loss improved from inf to 0.15023, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_1.hdf5\n",
      "6/6 [==============================] - 3s 149ms/step - loss: 0.2183 - mean_squared_error: 0.0888 - val_loss: 0.1502 - val_mean_squared_error: 0.0400\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1498 - mean_squared_error: 0.0512\n",
      "Epoch 2: val_loss improved from 0.15023 to 0.13662, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_1.hdf5\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.1483 - mean_squared_error: 0.0505 - val_loss: 0.1366 - val_mean_squared_error: 0.0408\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1426 - mean_squared_error: 0.0490\n",
      "Epoch 3: val_loss did not improve from 0.13662\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 0.1408 - mean_squared_error: 0.0482 - val_loss: 0.1584 - val_mean_squared_error: 0.0445\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1296 - mean_squared_error: 0.0420\n",
      "Epoch 4: val_loss did not improve from 0.13662\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 0.1308 - mean_squared_error: 0.0423 - val_loss: 0.2951 - val_mean_squared_error: 0.1212\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1156 - mean_squared_error: 0.0326\n",
      "Epoch 5: val_loss did not improve from 0.13662\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1148 - mean_squared_error: 0.0322 - val_loss: 0.5282 - val_mean_squared_error: 0.3124\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###1 fold : val mae 0.14, val rmse 0.21###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2349 - mean_squared_error: 0.1009\n",
      "Epoch 1: val_loss improved from inf to 0.15434, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_2.hdf5\n",
      "6/6 [==============================] - 3s 154ms/step - loss: 0.2269 - mean_squared_error: 0.0957 - val_loss: 0.1543 - val_mean_squared_error: 0.0378\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1499 - mean_squared_error: 0.0497\n",
      "Epoch 2: val_loss improved from 0.15434 to 0.13083, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_2.hdf5\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 0.1508 - mean_squared_error: 0.0502 - val_loss: 0.1308 - val_mean_squared_error: 0.0330\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1479 - mean_squared_error: 0.0511\n",
      "Epoch 3: val_loss did not improve from 0.13083\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 0.1463 - mean_squared_error: 0.0504 - val_loss: 0.1515 - val_mean_squared_error: 0.0399\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1323 - mean_squared_error: 0.0419\n",
      "Epoch 4: val_loss did not improve from 0.13083\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.1329 - mean_squared_error: 0.0417 - val_loss: 0.2279 - val_mean_squared_error: 0.0783\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1185 - mean_squared_error: 0.0328\n",
      "Epoch 5: val_loss did not improve from 0.13083\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.1184 - mean_squared_error: 0.0330 - val_loss: 0.3926 - val_mean_squared_error: 0.1893\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.14, val rmse 0.20###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2135 - mean_squared_error: 0.0826\n",
      "Epoch 1: val_loss improved from inf to 0.14066, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c16filt7str2,layer2:c128filt11str1,layer3:c16filt11str2,layer4:c128filt9str4,1conv,dropout0.4,dnodes16,dropout0.2/weights_3.hdf5\n",
      "6/6 [==============================] - 3s 168ms/step - loss: 0.2135 - mean_squared_error: 0.0826 - val_loss: 0.1407 - val_mean_squared_error: 0.0373\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1401 - mean_squared_error: 0.0454\n",
      "Epoch 2: val_loss did not improve from 0.14066\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.1396 - mean_squared_error: 0.0452 - val_loss: 0.1426 - val_mean_squared_error: 0.0331\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1272 - mean_squared_error: 0.0390\n",
      "Epoch 3: val_loss did not improve from 0.14066\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 0.1253 - mean_squared_error: 0.0382 - val_loss: 0.1887 - val_mean_squared_error: 0.0532\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1146 - mean_squared_error: 0.0323\n",
      "Epoch 4: val_loss did not improve from 0.14066\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 0.1141 - mean_squared_error: 0.0320 - val_loss: 0.4664 - val_mean_squared_error: 0.2504\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.15, val rmse 0.20###\n",
      "mae1.44+-0.05_rmse1.99+-0.10\n",
      "random search 9/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2682 - mean_squared_error: 0.1463\n",
      "Epoch 1: val_loss improved from inf to 0.15919, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_0.hdf5\n",
      "23/23 [==============================] - 2s 40ms/step - loss: 0.2651 - mean_squared_error: 0.1436 - val_loss: 0.1592 - val_mean_squared_error: 0.0595\n",
      "Epoch 2/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1607 - mean_squared_error: 0.0610\n",
      "Epoch 2: val_loss improved from 0.15919 to 0.15759, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.1600 - mean_squared_error: 0.0607 - val_loss: 0.1576 - val_mean_squared_error: 0.0594\n",
      "Epoch 3/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1596 - mean_squared_error: 0.0603\n",
      "Epoch 3: val_loss improved from 0.15759 to 0.15717, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1594 - mean_squared_error: 0.0604 - val_loss: 0.1572 - val_mean_squared_error: 0.0593\n",
      "Epoch 4/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1591 - mean_squared_error: 0.0604\n",
      "Epoch 4: val_loss improved from 0.15717 to 0.15300, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 0.1593 - mean_squared_error: 0.0605 - val_loss: 0.1530 - val_mean_squared_error: 0.0573\n",
      "Epoch 5/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1534 - mean_squared_error: 0.0571\n",
      "Epoch 5: val_loss did not improve from 0.15300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1542 - mean_squared_error: 0.0574 - val_loss: 0.1618 - val_mean_squared_error: 0.0600\n",
      "Epoch 6/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1361 - mean_squared_error: 0.0456\n",
      "Epoch 6: val_loss did not improve from 0.15300\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.1361 - mean_squared_error: 0.0455 - val_loss: 0.1953 - val_mean_squared_error: 0.0764\n",
      "Epoch 7/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1194 - mean_squared_error: 0.0336\n",
      "Epoch 7: val_loss did not improve from 0.15300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1187 - mean_squared_error: 0.0331 - val_loss: 0.1854 - val_mean_squared_error: 0.0650\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.16, val rmse 0.25###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2761 - mean_squared_error: 0.1502\n",
      "Epoch 1: val_loss improved from inf to 0.16477, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_1.hdf5\n",
      "23/23 [==============================] - 2s 39ms/step - loss: 0.2682 - mean_squared_error: 0.1437 - val_loss: 0.1648 - val_mean_squared_error: 0.0633\n",
      "Epoch 2/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1599 - mean_squared_error: 0.0611\n",
      "Epoch 2: val_loss did not improve from 0.16477\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.1594 - mean_squared_error: 0.0609 - val_loss: 0.1652 - val_mean_squared_error: 0.0637\n",
      "Epoch 3/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1592 - mean_squared_error: 0.0606\n",
      "Epoch 3: val_loss did not improve from 0.16477\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1593 - mean_squared_error: 0.0608 - val_loss: 0.1652 - val_mean_squared_error: 0.0637\n",
      "Epoch 4/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1586 - mean_squared_error: 0.0605\n",
      "Epoch 4: val_loss did not improve from 0.16477\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.1593 - mean_squared_error: 0.0608 - val_loss: 0.1652 - val_mean_squared_error: 0.0637\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3182 - mean_squared_error: 0.1946\n",
      "Epoch 1: val_loss improved from inf to 0.15313, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_2.hdf5\n",
      "23/23 [==============================] - 2s 39ms/step - loss: 0.3125 - mean_squared_error: 0.1898 - val_loss: 0.1531 - val_mean_squared_error: 0.0574\n",
      "Epoch 2/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1632 - mean_squared_error: 0.0622\n",
      "Epoch 2: val_loss did not improve from 0.15313\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1627 - mean_squared_error: 0.0620 - val_loss: 0.1536 - val_mean_squared_error: 0.0585\n",
      "Epoch 3/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0624\n",
      "Epoch 3: val_loss did not improve from 0.15313\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1539 - val_mean_squared_error: 0.0586\n",
      "Epoch 4/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1628 - mean_squared_error: 0.0626\n",
      "Epoch 4: val_loss did not improve from 0.15313\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1632 - mean_squared_error: 0.0626 - val_loss: 0.1539 - val_mean_squared_error: 0.0586\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.25###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.1362\n",
      "Epoch 1: val_loss improved from inf to 0.16147, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_3.hdf5\n",
      "23/23 [==============================] - 2s 41ms/step - loss: 0.2571 - mean_squared_error: 0.1327 - val_loss: 0.1615 - val_mean_squared_error: 0.0607\n",
      "Epoch 2/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1601 - mean_squared_error: 0.0613\n",
      "Epoch 2: val_loss did not improve from 0.16147\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 0.1597 - mean_squared_error: 0.0611 - val_loss: 0.1621 - val_mean_squared_error: 0.0613\n",
      "Epoch 3/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1599 - mean_squared_error: 0.0611\n",
      "Epoch 3: val_loss improved from 0.16147 to 0.16063, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_3.hdf5\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1597 - mean_squared_error: 0.0610 - val_loss: 0.1606 - val_mean_squared_error: 0.0606\n",
      "Epoch 4/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1561 - mean_squared_error: 0.0582\n",
      "Epoch 4: val_loss did not improve from 0.16063\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.1557 - mean_squared_error: 0.0582 - val_loss: 0.1690 - val_mean_squared_error: 0.0650\n",
      "Epoch 5/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1487 - mean_squared_error: 0.0532\n",
      "Epoch 5: val_loss improved from 0.16063 to 0.15207, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_3.hdf5\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1484 - mean_squared_error: 0.0527 - val_loss: 0.1521 - val_mean_squared_error: 0.0504\n",
      "Epoch 6/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1301 - mean_squared_error: 0.0401\n",
      "Epoch 6: val_loss improved from 0.15207 to 0.12326, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c16filt9str4,layer2:c16filt17str1,layer3:c128filt15str4,1conv,dropout0.1,dnodes0,dropout0.3/weights_3.hdf5\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.1297 - mean_squared_error: 0.0400 - val_loss: 0.1233 - val_mean_squared_error: 0.0340\n",
      "Epoch 7/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1222 - mean_squared_error: 0.0348\n",
      "Epoch 7: val_loss did not improve from 0.12326\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1225 - mean_squared_error: 0.0350 - val_loss: 0.1303 - val_mean_squared_error: 0.0350\n",
      "Epoch 8/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1117 - mean_squared_error: 0.0295\n",
      "Epoch 8: val_loss did not improve from 0.12326\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 0.1114 - mean_squared_error: 0.0294 - val_loss: 0.1643 - val_mean_squared_error: 0.0513\n",
      "Epoch 9/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1049 - mean_squared_error: 0.0263\n",
      "Epoch 9: val_loss did not improve from 0.12326\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1043 - mean_squared_error: 0.0260 - val_loss: 0.2314 - val_mean_squared_error: 0.0912\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.13, val rmse 0.19###\n",
      "mae1.59+-0.16_rmse2.38+-0.27\n",
      "random search 10/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1964 - mean_squared_error: 0.0879\n",
      "Epoch 1: val_loss improved from inf to 0.16151, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_0.hdf5\n",
      "23/23 [==============================] - 3s 34ms/step - loss: 0.1939 - mean_squared_error: 0.0859 - val_loss: 0.1615 - val_mean_squared_error: 0.0622\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1602 - mean_squared_error: 0.0611\n",
      "Epoch 2: val_loss improved from 0.16151 to 0.16137, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1602 - mean_squared_error: 0.0611 - val_loss: 0.1614 - val_mean_squared_error: 0.0621\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1598 - mean_squared_error: 0.0608\n",
      "Epoch 3: val_loss improved from 0.16137 to 0.16086, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1598 - mean_squared_error: 0.0608 - val_loss: 0.1609 - val_mean_squared_error: 0.0619\n",
      "Epoch 4/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1571 - mean_squared_error: 0.0588\n",
      "Epoch 4: val_loss improved from 0.16086 to 0.14935, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_0.hdf5\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1571 - mean_squared_error: 0.0587 - val_loss: 0.1493 - val_mean_squared_error: 0.0534\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1392 - mean_squared_error: 0.0447\n",
      "Epoch 5: val_loss improved from 0.14935 to 0.14790, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1392 - mean_squared_error: 0.0447 - val_loss: 0.1479 - val_mean_squared_error: 0.0455\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1188 - mean_squared_error: 0.0313\n",
      "Epoch 6: val_loss did not improve from 0.14790\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1188 - mean_squared_error: 0.0313 - val_loss: 0.2395 - val_mean_squared_error: 0.1025\n",
      "Epoch 7/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1110 - mean_squared_error: 0.0284\n",
      "Epoch 7: val_loss did not improve from 0.14790\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.1107 - mean_squared_error: 0.0283 - val_loss: 0.3987 - val_mean_squared_error: 0.2184\n",
      "Epoch 8/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1038 - mean_squared_error: 0.0249\n",
      "Epoch 8: val_loss did not improve from 0.14790\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1042 - mean_squared_error: 0.0251 - val_loss: 0.4514 - val_mean_squared_error: 0.2606\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###0 fold : val mae 0.15, val rmse 0.22###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1955 - mean_squared_error: 0.0875\n",
      "Epoch 1: val_loss improved from inf to 0.16521, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_1.hdf5\n",
      "23/23 [==============================] - 2s 32ms/step - loss: 0.1911 - mean_squared_error: 0.0839 - val_loss: 0.1652 - val_mean_squared_error: 0.0637\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1588 - mean_squared_error: 0.0605\n",
      "Epoch 2: val_loss improved from 0.16521 to 0.16506, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_1.hdf5\n",
      "23/23 [==============================] - 1s 37ms/step - loss: 0.1588 - mean_squared_error: 0.0605 - val_loss: 0.1651 - val_mean_squared_error: 0.0636\n",
      "Epoch 3/100\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1601 - mean_squared_error: 0.0610\n",
      "Epoch 3: val_loss improved from 0.16506 to 0.15989, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1571 - mean_squared_error: 0.0593 - val_loss: 0.1599 - val_mean_squared_error: 0.0595\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1441 - mean_squared_error: 0.0491\n",
      "Epoch 4: val_loss did not improve from 0.15989\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1441 - mean_squared_error: 0.0491 - val_loss: 0.1630 - val_mean_squared_error: 0.0606\n",
      "Epoch 5/100\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1242 - mean_squared_error: 0.0339\n",
      "Epoch 5: val_loss did not improve from 0.15989\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 0.1236 - mean_squared_error: 0.0333 - val_loss: 0.2106 - val_mean_squared_error: 0.0798\n",
      "Epoch 6/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1114 - mean_squared_error: 0.0283\n",
      "Epoch 6: val_loss did not improve from 0.15989\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1107 - mean_squared_error: 0.0280 - val_loss: 0.4160 - val_mean_squared_error: 0.2423\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.25###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2078 - mean_squared_error: 0.0952\n",
      "Epoch 1: val_loss improved from inf to 0.15391, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_2.hdf5\n",
      "23/23 [==============================] - 2s 32ms/step - loss: 0.2007 - mean_squared_error: 0.0902 - val_loss: 0.1539 - val_mean_squared_error: 0.0587\n",
      "Epoch 2/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1617 - mean_squared_error: 0.0617\n",
      "Epoch 2: val_loss improved from 0.15391 to 0.15292, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_2.hdf5\n",
      "23/23 [==============================] - 1s 39ms/step - loss: 0.1622 - mean_squared_error: 0.0619 - val_loss: 0.1529 - val_mean_squared_error: 0.0581\n",
      "Epoch 3/100\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1573 - mean_squared_error: 0.0582\n",
      "Epoch 3: val_loss improved from 0.15292 to 0.14145, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_2.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1545 - mean_squared_error: 0.0563 - val_loss: 0.1415 - val_mean_squared_error: 0.0475\n",
      "Epoch 4/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1351 - mean_squared_error: 0.0407\n",
      "Epoch 4: val_loss did not improve from 0.14145\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1340 - mean_squared_error: 0.0400 - val_loss: 0.2291 - val_mean_squared_error: 0.1003\n",
      "Epoch 5/100\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1134 - mean_squared_error: 0.0283\n",
      "Epoch 5: val_loss did not improve from 0.14145\n",
      "23/23 [==============================] - 1s 36ms/step - loss: 0.1130 - mean_squared_error: 0.0283 - val_loss: 0.4579 - val_mean_squared_error: 0.2831\n",
      "Epoch 6/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1061 - mean_squared_error: 0.0257\n",
      "Epoch 6: val_loss did not improve from 0.14145\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1061 - mean_squared_error: 0.0258 - val_loss: 0.4725 - val_mean_squared_error: 0.2909\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.16, val rmse 0.24###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2075 - mean_squared_error: 0.0963\n",
      "Epoch 1: val_loss improved from inf to 0.16253, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_3.hdf5\n",
      "23/23 [==============================] - 2s 34ms/step - loss: 0.2040 - mean_squared_error: 0.0942 - val_loss: 0.1625 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1602 - mean_squared_error: 0.0615\n",
      "Epoch 2: val_loss improved from 0.16253 to 0.16252, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_3.hdf5\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 0.1602 - mean_squared_error: 0.0615 - val_loss: 0.1625 - val_mean_squared_error: 0.0616\n",
      "Epoch 3/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1609 - mean_squared_error: 0.0617\n",
      "Epoch 3: val_loss improved from 0.16252 to 0.16249, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1602 - mean_squared_error: 0.0614 - val_loss: 0.1625 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1600 - mean_squared_error: 0.0612\n",
      "Epoch 4: val_loss improved from 0.16249 to 0.16228, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1599 - mean_squared_error: 0.0613 - val_loss: 0.1623 - val_mean_squared_error: 0.0615\n",
      "Epoch 5/100\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1599 - mean_squared_error: 0.0615\n",
      "Epoch 5: val_loss did not improve from 0.16228\n",
      "23/23 [==============================] - 1s 33ms/step - loss: 0.1597 - mean_squared_error: 0.0611 - val_loss: 0.1624 - val_mean_squared_error: 0.0616\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1595 - mean_squared_error: 0.0611\n",
      "Epoch 6: val_loss improved from 0.16228 to 0.16195, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1595 - mean_squared_error: 0.0611 - val_loss: 0.1620 - val_mean_squared_error: 0.0610\n",
      "Epoch 7/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1481 - mean_squared_error: 0.0528\n",
      "Epoch 7: val_loss improved from 0.16195 to 0.13929, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch128,layer1:c64filt9str1,layer2:c64filt13str4,layer3:c16filt9str2,1conv,dropout0.5,dnodes32,dropout0.2/weights_3.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1470 - mean_squared_error: 0.0521 - val_loss: 0.1393 - val_mean_squared_error: 0.0465\n",
      "Epoch 8/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1251 - mean_squared_error: 0.0343\n",
      "Epoch 8: val_loss did not improve from 0.13929\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 0.1251 - mean_squared_error: 0.0343 - val_loss: 0.3477 - val_mean_squared_error: 0.1827\n",
      "Epoch 9/100\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1137 - mean_squared_error: 0.0290\n",
      "Epoch 9: val_loss did not improve from 0.13929\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1124 - mean_squared_error: 0.0284 - val_loss: 0.3589 - val_mean_squared_error: 0.1884\n",
      "Epoch 10/100\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1048 - mean_squared_error: 0.0248\n",
      "Epoch 10: val_loss did not improve from 0.13929\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1047 - mean_squared_error: 0.0249 - val_loss: 0.4036 - val_mean_squared_error: 0.2303\n",
      "31/31 [==============================] - 1s 3ms/step\n",
      " ###3 fold : val mae 0.15, val rmse 0.22###\n",
      "mae1.58+-0.07_rmse2.35+-0.12\n",
      "random search 11/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3656 - mean_squared_error: 0.2079\n",
      "Epoch 1: val_loss improved from inf to 0.19889, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_0.hdf5\n",
      "6/6 [==============================] - 2s 150ms/step - loss: 0.3656 - mean_squared_error: 0.2079 - val_loss: 0.1989 - val_mean_squared_error: 0.0579\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2042 - mean_squared_error: 0.0796\n",
      "Epoch 2: val_loss improved from 0.19889 to 0.15504, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_0.hdf5\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.1973 - mean_squared_error: 0.0760 - val_loss: 0.1550 - val_mean_squared_error: 0.0527\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.0596\n",
      "Epoch 3: val_loss improved from 0.15504 to 0.15351, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_0.hdf5\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.1587 - mean_squared_error: 0.0589 - val_loss: 0.1535 - val_mean_squared_error: 0.0546\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1589 - mean_squared_error: 0.0601\n",
      "Epoch 4: val_loss improved from 0.15351 to 0.14885, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_0.hdf5\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.1592 - mean_squared_error: 0.0604 - val_loss: 0.1489 - val_mean_squared_error: 0.0518\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1590 - mean_squared_error: 0.0604\n",
      "Epoch 5: val_loss improved from 0.14885 to 0.14247, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_0.hdf5\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.1595 - mean_squared_error: 0.0604 - val_loss: 0.1425 - val_mean_squared_error: 0.0477\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1585 - mean_squared_error: 0.0600\n",
      "Epoch 6: val_loss did not improve from 0.14247\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 0.1581 - mean_squared_error: 0.0598 - val_loss: 0.1572 - val_mean_squared_error: 0.0568\n",
      "Epoch 7/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1578 - mean_squared_error: 0.0600\n",
      "Epoch 7: val_loss did not improve from 0.14247\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 0.1559 - mean_squared_error: 0.0588 - val_loss: 0.1931 - val_mean_squared_error: 0.0830\n",
      "Epoch 8/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1482 - mean_squared_error: 0.0542\n",
      "Epoch 8: val_loss did not improve from 0.14247\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.1489 - mean_squared_error: 0.0543 - val_loss: 0.1976 - val_mean_squared_error: 0.0869\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.15, val rmse 0.22###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3586 - mean_squared_error: 0.1936\n",
      "Epoch 1: val_loss improved from inf to 0.19324, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_1.hdf5\n",
      "6/6 [==============================] - 2s 155ms/step - loss: 0.3455 - mean_squared_error: 0.1836 - val_loss: 0.1932 - val_mean_squared_error: 0.0557\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1817 - mean_squared_error: 0.0647\n",
      "Epoch 2: val_loss improved from 0.19324 to 0.14631, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_1.hdf5\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 0.1774 - mean_squared_error: 0.0628 - val_loss: 0.1463 - val_mean_squared_error: 0.0420\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1502 - mean_squared_error: 0.0530\n",
      "Epoch 3: val_loss did not improve from 0.14631\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 0.1487 - mean_squared_error: 0.0525 - val_loss: 0.2069 - val_mean_squared_error: 0.0784\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1437 - mean_squared_error: 0.0509\n",
      "Epoch 4: val_loss did not improve from 0.14631\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 0.1439 - mean_squared_error: 0.0511 - val_loss: 0.1735 - val_mean_squared_error: 0.0602\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1374 - mean_squared_error: 0.0463\n",
      "Epoch 5: val_loss did not improve from 0.14631\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.1364 - mean_squared_error: 0.0457 - val_loss: 0.2621 - val_mean_squared_error: 0.1256\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.14, val rmse 0.20###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3555 - mean_squared_error: 0.1896\n",
      "Epoch 1: val_loss improved from inf to 0.18189, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_2.hdf5\n",
      "6/6 [==============================] - 2s 151ms/step - loss: 0.3467 - mean_squared_error: 0.1832 - val_loss: 0.1819 - val_mean_squared_error: 0.0513\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1876 - mean_squared_error: 0.0703\n",
      "Epoch 2: val_loss improved from 0.18189 to 0.14420, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_2.hdf5\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 0.1850 - mean_squared_error: 0.0690 - val_loss: 0.1442 - val_mean_squared_error: 0.0482\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1576 - mean_squared_error: 0.0576\n",
      "Epoch 3: val_loss improved from 0.14420 to 0.13524, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_2.hdf5\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.1572 - mean_squared_error: 0.0574 - val_loss: 0.1352 - val_mean_squared_error: 0.0433\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1512 - mean_squared_error: 0.0544\n",
      "Epoch 4: val_loss did not improve from 0.13524\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.1508 - mean_squared_error: 0.0538 - val_loss: 0.1485 - val_mean_squared_error: 0.0512\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1449 - mean_squared_error: 0.0501\n",
      "Epoch 5: val_loss did not improve from 0.13524\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 0.1439 - mean_squared_error: 0.0497 - val_loss: 0.1715 - val_mean_squared_error: 0.0662\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1351 - mean_squared_error: 0.0445\n",
      "Epoch 6: val_loss did not improve from 0.13524\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.1342 - mean_squared_error: 0.0438 - val_loss: 0.2090 - val_mean_squared_error: 0.0901\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###2 fold : val mae 0.15, val rmse 0.22###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3477 - mean_squared_error: 0.1880\n",
      "Epoch 1: val_loss improved from inf to 0.18073, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_3.hdf5\n",
      "6/6 [==============================] - 3s 175ms/step - loss: 0.3477 - mean_squared_error: 0.1880 - val_loss: 0.1807 - val_mean_squared_error: 0.0494\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1757 - mean_squared_error: 0.0619\n",
      "Epoch 2: val_loss improved from 0.18073 to 0.14852, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_3.hdf5\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 0.1733 - mean_squared_error: 0.0611 - val_loss: 0.1485 - val_mean_squared_error: 0.0488\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1568 - mean_squared_error: 0.0580\n",
      "Epoch 3: val_loss improved from 0.14852 to 0.13708, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c32filt15str1,layer2:c32filt9str4,layer3:c128filt19str4,1conv,dropout0,dnodes0,dropout0.3/weights_3.hdf5\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.1550 - mean_squared_error: 0.0573 - val_loss: 0.1371 - val_mean_squared_error: 0.0438\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1490 - mean_squared_error: 0.0541\n",
      "Epoch 4: val_loss did not improve from 0.13708\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 0.1499 - mean_squared_error: 0.0544 - val_loss: 0.1479 - val_mean_squared_error: 0.0496\n",
      "Epoch 5/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1477 - mean_squared_error: 0.0538\n",
      "Epoch 5: val_loss did not improve from 0.13708\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 0.1474 - mean_squared_error: 0.0534 - val_loss: 0.1832 - val_mean_squared_error: 0.0730\n",
      "Epoch 6/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1411 - mean_squared_error: 0.0486\n",
      "Epoch 6: val_loss did not improve from 0.13708\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.1399 - mean_squared_error: 0.0479 - val_loss: 0.1460 - val_mean_squared_error: 0.0479\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.15, val rmse 0.22###\n",
      "mae1.47+-0.02_rmse2.17+-0.09\n",
      "random search 12/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1663 - mean_squared_error: 0.0632\n",
      "Epoch 1: val_loss improved from inf to 0.16134, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 3s 19ms/step - loss: 0.1663 - mean_squared_error: 0.0632 - val_loss: 0.1613 - val_mean_squared_error: 0.0620\n",
      "Epoch 2/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1525 - mean_squared_error: 0.0556\n",
      "Epoch 2: val_loss did not improve from 0.16134\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1519 - mean_squared_error: 0.0551 - val_loss: 0.2520 - val_mean_squared_error: 0.1134\n",
      "Epoch 3/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1261 - mean_squared_error: 0.0366\n",
      "Epoch 3: val_loss improved from 0.16134 to 0.12209, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1259 - mean_squared_error: 0.0365 - val_loss: 0.1221 - val_mean_squared_error: 0.0334\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1051 - mean_squared_error: 0.0264\n",
      "Epoch 4: val_loss improved from 0.12209 to 0.11116, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1051 - mean_squared_error: 0.0264 - val_loss: 0.1112 - val_mean_squared_error: 0.0268\n",
      "Epoch 5/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0932 - mean_squared_error: 0.0208\n",
      "Epoch 5: val_loss improved from 0.11116 to 0.09455, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0931 - mean_squared_error: 0.0207 - val_loss: 0.0945 - val_mean_squared_error: 0.0235\n",
      "Epoch 6/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0838 - mean_squared_error: 0.0174\n",
      "Epoch 6: val_loss did not improve from 0.09455\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0837 - mean_squared_error: 0.0174 - val_loss: 0.1067 - val_mean_squared_error: 0.0252\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0806 - mean_squared_error: 0.0162\n",
      "Epoch 7: val_loss improved from 0.09455 to 0.08964, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0806 - mean_squared_error: 0.0162 - val_loss: 0.0896 - val_mean_squared_error: 0.0200\n",
      "Epoch 8/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.0145\n",
      "Epoch 8: val_loss improved from 0.08964 to 0.08823, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0767 - mean_squared_error: 0.0147 - val_loss: 0.0882 - val_mean_squared_error: 0.0191\n",
      "Epoch 9/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.0713 - mean_squared_error: 0.0130\n",
      "Epoch 9: val_loss improved from 0.08823 to 0.08615, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0710 - mean_squared_error: 0.0129 - val_loss: 0.0861 - val_mean_squared_error: 0.0195\n",
      "Epoch 10/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.0715 - mean_squared_error: 0.0132\n",
      "Epoch 10: val_loss improved from 0.08615 to 0.08446, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0713 - mean_squared_error: 0.0131 - val_loss: 0.0845 - val_mean_squared_error: 0.0190\n",
      "Epoch 11/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0698 - mean_squared_error: 0.0126\n",
      "Epoch 11: val_loss did not improve from 0.08446\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0698 - mean_squared_error: 0.0125 - val_loss: 0.0933 - val_mean_squared_error: 0.0198\n",
      "Epoch 12/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0627 - mean_squared_error: 0.0105\n",
      "Epoch 12: val_loss did not improve from 0.08446\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0626 - mean_squared_error: 0.0105 - val_loss: 0.0987 - val_mean_squared_error: 0.0216\n",
      "Epoch 13/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0607 - mean_squared_error: 0.0100\n",
      "Epoch 13: val_loss did not improve from 0.08446\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0606 - mean_squared_error: 0.0100 - val_loss: 0.0982 - val_mean_squared_error: 0.0211\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.09, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1574 - mean_squared_error: 0.0579\n",
      "Epoch 1: val_loss improved from inf to 0.25971, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 4s 20ms/step - loss: 0.1574 - mean_squared_error: 0.0579 - val_loss: 0.2597 - val_mean_squared_error: 0.1200\n",
      "Epoch 2/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.1140 - mean_squared_error: 0.0306\n",
      "Epoch 2: val_loss improved from 0.25971 to 0.12840, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.1130 - mean_squared_error: 0.0301 - val_loss: 0.1284 - val_mean_squared_error: 0.0361\n",
      "Epoch 3/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.0949 - mean_squared_error: 0.0212\n",
      "Epoch 3: val_loss improved from 0.12840 to 0.11514, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0948 - mean_squared_error: 0.0213 - val_loss: 0.1151 - val_mean_squared_error: 0.0325\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0874 - mean_squared_error: 0.0188\n",
      "Epoch 4: val_loss improved from 0.11514 to 0.09544, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0875 - mean_squared_error: 0.0188 - val_loss: 0.0954 - val_mean_squared_error: 0.0222\n",
      "Epoch 5/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.0811 - mean_squared_error: 0.0160\n",
      "Epoch 5: val_loss did not improve from 0.09544\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0809 - mean_squared_error: 0.0160 - val_loss: 0.0969 - val_mean_squared_error: 0.0235\n",
      "Epoch 6/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0751 - mean_squared_error: 0.0142\n",
      "Epoch 6: val_loss improved from 0.09544 to 0.09468, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0752 - mean_squared_error: 0.0144 - val_loss: 0.0947 - val_mean_squared_error: 0.0225\n",
      "Epoch 7/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0738 - mean_squared_error: 0.0135\n",
      "Epoch 7: val_loss did not improve from 0.09468\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0735 - mean_squared_error: 0.0135 - val_loss: 0.0985 - val_mean_squared_error: 0.0229\n",
      "Epoch 8/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0715 - mean_squared_error: 0.0129\n",
      "Epoch 8: val_loss did not improve from 0.09468\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0715 - mean_squared_error: 0.0129 - val_loss: 0.0993 - val_mean_squared_error: 0.0227\n",
      "Epoch 9/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.0664 - mean_squared_error: 0.0115\n",
      "Epoch 9: val_loss did not improve from 0.09468\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0666 - mean_squared_error: 0.0116 - val_loss: 0.1008 - val_mean_squared_error: 0.0240\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.10, val rmse 0.15###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1698 - mean_squared_error: 0.0653\n",
      "Epoch 1: val_loss improved from inf to 0.14941, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1698 - mean_squared_error: 0.0653 - val_loss: 0.1494 - val_mean_squared_error: 0.0521\n",
      "Epoch 2/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.1264 - mean_squared_error: 0.0362\n",
      "Epoch 2: val_loss did not improve from 0.14941\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1257 - mean_squared_error: 0.0359 - val_loss: 0.1642 - val_mean_squared_error: 0.0531\n",
      "Epoch 3/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.1019 - mean_squared_error: 0.0247\n",
      "Epoch 3: val_loss improved from 0.14941 to 0.10569, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1019 - mean_squared_error: 0.0248 - val_loss: 0.1057 - val_mean_squared_error: 0.0277\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0945 - mean_squared_error: 0.0215\n",
      "Epoch 4: val_loss improved from 0.10569 to 0.09583, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0945 - mean_squared_error: 0.0215 - val_loss: 0.0958 - val_mean_squared_error: 0.0224\n",
      "Epoch 5/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.0828 - mean_squared_error: 0.0169\n",
      "Epoch 5: val_loss improved from 0.09583 to 0.08985, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0831 - mean_squared_error: 0.0171 - val_loss: 0.0898 - val_mean_squared_error: 0.0196\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0791 - mean_squared_error: 0.0157\n",
      "Epoch 6: val_loss did not improve from 0.08985\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0791 - mean_squared_error: 0.0157 - val_loss: 0.0967 - val_mean_squared_error: 0.0216\n",
      "Epoch 7/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0734 - mean_squared_error: 0.0142\n",
      "Epoch 7: val_loss improved from 0.08985 to 0.08716, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0733 - mean_squared_error: 0.0142 - val_loss: 0.0872 - val_mean_squared_error: 0.0184\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0696 - mean_squared_error: 0.0128\n",
      "Epoch 8: val_loss did not improve from 0.08716\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0696 - mean_squared_error: 0.0128 - val_loss: 0.1169 - val_mean_squared_error: 0.0283\n",
      "Epoch 9/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.0686 - mean_squared_error: 0.0126\n",
      "Epoch 9: val_loss improved from 0.08716 to 0.08402, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0686 - mean_squared_error: 0.0126 - val_loss: 0.0840 - val_mean_squared_error: 0.0180\n",
      "Epoch 10/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.0637 - mean_squared_error: 0.0113\n",
      "Epoch 10: val_loss did not improve from 0.08402\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0638 - mean_squared_error: 0.0113 - val_loss: 0.0850 - val_mean_squared_error: 0.0183\n",
      "Epoch 11/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.0624 - mean_squared_error: 0.0109\n",
      "Epoch 11: val_loss did not improve from 0.08402\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0622 - mean_squared_error: 0.0108 - val_loss: 0.0955 - val_mean_squared_error: 0.0207\n",
      "Epoch 12/100\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 0.0590 - mean_squared_error: 0.0097\n",
      "Epoch 12: val_loss did not improve from 0.08402\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0592 - mean_squared_error: 0.0099 - val_loss: 0.0877 - val_mean_squared_error: 0.0202\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1496 - mean_squared_error: 0.0503\n",
      "Epoch 1: val_loss improved from inf to 0.43991, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 3s 13ms/step - loss: 0.1494 - mean_squared_error: 0.0502 - val_loss: 0.4399 - val_mean_squared_error: 0.2581\n",
      "Epoch 2/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1115 - mean_squared_error: 0.0290\n",
      "Epoch 2: val_loss improved from 0.43991 to 0.11056, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.1109 - mean_squared_error: 0.0287 - val_loss: 0.1106 - val_mean_squared_error: 0.0279\n",
      "Epoch 3/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.0989 - mean_squared_error: 0.0240\n",
      "Epoch 3: val_loss did not improve from 0.11056\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0988 - mean_squared_error: 0.0239 - val_loss: 0.1285 - val_mean_squared_error: 0.0399\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.0853 - mean_squared_error: 0.0185\n",
      "Epoch 4: val_loss improved from 0.11056 to 0.08974, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c32filt19str2,layer2:c16filt17str1,1conv,dropout0.1,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.0854 - mean_squared_error: 0.0185 - val_loss: 0.0897 - val_mean_squared_error: 0.0202\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0770 - mean_squared_error: 0.0155\n",
      "Epoch 5: val_loss did not improve from 0.08974\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0770 - mean_squared_error: 0.0155 - val_loss: 0.0927 - val_mean_squared_error: 0.0193\n",
      "Epoch 6/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.0733 - mean_squared_error: 0.0142\n",
      "Epoch 6: val_loss did not improve from 0.08974\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0738 - mean_squared_error: 0.0144 - val_loss: 0.1085 - val_mean_squared_error: 0.0247\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.0735 - mean_squared_error: 0.0145\n",
      "Epoch 7: val_loss did not improve from 0.08974\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.0735 - mean_squared_error: 0.0145 - val_loss: 0.0985 - val_mean_squared_error: 0.0212\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.10, val rmse 0.15###\n",
      "mae0.95+-0.02_rmse1.46+-0.02\n",
      "random search 13/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1906 - mean_squared_error: 0.0812\n",
      "Epoch 1: val_loss improved from inf to 0.12655, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_0.hdf5\n",
      "46/46 [==============================] - 2s 20ms/step - loss: 0.1883 - mean_squared_error: 0.0797 - val_loss: 0.1265 - val_mean_squared_error: 0.0398\n",
      "Epoch 2/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1146 - mean_squared_error: 0.0307\n",
      "Epoch 2: val_loss did not improve from 0.12655\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.1147 - mean_squared_error: 0.0307 - val_loss: 0.1430 - val_mean_squared_error: 0.0386\n",
      "Epoch 3/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.1002 - mean_squared_error: 0.0235\n",
      "Epoch 3: val_loss did not improve from 0.12655\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0992 - mean_squared_error: 0.0232 - val_loss: 0.1369 - val_mean_squared_error: 0.0358\n",
      "Epoch 4/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0920 - mean_squared_error: 0.0200\n",
      "Epoch 4: val_loss did not improve from 0.12655\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0919 - mean_squared_error: 0.0200 - val_loss: 0.1599 - val_mean_squared_error: 0.0460\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###0 fold : val mae 0.14, val rmse 0.21###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1933 - mean_squared_error: 0.0839\n",
      "Epoch 1: val_loss improved from inf to 0.18650, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 2s 19ms/step - loss: 0.1912 - mean_squared_error: 0.0822 - val_loss: 0.1865 - val_mean_squared_error: 0.0766\n",
      "Epoch 2/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1180 - mean_squared_error: 0.0329\n",
      "Epoch 2: val_loss improved from 0.18650 to 0.12135, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.1167 - mean_squared_error: 0.0323 - val_loss: 0.1213 - val_mean_squared_error: 0.0315\n",
      "Epoch 3/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.1020 - mean_squared_error: 0.0239\n",
      "Epoch 3: val_loss improved from 0.12135 to 0.11356, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.1010 - mean_squared_error: 0.0236 - val_loss: 0.1136 - val_mean_squared_error: 0.0298\n",
      "Epoch 4/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.0195\n",
      "Epoch 4: val_loss did not improve from 0.11356\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0906 - mean_squared_error: 0.0199 - val_loss: 0.1197 - val_mean_squared_error: 0.0288\n",
      "Epoch 5/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0855 - mean_squared_error: 0.0179\n",
      "Epoch 5: val_loss improved from 0.11356 to 0.09628, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0858 - mean_squared_error: 0.0181 - val_loss: 0.0963 - val_mean_squared_error: 0.0213\n",
      "Epoch 6/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0819 - mean_squared_error: 0.0163\n",
      "Epoch 6: val_loss improved from 0.09628 to 0.09606, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0820 - mean_squared_error: 0.0163 - val_loss: 0.0961 - val_mean_squared_error: 0.0212\n",
      "Epoch 7/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0816 - mean_squared_error: 0.0163\n",
      "Epoch 7: val_loss did not improve from 0.09606\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0816 - mean_squared_error: 0.0163 - val_loss: 0.1046 - val_mean_squared_error: 0.0237\n",
      "Epoch 8/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0778 - mean_squared_error: 0.0152\n",
      "Epoch 8: val_loss did not improve from 0.09606\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0788 - mean_squared_error: 0.0156 - val_loss: 0.1002 - val_mean_squared_error: 0.0227\n",
      "Epoch 9/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.0148\n",
      "Epoch 9: val_loss improved from 0.09606 to 0.09074, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0763 - mean_squared_error: 0.0147 - val_loss: 0.0907 - val_mean_squared_error: 0.0207\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0759 - mean_squared_error: 0.0144\n",
      "Epoch 10: val_loss improved from 0.09074 to 0.08944, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_1.hdf5\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0759 - mean_squared_error: 0.0144 - val_loss: 0.0894 - val_mean_squared_error: 0.0200\n",
      "Epoch 11/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0746 - mean_squared_error: 0.0141\n",
      "Epoch 11: val_loss did not improve from 0.08944\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0747 - mean_squared_error: 0.0142 - val_loss: 0.0960 - val_mean_squared_error: 0.0210\n",
      "Epoch 12/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0741 - mean_squared_error: 0.0138\n",
      "Epoch 12: val_loss did not improve from 0.08944\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0740 - mean_squared_error: 0.0137 - val_loss: 0.0983 - val_mean_squared_error: 0.0225\n",
      "Epoch 13/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0711 - mean_squared_error: 0.0131\n",
      "Epoch 13: val_loss did not improve from 0.08944\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0703 - mean_squared_error: 0.0128 - val_loss: 0.0905 - val_mean_squared_error: 0.0195\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###1 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1905 - mean_squared_error: 0.0811\n",
      "Epoch 1: val_loss improved from inf to 0.13764, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 3s 20ms/step - loss: 0.1901 - mean_squared_error: 0.0807 - val_loss: 0.1376 - val_mean_squared_error: 0.0453\n",
      "Epoch 2/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.1091 - mean_squared_error: 0.0279\n",
      "Epoch 2: val_loss did not improve from 0.13764\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.1082 - mean_squared_error: 0.0275 - val_loss: 0.1884 - val_mean_squared_error: 0.0611\n",
      "Epoch 3/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0985 - mean_squared_error: 0.0225\n",
      "Epoch 3: val_loss improved from 0.13764 to 0.09584, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0984 - mean_squared_error: 0.0225 - val_loss: 0.0958 - val_mean_squared_error: 0.0212\n",
      "Epoch 4/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.0199\n",
      "Epoch 4: val_loss did not improve from 0.09584\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0900 - mean_squared_error: 0.0198 - val_loss: 0.1236 - val_mean_squared_error: 0.0299\n",
      "Epoch 5/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0877 - mean_squared_error: 0.0185\n",
      "Epoch 5: val_loss did not improve from 0.09584\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0881 - mean_squared_error: 0.0186 - val_loss: 0.1294 - val_mean_squared_error: 0.0334\n",
      "Epoch 6/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0850 - mean_squared_error: 0.0177\n",
      "Epoch 6: val_loss improved from 0.09584 to 0.08610, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0849 - mean_squared_error: 0.0177 - val_loss: 0.0861 - val_mean_squared_error: 0.0181\n",
      "Epoch 7/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0796 - mean_squared_error: 0.0165\n",
      "Epoch 7: val_loss did not improve from 0.08610\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0798 - mean_squared_error: 0.0166 - val_loss: 0.0897 - val_mean_squared_error: 0.0186\n",
      "Epoch 8/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0779 - mean_squared_error: 0.0150\n",
      "Epoch 8: val_loss did not improve from 0.08610\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0787 - mean_squared_error: 0.0155 - val_loss: 0.1001 - val_mean_squared_error: 0.0225\n",
      "Epoch 9/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0779 - mean_squared_error: 0.0149\n",
      "Epoch 9: val_loss improved from 0.08610 to 0.08556, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0781 - mean_squared_error: 0.0149 - val_loss: 0.0856 - val_mean_squared_error: 0.0183\n",
      "Epoch 10/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0760 - mean_squared_error: 0.0148\n",
      "Epoch 10: val_loss did not improve from 0.08556\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0757 - mean_squared_error: 0.0147 - val_loss: 0.0911 - val_mean_squared_error: 0.0185\n",
      "Epoch 11/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0754 - mean_squared_error: 0.0145\n",
      "Epoch 11: val_loss did not improve from 0.08556\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0750 - mean_squared_error: 0.0144 - val_loss: 0.1037 - val_mean_squared_error: 0.0237\n",
      "Epoch 12/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0730 - mean_squared_error: 0.0138\n",
      "Epoch 12: val_loss improved from 0.08556 to 0.08500, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0727 - mean_squared_error: 0.0137 - val_loss: 0.0850 - val_mean_squared_error: 0.0179\n",
      "Epoch 13/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0724 - mean_squared_error: 0.0136\n",
      "Epoch 13: val_loss did not improve from 0.08500\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0724 - mean_squared_error: 0.0137 - val_loss: 0.0878 - val_mean_squared_error: 0.0183\n",
      "Epoch 14/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0708 - mean_squared_error: 0.0130\n",
      "Epoch 14: val_loss improved from 0.08500 to 0.08330, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_2.hdf5\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0710 - mean_squared_error: 0.0131 - val_loss: 0.0833 - val_mean_squared_error: 0.0173\n",
      "Epoch 15/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0709 - mean_squared_error: 0.0126\n",
      "Epoch 15: val_loss did not improve from 0.08330\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0714 - mean_squared_error: 0.0130 - val_loss: 0.0892 - val_mean_squared_error: 0.0186\n",
      "Epoch 16/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0682 - mean_squared_error: 0.0122\n",
      "Epoch 16: val_loss did not improve from 0.08330\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0679 - mean_squared_error: 0.0119 - val_loss: 0.1061 - val_mean_squared_error: 0.0243\n",
      "Epoch 17/100\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0677 - mean_squared_error: 0.0121\n",
      "Epoch 17: val_loss did not improve from 0.08330\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0691 - mean_squared_error: 0.0125 - val_loss: 0.0854 - val_mean_squared_error: 0.0175\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###2 fold : val mae 0.09, val rmse 0.14###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.2008 - mean_squared_error: 0.0870\n",
      "Epoch 1: val_loss improved from inf to 0.15203, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 2s 20ms/step - loss: 0.2008 - mean_squared_error: 0.0870 - val_loss: 0.1520 - val_mean_squared_error: 0.0544\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1147 - mean_squared_error: 0.0315\n",
      "Epoch 2: val_loss improved from 0.15203 to 0.11220, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.1147 - mean_squared_error: 0.0315 - val_loss: 0.1122 - val_mean_squared_error: 0.0279\n",
      "Epoch 3/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.1006 - mean_squared_error: 0.0232\n",
      "Epoch 3: val_loss improved from 0.11220 to 0.10533, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch64,layer1:c16filt15str1,layer2:c128filt11str1,layer3:c16filt13str4,1conv,dropout0.4,dnodes128,dropout0/weights_3.hdf5\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.1005 - mean_squared_error: 0.0234 - val_loss: 0.1053 - val_mean_squared_error: 0.0260\n",
      "Epoch 4/100\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0935 - mean_squared_error: 0.0211\n",
      "Epoch 4: val_loss did not improve from 0.10533\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0937 - mean_squared_error: 0.0210 - val_loss: 0.1658 - val_mean_squared_error: 0.0484\n",
      "Epoch 5/100\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0893 - mean_squared_error: 0.0198\n",
      "Epoch 5: val_loss did not improve from 0.10533\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0888 - mean_squared_error: 0.0194 - val_loss: 0.1066 - val_mean_squared_error: 0.0242\n",
      "Epoch 6/100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0874 - mean_squared_error: 0.0183\n",
      "Epoch 6: val_loss did not improve from 0.10533\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0878 - mean_squared_error: 0.0184 - val_loss: 0.1142 - val_mean_squared_error: 0.0264\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.11, val rmse 0.16###\n",
      "mae1.06+-0.20_rmse1.61+-0.29\n",
      "random search 14/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2447 - mean_squared_error: 0.0980\n",
      "Epoch 1: val_loss improved from inf to 0.18341, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c64filt17str2,layer2:c32filt11str2,1conv,dropout0.4,dnodes64,dropout0/weights_0.hdf5\n",
      "6/6 [==============================] - 3s 146ms/step - loss: 0.2447 - mean_squared_error: 0.0980 - val_loss: 0.1834 - val_mean_squared_error: 0.0545\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1534 - mean_squared_error: 0.0551\n",
      "Epoch 2: val_loss did not improve from 0.18341\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.1515 - mean_squared_error: 0.0542 - val_loss: 0.3017 - val_mean_squared_error: 0.1596\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1450 - mean_squared_error: 0.0507\n",
      "Epoch 3: val_loss did not improve from 0.18341\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.1429 - mean_squared_error: 0.0497 - val_loss: 0.3811 - val_mean_squared_error: 0.2260\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1347 - mean_squared_error: 0.0446\n",
      "Epoch 4: val_loss did not improve from 0.18341\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.1352 - mean_squared_error: 0.0447 - val_loss: 0.6421 - val_mean_squared_error: 0.4804\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###0 fold : val mae 0.18, val rmse 0.24###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2588 - mean_squared_error: 0.1064\n",
      "Epoch 1: val_loss improved from inf to 0.18314, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c64filt17str2,layer2:c32filt11str2,1conv,dropout0.4,dnodes64,dropout0/weights_1.hdf5\n",
      "6/6 [==============================] - 2s 151ms/step - loss: 0.2470 - mean_squared_error: 0.0996 - val_loss: 0.1831 - val_mean_squared_error: 0.0549\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1476 - mean_squared_error: 0.0514\n",
      "Epoch 2: val_loss did not improve from 0.18314\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.1462 - mean_squared_error: 0.0507 - val_loss: 0.2987 - val_mean_squared_error: 0.1499\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1380 - mean_squared_error: 0.0465\n",
      "Epoch 3: val_loss did not improve from 0.18314\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.1369 - mean_squared_error: 0.0460 - val_loss: 0.5284 - val_mean_squared_error: 0.3681\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1321 - mean_squared_error: 0.0429\n",
      "Epoch 4: val_loss did not improve from 0.18314\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.1314 - mean_squared_error: 0.0427 - val_loss: 0.6537 - val_mean_squared_error: 0.4980\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.19, val rmse 0.24###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2493 - mean_squared_error: 0.1009\n",
      "Epoch 1: val_loss improved from inf to 0.19938, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c64filt17str2,layer2:c32filt11str2,1conv,dropout0.4,dnodes64,dropout0/weights_2.hdf5\n",
      "6/6 [==============================] - 2s 132ms/step - loss: 0.2493 - mean_squared_error: 0.1009 - val_loss: 0.1994 - val_mean_squared_error: 0.0636\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1509 - mean_squared_error: 0.0529\n",
      "Epoch 2: val_loss did not improve from 0.19938\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.1517 - mean_squared_error: 0.0533 - val_loss: 0.4431 - val_mean_squared_error: 0.2839\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1412 - mean_squared_error: 0.0475\n",
      "Epoch 3: val_loss did not improve from 0.19938\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.1401 - mean_squared_error: 0.0471 - val_loss: 0.5932 - val_mean_squared_error: 0.4348\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1290 - mean_squared_error: 0.0409\n",
      "Epoch 4: val_loss did not improve from 0.19938\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.1289 - mean_squared_error: 0.0407 - val_loss: 0.7395 - val_mean_squared_error: 0.6010\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      " ###2 fold : val mae 0.20, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2462 - mean_squared_error: 0.1008\n",
      "Epoch 1: val_loss improved from inf to 0.16682, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch512,layer1:c64filt17str2,layer2:c32filt11str2,1conv,dropout0.4,dnodes64,dropout0/weights_3.hdf5\n",
      "6/6 [==============================] - 2s 147ms/step - loss: 0.2462 - mean_squared_error: 0.1008 - val_loss: 0.1668 - val_mean_squared_error: 0.0514\n",
      "Epoch 2/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1558 - mean_squared_error: 0.0567\n",
      "Epoch 2: val_loss did not improve from 0.16682\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1548 - mean_squared_error: 0.0560 - val_loss: 0.1804 - val_mean_squared_error: 0.0674\n",
      "Epoch 3/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1452 - mean_squared_error: 0.0514\n",
      "Epoch 3: val_loss did not improve from 0.16682\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.1429 - mean_squared_error: 0.0503 - val_loss: 0.2703 - val_mean_squared_error: 0.1323\n",
      "Epoch 4/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1326 - mean_squared_error: 0.0440\n",
      "Epoch 4: val_loss did not improve from 0.16682\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.1332 - mean_squared_error: 0.0441 - val_loss: 0.5393 - val_mean_squared_error: 0.3666\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###3 fold : val mae 0.17, val rmse 0.24###\n",
      "mae1.86+-0.10_rmse2.43+-0.08\n",
      "random search 15/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1649 - mean_squared_error: 0.0650\n",
      "Epoch 1: val_loss improved from inf to 0.16155, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_0.hdf5\n",
      "92/92 [==============================] - 3s 22ms/step - loss: 0.1649 - mean_squared_error: 0.0650 - val_loss: 0.1615 - val_mean_squared_error: 0.0622\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0613\n",
      "Epoch 2: val_loss did not improve from 0.16155\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1605 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 3/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.0614\n",
      "Epoch 3: val_loss did not improve from 0.16155\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0613\n",
      "Epoch 4: val_loss did not improve from 0.16155\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1606 - mean_squared_error: 0.0613 - val_loss: 0.1616 - val_mean_squared_error: 0.0622\n",
      "31/31 [==============================] - 1s 4ms/step\n",
      " ###0 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0641\n",
      "Epoch 1: val_loss improved from inf to 0.16526, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_1.hdf5\n",
      "92/92 [==============================] - 4s 21ms/step - loss: 0.1634 - mean_squared_error: 0.0641 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 2/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1595 - mean_squared_error: 0.0609\n",
      "Epoch 2: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 3/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1597 - mean_squared_error: 0.0609\n",
      "Epoch 3: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1593 - mean_squared_error: 0.0608\n",
      "Epoch 4: val_loss did not improve from 0.16526\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1594 - mean_squared_error: 0.0608 - val_loss: 0.1653 - val_mean_squared_error: 0.0637\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###1 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1664 - mean_squared_error: 0.0652\n",
      "Epoch 1: val_loss improved from inf to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 3s 22ms/step - loss: 0.1663 - mean_squared_error: 0.0650 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 2/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1631 - mean_squared_error: 0.0625\n",
      "Epoch 2: val_loss improved from 0.15396 to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 3/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1630 - mean_squared_error: 0.0625\n",
      "Epoch 3: val_loss improved from 0.15396 to 0.15396, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_2.hdf5\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 4/100\n",
      "89/92 [============================>.] - ETA: 0s - loss: 0.1630 - mean_squared_error: 0.0625\n",
      "Epoch 4: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 5/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1624 - mean_squared_error: 0.0620\n",
      "Epoch 5: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "Epoch 6/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1636 - mean_squared_error: 0.0628\n",
      "Epoch 6: val_loss did not improve from 0.15396\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1631 - mean_squared_error: 0.0625 - val_loss: 0.1540 - val_mean_squared_error: 0.0587\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      " ###2 fold : val mae 0.17, val rmse 0.26###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.1635 - mean_squared_error: 0.0639\n",
      "Epoch 1: val_loss improved from inf to 0.16255, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 3s 22ms/step - loss: 0.1634 - mean_squared_error: 0.0639 - val_loss: 0.1625 - val_mean_squared_error: 0.0616\n",
      "Epoch 2/100\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 0.1601 - mean_squared_error: 0.0615\n",
      "Epoch 2: val_loss did not improve from 0.16255\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 3/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.0616\n",
      "Epoch 3: val_loss improved from 0.16255 to 0.16247, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN_4layers_largekernel(7~19)_age%10(sigmoid)_mae(nodecay)_4fold_test500/batch32,layer1:c64filt19str2,layer2:c64filt13str2,layer3:c128filt19str2,1conv,dropout0.1,dnodes64,dropout0.2/weights_3.hdf5\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1602 - mean_squared_error: 0.0615 - val_loss: 0.1625 - val_mean_squared_error: 0.0616\n",
      "Epoch 4/100\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 0.1602 - mean_squared_error: 0.0613\n",
      "Epoch 4: val_loss did not improve from 0.16247\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.1602 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 5/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.1603 - mean_squared_error: 0.0617\n",
      "Epoch 5: val_loss did not improve from 0.16247\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.1603 - mean_squared_error: 0.0615 - val_loss: 0.1626 - val_mean_squared_error: 0.0616\n",
      "Epoch 6/100\n",
      "59/92 [==================>...........] - ETA: 0s - loss: 0.1630 - mean_squared_error: 0.0630"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_acc, test_roc, test_prc = [], [], []\n",
    "#test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    \n",
    "    # total conv layers of the model\n",
    "    n_conv = random.choice([2,3,4]) \n",
    "    # test settings\n",
    "    for i in range(n_conv):\n",
    "        nfilt[i], kernels[i], strides[i] = random.choice(layer_settings)\n",
    "    dense_node, dropout_cnn, dropout_fc, batch_size, learning_rate = random.choice(test_settings)\n",
    "    \n",
    "    if itrial < 0:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch{batch_size},'\n",
    "    for i in range(n_conv):\n",
    "        odir_f += f'layer{i+1}:c{nfilt[i]}filt{kernels[i]}str{strides[i]},'\n",
    "    odir_f += f'1conv,dropout{dropout_cnn},dnodes{dense_node},dropout{dropout_fc}'#,lr{learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "        \n",
    "    # model validation (VGG)\n",
    "    out_shape = x_train.shape[1]\n",
    "    for i in range(n_conv):\n",
    "        out_shape = out_shape / pool_size / strides[i]\n",
    "    if out_shape < 1:\n",
    "        print('non-valid structure')\n",
    "        os.rmdir(odir)\n",
    "        itrial -= 1\n",
    "        continue\n",
    "        \n",
    "    #mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/GPU:0\",\"/GPU:1\",\"/GPU:2\",\"/GPU:3\"])\n",
    "    #with mirrored_strategy.scope(): \n",
    "    \n",
    "    with tf.device(f'/gpu:{GPU}'):\n",
    "        # build a model\n",
    "        inp = Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "        out = inp\n",
    "\n",
    "\n",
    "        # VGC block\n",
    "        out = Conv1D(filters=nfilt[i], kernel_size=kernels[i], strides=strides[i], padding='same', activation=None)(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)\n",
    "        out = MaxPooling1D(pool_size)(out)\n",
    "\n",
    "\n",
    "        # globalpooling vs flattening vs 1x1 convolution\n",
    "        #elif globalpool_opt == 'ave':\n",
    "        #    out = GlobalAveragePooling1D()(out)\n",
    "        out = Conv1D(filters=1, kernel_size=1)(out)\n",
    "        out = Flatten() (out)\n",
    "\n",
    "\n",
    "\n",
    "        if dense_node != 0:\n",
    "            out = Dropout(dropout_cnn)(out)\n",
    "            out = Dense(dense_node, activation='relu')(out)\n",
    "        out = Dropout(dropout_fc)(out)\n",
    "        out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "\n",
    "        model = Model(inputs=[inp], outputs=[out])\n",
    "        model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "\n",
    "        # 4-fold cv\n",
    "        kfold = KFold(nfold)\n",
    "        tprs, aucs, prs = [], [], []\n",
    "        test_rmse, test_mae = [], []\n",
    "        f1_scores, thvals = [], []\n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        mean_recall = np.linspace(0,1,100)\n",
    "\n",
    "        switch = 0\n",
    "        caseids_train = np.unique(c_train)\n",
    "        for fold, (c_cv_trains_mask, c_cv_test_mask) in enumerate(kfold.split(caseids_train)):\n",
    "            c_cv_trains = caseids_train[c_cv_trains_mask]\n",
    "            \n",
    "            cv_train_mask = np.isin(c_train, c_cv_trains)\n",
    "            cv_val_mask = ~cv_train_mask\n",
    "            \n",
    "            X_train = x_train[cv_train_mask]\n",
    "            X_val = x_train[cv_val_mask]\n",
    "\n",
    "            Y_train = y_train[cv_train_mask]\n",
    "            Y_val = y_train[cv_val_mask]\n",
    "\n",
    "\n",
    "            # model 학습\n",
    "            try:\n",
    "                # learning scheduler\n",
    "                def step_decay(epoch):\n",
    "                    start = 1e-3\n",
    "                    drop = 0.1\n",
    "                    epochs_drop = 10\n",
    "                    lr = start * (drop ** np.floor((epoch)/epochs_drop))\n",
    "                    return lr\n",
    "                lr_scheduler = LearningRateScheduler(step_decay, verbose=1)\n",
    "                \n",
    "                weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "                \n",
    "                model.compile(loss='mean_absolute_error', optimizer=Adam(lr=lr_scheduler, weight_decay=0), metrics=['mean_squared_error'])\n",
    "                hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                        callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                    EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "\n",
    "                model.load_weights(weightcache)\n",
    "                y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "                # MAE 계산\n",
    "                model_err = metrics.MeanAbsoluteError()\n",
    "                model_err.update_state(y_test, y_pred)\n",
    "                mae_val = model_err.result().numpy()\n",
    "                test_mae.append(mae_val)\n",
    "\n",
    "                # RMSE 계산\n",
    "                model_err = metrics.RootMeanSquaredError() \n",
    "                model_err.update_state(y_test, y_pred)\n",
    "                rmse_val = model_err.result().numpy()\n",
    "                test_rmse.append(rmse_val)\n",
    "\n",
    "\n",
    "                print(f' ###{fold} fold : val mae {mae_val:.2f}, val rmse {rmse_val:.2f}###')\n",
    "                tf.keras.backend.clear_session()\n",
    "                model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                switch = 1\n",
    "                shutil.rmtree(odir)\n",
    "                itrial -= 1\n",
    "                break\n",
    "        ###\n",
    "        if switch:\n",
    "            switch = 0\n",
    "            tf.keras.backend.clear_session()\n",
    "            continue\n",
    "\n",
    "    # \n",
    "\n",
    "    # RMSE 계산\n",
    "    mean_rmse = np.mean(test_rmse)\n",
    "    std_rmse = np.std(test_rmse)\n",
    "    \n",
    "    mean_mae = np.mean(test_mae)\n",
    "    std_mae = np.std(test_mae)\n",
    "\n",
    "    max_idx = test_mae.index(min(test_mae))\n",
    "    \n",
    "\n",
    "    print(f'mae{mean_mae*10:.2f}+-{std_mae*10:.2f}_rmse{mean_rmse*10:.2f}+-{std_rmse*10:.2f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/mae{mean_mae*10:.2f}+-{std_mae*10:.2f}_rmse{mean_rmse*10:.2f}+-{std_rmse*10:.2f}_max{max_idx}__{odir_f}')\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb39303-ba79-4fcb-9344-921acd7b37b4",
   "metadata": {},
   "source": [
    "## 1d-cnn + concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a2f5f-62ac-46c2-bcbe-e0ef69f69176",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def signal_data_generator(data, batch_size):\n",
    "    num_samples = len(data[0])\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    while True:\n",
    "        for idx in range(num_batches):\n",
    "            batch_data = np.array(data)[:,idx * batch_size:(idx+1) * batch_size,:]\n",
    "\n",
    "            # Perform preprocessing on batch_data if needed\n",
    "\n",
    "            yield batch_data, labels  # Yield a batch of data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e447a7e-f97a-4bc5-9897-8a1489aa5345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n",
      "2023-09-22 16:10:20.964094\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 4  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"randomSearch/{hyper_path}/CNN_4layers+feats_2aug_kernel(7~19)_age%{SCALE_Y}(sigmoid)_mae(decay)_{nfold}fold_test{ntest}\"\n",
    "\n",
    "if not os.path.exists(f\"randomSearch/{hyper_path}\"):\n",
    "    os.mkdir(f\"randomSearch/{hyper_path}\")\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: 1D CNN 4 layers, regression')\n",
    "f.write(f'input: ECG of 10 second, output: age')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "layer_settings, test_settings = [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "globalpool_opts = ['max','ave']\n",
    "\n",
    "# hyperparamters pool\n",
    "filt_opts = [16, 32, 64, 128] # num of filters(kernel)\n",
    "stride_opts = [1,2,4]  # other opts: stride = (kernel-1)/2\n",
    "kernel_opts = range(7,20,2) # kernel size\n",
    "pool_size = 2\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [0, 16, 32, 64, 128]\n",
    "BATCH_SIZE = [32, 64, 128, 256, 512]\n",
    "lr_opts = [0.001, 0.002, 0.0005]\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "nfilt, kernels, strides = [], [], []\n",
    "for i in range(5):\n",
    "    nfilt.append(0)\n",
    "    kernels.append(0)\n",
    "    strides.append(0)\n",
    "\n",
    "for nfilter in filt_opts:\n",
    "    for kernel in kernel_opts:\n",
    "        for stride in stride_opts:\n",
    "        #layer_settings.append([nfilter, kernel, int((kernel-1)/2)])       \n",
    "            layer_settings.append([nfilter, kernel, stride])\n",
    "    \n",
    "for dense_node in dense_opts:\n",
    "    for dropout_cnn in dropout_opts:\n",
    "        for dropout_fc in dropout_opts:\n",
    "            for batch_size in BATCH_SIZE:\n",
    "                for learning_rate in lr_opts:\n",
    "                    test_settings.append([dense_node, dropout_cnn, dropout_fc, batch_size, learning_rate])                                   \n",
    "\n",
    "                        \n",
    "print('done')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6293d-4103-4c94-963c-0b948e37eefc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 16:10:47.985906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8904\n",
      "2023-09-22 16:10:48.503966: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-22 16:10:48.599388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-22 16:10:48.601039: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7efd34567110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-22 16:10:48.601055: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-09-22 16:10:48.605436: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-22 16:10:48.680387: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-22 16:10:48.738269: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/627 [=======>......................] - ETA: 6s - loss: 0.1142"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Concatenate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    start = 1e-3\n",
    "    drop = 0.1\n",
    "    epochs_drop = 10\n",
    "    lr = start * (drop ** np.floor((epoch)/epochs_drop))\n",
    "    return lr\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "test_acc, test_roc, test_prc = [], [], []\n",
    "#test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    \n",
    "    # total conv layers of the model\n",
    "    n_conv = random.choice([2,3,4]) \n",
    "    # test settings\n",
    "    for i in range(n_conv):\n",
    "        nfilt[i], kernels[i], strides[i] = random.choice(layer_settings)\n",
    "    dense_node, dropout_cnn, dropout_fc, batch_size, learning_rate = random.choice(test_settings)\n",
    "    \n",
    "    if itrial < 0:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch{batch_size},'\n",
    "    for i in range(n_conv):\n",
    "        odir_f += f'layer{i+1}:c{nfilt[i]}filt{kernels[i]}str{strides[i]},'\n",
    "    odir_f += f'1conv,dropout{dropout_cnn},dnodes{dense_node},dropout{dropout_fc}'#,lr{learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "        \n",
    "    # model validation (VGG)\n",
    "    out_shape = x_train.shape[1]\n",
    "    for i in range(n_conv):\n",
    "        out_shape = out_shape / pool_size / strides[i]\n",
    "    if out_shape < 1:\n",
    "        print('non-valid structure')\n",
    "        os.rmdir(odir)\n",
    "        itrial -= 1\n",
    "        continue\n",
    "        \n",
    "    #mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/GPU:0\",\"/GPU:1\",\"/GPU:2\",\"/GPU:3\"])\n",
    "    #with mirrored_strategy.scope(): \n",
    "    \n",
    "    with tf.device(f'/gpu:{GPU}'):\n",
    "        # build a model\n",
    "        inp = Input(shape=(x_train.shape[1],x_train.shape[2]), name=\"input_1\")\n",
    "        out = inp\n",
    "\n",
    "\n",
    "        # VGC block\n",
    "        for i in range(n_conv):\n",
    "            out = Conv1D(filters=nfilt[i], kernel_size=kernels[i], strides=strides[i], padding='same', activation=None)(out)\n",
    "            out = BatchNormalization()(out)\n",
    "            out = Activation('relu')(out)\n",
    "            out = MaxPooling1D(pool_size)(out)\n",
    "\n",
    "\n",
    "        # globalpooling vs flattening vs 1x1 convolution\n",
    "        #elif globalpool_opt == 'ave':\n",
    "        #    out = GlobalAveragePooling1D()(out)\n",
    "        out = Conv1D(filters=1, kernel_size=1)(out)\n",
    "        out = Flatten() (out)\n",
    "\n",
    "    \n",
    "        # concat with X_feature\n",
    "        inp_feat = Input(shape=(feat_train.shape[1]), name=\"input_2\")\n",
    "        out = Concatenate(axis=1)([out, inp_feat])\n",
    "\n",
    "        if dense_node != 0:\n",
    "            out = Dropout(dropout_cnn)(out)\n",
    "            out = Dense(dense_node, activation='relu')(out)\n",
    "        out = Dropout(dropout_fc)(out)\n",
    "        out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "\n",
    "        model = Model(inputs=[inp, inp_feat], outputs=[out])\n",
    "        lr_scheduler = LearningRateScheduler(step_decay, verbose=1)\n",
    "        model.compile(loss='mean_absolute_error', optimizer=Adam(lr=lr_scheduler, weight_decay=1e-4), metrics=[])\n",
    "        model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "\n",
    "    # 4-fold cv\n",
    "    kfold = KFold(nfold)\n",
    "    tprs, aucs, prs = [], [], []\n",
    "    test_rmse, test_mae = [], []\n",
    "    f1_scores, thvals = [], []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    mean_recall = np.linspace(0,1,100)\n",
    "\n",
    "    switch = 0\n",
    "    caseids_train = np.unique(c_train)\n",
    "    for fold, (c_cv_trains_mask, c_cv_test_mask) in enumerate(kfold.split(caseids_train)):\n",
    "        c_cv_trains = caseids_train[c_cv_trains_mask]\n",
    "\n",
    "        cv_train_mask = np.isin(c_train, c_cv_trains)\n",
    "        cv_val_mask = ~cv_train_mask\n",
    "\n",
    "        X_train = x_train[cv_train_mask]\n",
    "        X_val = x_train[cv_val_mask]\n",
    "\n",
    "        Y_train = y_train[cv_train_mask]\n",
    "        Y_val = y_train[cv_val_mask]\n",
    "\n",
    "        Feat_train = feat_train[cv_train_mask]\n",
    "        Feat_val = feat_train[cv_val_mask]\n",
    "\n",
    "        \n",
    "        # model 학습\n",
    "        try:\n",
    "\n",
    "            with tf.device(\"/CPU:0\"):\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": X_train, \"input_2\": Feat_train}, Y_train)).shuffle(x_train.shape[0]).batch(batch_size)\n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": X_val, \"input_2\": Feat_val}, Y_val)).batch(batch_size)\n",
    "                test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": x_test, \"input_2\": feat_test}, y_test)).batch(batch_size)\n",
    "       \n",
    "            weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "            #model = multi_gpu_model(model, gpus=4)\n",
    "            #model.compile(loss='mean_squared_error', optimizer=Adam(lr=lr_scheduler, weight_decay=1e-4), metrics=['mean_absolute_error'])\n",
    "            hist = model.fit(train_dataset, validation_data=val_dataset, epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "\n",
    "            model.load_weights(weightcache)\n",
    "            y_pred = model.predict(test_dataset).flatten()\n",
    "\n",
    "            # MAE 계산\n",
    "            mae_val = mean_absolute_error(y_test, y_pred)\n",
    "            test_mae.append(mae_val)\n",
    "\n",
    "\n",
    "            print(f' ###{fold} fold : val mae {mae_val:.2f}###')\n",
    "            tf.keras.backend.clear_session()\n",
    "            model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            switch = 1\n",
    "            shutil.rmtree(odir)\n",
    "            itrial -= 1\n",
    "\n",
    "            break\n",
    "    ###\n",
    "    if switch:\n",
    "        switch = 0\n",
    "        tf.keras.backend.clear_session()\n",
    "        continue\n",
    "\n",
    "    # \n",
    "\n",
    "    \n",
    "    mean_mae = np.mean(test_mae)\n",
    "    std_mae = np.std(test_mae)\n",
    "\n",
    "    max_idx = test_mae.index(min(test_mae))\n",
    "    \n",
    "\n",
    "    print(f'mae{mean_mae*SCALE_Y:.2f}+-{std_mae*SCALE_Y:.2f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/mae{mean_mae*SCALE_Y:.2f}+-{std_mae*SCALE_Y:.2f}_max{max_idx}__{odir_f}')\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c22988-3985-4d0d-bfc3-f1db9850fae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8bbf1-6449-47c7-a047-75bf051f452a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554db8af-c4f8-46f2-873e-d163ffa48c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with stratetgy.scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6560e5f8-ae80-4bdb-a5a9-618f544960fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
       " LogicalDevice(name='/device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d63a4d6d-7bb0-4ca0-874e-23c1dc700202",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5000, 12)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 2500, 128)    20096       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 2500, 128)   512         ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 2500, 128)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 1250, 128)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 313, 16)      18448       ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 313, 16)     64          ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 313, 16)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 156, 16)     0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 156, 64)      9280        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 156, 64)     256         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 156, 64)      0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 78, 64)      0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 78, 1)        65          ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 78)           0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 55)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 133)          0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 133)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8576        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57,362\n",
      "Trainable params: 56,946\n",
      "Non-trainable params: 416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f56f9-8f9d-4d18-b81d-42df03567768",
   "metadata": {},
   "source": [
    "## cnn transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc9af9f7-5eec-4249-ace7-67baa70f971d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1068 - mean_squared_error: 0.0273\n",
      "Epoch 1: val_loss improved from inf to 0.07329, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 19s 355ms/step - loss: 0.1068 - mean_squared_error: 0.0273 - val_loss: 0.0733 - val_mean_squared_error: 0.0097\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0787 - mean_squared_error: 0.0120\n",
      "Epoch 2: val_loss improved from 0.07329 to 0.07197, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 308ms/step - loss: 0.0787 - mean_squared_error: 0.0120 - val_loss: 0.0720 - val_mean_squared_error: 0.0103\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0779 - mean_squared_error: 0.0118\n",
      "Epoch 3: val_loss improved from 0.07197 to 0.07152, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 327ms/step - loss: 0.0779 - mean_squared_error: 0.0118 - val_loss: 0.0715 - val_mean_squared_error: 0.0108\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0762 - mean_squared_error: 0.0116\n",
      "Epoch 4: val_loss improved from 0.07152 to 0.07066, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 309ms/step - loss: 0.0762 - mean_squared_error: 0.0116 - val_loss: 0.0707 - val_mean_squared_error: 0.0107\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0738 - mean_squared_error: 0.0110\n",
      "Epoch 5: val_loss improved from 0.07066 to 0.06434, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 310ms/step - loss: 0.0738 - mean_squared_error: 0.0110 - val_loss: 0.0643 - val_mean_squared_error: 0.0088\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0620 - mean_squared_error: 0.0081\n",
      "Epoch 6: val_loss improved from 0.06434 to 0.06346, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 327ms/step - loss: 0.0620 - mean_squared_error: 0.0081 - val_loss: 0.0635 - val_mean_squared_error: 0.0061\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0562 - mean_squared_error: 0.0066\n",
      "Epoch 7: val_loss did not improve from 0.06346\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0562 - mean_squared_error: 0.0066 - val_loss: 0.0716 - val_mean_squared_error: 0.0073\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0061\n",
      "Epoch 8: val_loss improved from 0.06346 to 0.05122, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0536 - mean_squared_error: 0.0061 - val_loss: 0.0512 - val_mean_squared_error: 0.0045\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0508 - mean_squared_error: 0.0056\n",
      "Epoch 9: val_loss improved from 0.05122 to 0.04072, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 8s 331ms/step - loss: 0.0508 - mean_squared_error: 0.0056 - val_loss: 0.0407 - val_mean_squared_error: 0.0039\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0519 - mean_squared_error: 0.0058\n",
      "Epoch 10: val_loss did not improve from 0.04072\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0519 - mean_squared_error: 0.0058 - val_loss: 0.0476 - val_mean_squared_error: 0.0040\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0487 - mean_squared_error: 0.0052\n",
      "Epoch 11: val_loss did not improve from 0.04072\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0487 - mean_squared_error: 0.0052 - val_loss: 0.0483 - val_mean_squared_error: 0.0041\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0047\n",
      "Epoch 12: val_loss improved from 0.04072 to 0.04016, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 8s 336ms/step - loss: 0.0458 - mean_squared_error: 0.0047 - val_loss: 0.0402 - val_mean_squared_error: 0.0034\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.0043\n",
      "Epoch 13: val_loss did not improve from 0.04016\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0441 - mean_squared_error: 0.0043 - val_loss: 0.0418 - val_mean_squared_error: 0.0036\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0044\n",
      "Epoch 14: val_loss did not improve from 0.04016\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0447 - mean_squared_error: 0.0044 - val_loss: 0.0450 - val_mean_squared_error: 0.0038\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0041\n",
      "Epoch 15: val_loss improved from 0.04016 to 0.03810, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 312ms/step - loss: 0.0426 - mean_squared_error: 0.0041 - val_loss: 0.0381 - val_mean_squared_error: 0.0034\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0428 - mean_squared_error: 0.0041\n",
      "Epoch 16: val_loss did not improve from 0.03810\n",
      "23/23 [==============================] - 7s 314ms/step - loss: 0.0428 - mean_squared_error: 0.0041 - val_loss: 0.0452 - val_mean_squared_error: 0.0051\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0037\n",
      "Epoch 17: val_loss did not improve from 0.03810\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 0.0401 - mean_squared_error: 0.0037 - val_loss: 0.0466 - val_mean_squared_error: 0.0052\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0040\n",
      "Epoch 18: val_loss did not improve from 0.03810\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0419 - mean_squared_error: 0.0040 - val_loss: 0.0418 - val_mean_squared_error: 0.0042\n",
      "31/31 [==============================] - 1s 30ms/step\n",
      " ###0 fold : val mae 0.04###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1066 - mean_squared_error: 0.0272\n",
      "Epoch 1: val_loss improved from inf to 0.07314, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_1.hdf5\n",
      "23/23 [==============================] - 19s 354ms/step - loss: 0.1066 - mean_squared_error: 0.0272 - val_loss: 0.0731 - val_mean_squared_error: 0.0106\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0788 - mean_squared_error: 0.0121\n",
      "Epoch 2: val_loss did not improve from 0.07314\n",
      "23/23 [==============================] - 7s 322ms/step - loss: 0.0788 - mean_squared_error: 0.0121 - val_loss: 0.0783 - val_mean_squared_error: 0.0093\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0781 - mean_squared_error: 0.0121\n",
      "Epoch 3: val_loss did not improve from 0.07314\n",
      "23/23 [==============================] - 7s 304ms/step - loss: 0.0781 - mean_squared_error: 0.0121 - val_loss: 0.0770 - val_mean_squared_error: 0.0093\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0773 - mean_squared_error: 0.0116\n",
      "Epoch 4: val_loss improved from 0.07314 to 0.07296, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_1.hdf5\n",
      "23/23 [==============================] - 8s 330ms/step - loss: 0.0773 - mean_squared_error: 0.0116 - val_loss: 0.0730 - val_mean_squared_error: 0.0101\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0771 - mean_squared_error: 0.0121\n",
      "Epoch 5: val_loss improved from 0.07296 to 0.07290, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_1.hdf5\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0771 - mean_squared_error: 0.0121 - val_loss: 0.0729 - val_mean_squared_error: 0.0098\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0729 - mean_squared_error: 0.0107\n",
      "Epoch 6: val_loss improved from 0.07290 to 0.06826, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_1.hdf5\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0729 - mean_squared_error: 0.0107 - val_loss: 0.0683 - val_mean_squared_error: 0.0094\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0619 - mean_squared_error: 0.0079\n",
      "Epoch 7: val_loss improved from 0.06826 to 0.05710, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_1.hdf5\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 0.0619 - mean_squared_error: 0.0079 - val_loss: 0.0571 - val_mean_squared_error: 0.0058\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0539 - mean_squared_error: 0.0061\n",
      "Epoch 8: val_loss did not improve from 0.05710\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0539 - mean_squared_error: 0.0061 - val_loss: 0.0797 - val_mean_squared_error: 0.0090\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0534 - mean_squared_error: 0.0061\n",
      "Epoch 9: val_loss improved from 0.05710 to 0.04667, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_1.hdf5\n",
      "23/23 [==============================] - 7s 312ms/step - loss: 0.0534 - mean_squared_error: 0.0061 - val_loss: 0.0467 - val_mean_squared_error: 0.0045\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0500 - mean_squared_error: 0.0052\n",
      "Epoch 10: val_loss did not improve from 0.04667\n",
      "23/23 [==============================] - 7s 325ms/step - loss: 0.0500 - mean_squared_error: 0.0052 - val_loss: 0.0496 - val_mean_squared_error: 0.0044\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0047\n",
      "Epoch 11: val_loss did not improve from 0.04667\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0468 - mean_squared_error: 0.0047 - val_loss: 0.0629 - val_mean_squared_error: 0.0056\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0459 - mean_squared_error: 0.0046\n",
      "Epoch 12: val_loss did not improve from 0.04667\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 0.0459 - mean_squared_error: 0.0046 - val_loss: 0.0654 - val_mean_squared_error: 0.0059\n",
      "31/31 [==============================] - 1s 29ms/step\n",
      " ###1 fold : val mae 0.05###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1052 - mean_squared_error: 0.0264\n",
      "Epoch 1: val_loss improved from inf to 0.07653, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_2.hdf5\n",
      "23/23 [==============================] - 19s 344ms/step - loss: 0.1052 - mean_squared_error: 0.0264 - val_loss: 0.0765 - val_mean_squared_error: 0.0088\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0798 - mean_squared_error: 0.0125\n",
      "Epoch 2: val_loss improved from 0.07653 to 0.07459, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 310ms/step - loss: 0.0798 - mean_squared_error: 0.0125 - val_loss: 0.0746 - val_mean_squared_error: 0.0088\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0782 - mean_squared_error: 0.0122\n",
      "Epoch 3: val_loss did not improve from 0.07459\n",
      "23/23 [==============================] - 7s 319ms/step - loss: 0.0782 - mean_squared_error: 0.0122 - val_loss: 0.0758 - val_mean_squared_error: 0.0085\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0765 - mean_squared_error: 0.0115\n",
      "Epoch 4: val_loss improved from 0.07459 to 0.07107, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 312ms/step - loss: 0.0765 - mean_squared_error: 0.0115 - val_loss: 0.0711 - val_mean_squared_error: 0.0080\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0670 - mean_squared_error: 0.0090\n",
      "Epoch 5: val_loss did not improve from 0.07107\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 0.0670 - mean_squared_error: 0.0090 - val_loss: 0.0925 - val_mean_squared_error: 0.0117\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0579 - mean_squared_error: 0.0069\n",
      "Epoch 6: val_loss did not improve from 0.07107\n",
      "23/23 [==============================] - 7s 320ms/step - loss: 0.0579 - mean_squared_error: 0.0069 - val_loss: 0.0915 - val_mean_squared_error: 0.0124\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0549 - mean_squared_error: 0.0063\n",
      "Epoch 7: val_loss improved from 0.07107 to 0.05948, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 313ms/step - loss: 0.0549 - mean_squared_error: 0.0063 - val_loss: 0.0595 - val_mean_squared_error: 0.0056\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0540 - mean_squared_error: 0.0062\n",
      "Epoch 8: val_loss did not improve from 0.05948\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 0.0540 - mean_squared_error: 0.0062 - val_loss: 0.0664 - val_mean_squared_error: 0.0059\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0490 - mean_squared_error: 0.0053\n",
      "Epoch 9: val_loss improved from 0.05948 to 0.04977, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_2.hdf5\n",
      "23/23 [==============================] - 8s 333ms/step - loss: 0.0490 - mean_squared_error: 0.0053 - val_loss: 0.0498 - val_mean_squared_error: 0.0041\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0482 - mean_squared_error: 0.0050\n",
      "Epoch 10: val_loss improved from 0.04977 to 0.04504, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0482 - mean_squared_error: 0.0050 - val_loss: 0.0450 - val_mean_squared_error: 0.0040\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0450 - mean_squared_error: 0.0045\n",
      "Epoch 11: val_loss did not improve from 0.04504\n",
      "23/23 [==============================] - 7s 307ms/step - loss: 0.0450 - mean_squared_error: 0.0045 - val_loss: 0.0464 - val_mean_squared_error: 0.0044\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0445 - mean_squared_error: 0.0044\n",
      "Epoch 12: val_loss did not improve from 0.04504\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 0.0445 - mean_squared_error: 0.0044 - val_loss: 0.0480 - val_mean_squared_error: 0.0054\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0041\n",
      "Epoch 13: val_loss did not improve from 0.04504\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 0.0432 - mean_squared_error: 0.0041 - val_loss: 0.0497 - val_mean_squared_error: 0.0058\n",
      "31/31 [==============================] - 2s 29ms/step\n",
      " ###2 fold : val mae 0.05###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1070 - mean_squared_error: 0.0275\n",
      "Epoch 1: val_loss improved from inf to 0.07253, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 19s 355ms/step - loss: 0.1070 - mean_squared_error: 0.0275 - val_loss: 0.0725 - val_mean_squared_error: 0.0093\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0802 - mean_squared_error: 0.0123\n",
      "Epoch 2: val_loss did not improve from 0.07253\n",
      "23/23 [==============================] - 7s 304ms/step - loss: 0.0802 - mean_squared_error: 0.0123 - val_loss: 0.0755 - val_mean_squared_error: 0.0089\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0783 - mean_squared_error: 0.0122\n",
      "Epoch 3: val_loss did not improve from 0.07253\n",
      "23/23 [==============================] - 7s 328ms/step - loss: 0.0783 - mean_squared_error: 0.0122 - val_loss: 0.0755 - val_mean_squared_error: 0.0088\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0780 - mean_squared_error: 0.0118\n",
      "Epoch 4: val_loss improved from 0.07253 to 0.07085, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0780 - mean_squared_error: 0.0118 - val_loss: 0.0708 - val_mean_squared_error: 0.0095\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0766 - mean_squared_error: 0.0119\n",
      "Epoch 5: val_loss improved from 0.07085 to 0.07000, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0766 - mean_squared_error: 0.0119 - val_loss: 0.0700 - val_mean_squared_error: 0.0091\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0718 - mean_squared_error: 0.0104\n",
      "Epoch 6: val_loss improved from 0.07000 to 0.06019, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 8s 331ms/step - loss: 0.0718 - mean_squared_error: 0.0104 - val_loss: 0.0602 - val_mean_squared_error: 0.0072\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0604 - mean_squared_error: 0.0077\n",
      "Epoch 7: val_loss did not improve from 0.06019\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0604 - mean_squared_error: 0.0077 - val_loss: 0.0622 - val_mean_squared_error: 0.0058\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0548 - mean_squared_error: 0.0064\n",
      "Epoch 8: val_loss improved from 0.06019 to 0.04626, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 7s 310ms/step - loss: 0.0548 - mean_squared_error: 0.0064 - val_loss: 0.0463 - val_mean_squared_error: 0.0050\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0514 - mean_squared_error: 0.0057\n",
      "Epoch 9: val_loss did not improve from 0.04626\n",
      "23/23 [==============================] - 8s 330ms/step - loss: 0.0514 - mean_squared_error: 0.0057 - val_loss: 0.0463 - val_mean_squared_error: 0.0047\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0486 - mean_squared_error: 0.0053\n",
      "Epoch 10: val_loss improved from 0.04626 to 0.04489, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 7s 312ms/step - loss: 0.0486 - mean_squared_error: 0.0053 - val_loss: 0.0449 - val_mean_squared_error: 0.0047\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0459 - mean_squared_error: 0.0046\n",
      "Epoch 11: val_loss did not improve from 0.04489\n",
      "23/23 [==============================] - 8s 333ms/step - loss: 0.0459 - mean_squared_error: 0.0046 - val_loss: 0.0463 - val_mean_squared_error: 0.0050\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0457 - mean_squared_error: 0.0047\n",
      "Epoch 12: val_loss did not improve from 0.04489\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 0.0457 - mean_squared_error: 0.0047 - val_loss: 0.0469 - val_mean_squared_error: 0.0052\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0043\n",
      "Epoch 13: val_loss improved from 0.04489 to 0.04372, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn3_filt16_size5_pool2_do0.2_tra4_head8_kdim16_fnn128/weights_3.hdf5\n",
      "23/23 [==============================] - 7s 312ms/step - loss: 0.0442 - mean_squared_error: 0.0043 - val_loss: 0.0437 - val_mean_squared_error: 0.0041\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0040\n",
      "Epoch 14: val_loss did not improve from 0.04372\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 0.0427 - mean_squared_error: 0.0040 - val_loss: 0.0503 - val_mean_squared_error: 0.0060\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0042\n",
      "Epoch 15: val_loss did not improve from 0.04372\n",
      "23/23 [==============================] - 7s 308ms/step - loss: 0.0431 - mean_squared_error: 0.0042 - val_loss: 0.0452 - val_mean_squared_error: 0.0048\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0457 - mean_squared_error: 0.0045\n",
      "Epoch 16: val_loss did not improve from 0.04372\n",
      "23/23 [==============================] - 7s 308ms/step - loss: 0.0457 - mean_squared_error: 0.0045 - val_loss: 0.0543 - val_mean_squared_error: 0.0068\n",
      "31/31 [==============================] - 1s 30ms/step\n",
      " ###3 fold : val mae 0.05###\n",
      "mae0.91+-0.06\n",
      "============================\n",
      "randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1824 - mean_squared_error: 0.0914\n",
      "Epoch 1: val_loss improved from inf to 0.10263, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 14s 172ms/step - loss: 0.1794 - mean_squared_error: 0.0887 - val_loss: 0.1026 - val_mean_squared_error: 0.0196\n",
      "Epoch 2/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1004 - mean_squared_error: 0.0193\n",
      "Epoch 2: val_loss did not improve from 0.10263\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.1000 - mean_squared_error: 0.0191 - val_loss: 0.1027 - val_mean_squared_error: 0.0196\n",
      "Epoch 3/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1004 - mean_squared_error: 0.0191\n",
      "Epoch 3: val_loss did not improve from 0.10263\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 0.1005 - mean_squared_error: 0.0191 - val_loss: 0.1027 - val_mean_squared_error: 0.0196\n",
      "Epoch 4/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1002 - mean_squared_error: 0.0190\n",
      "Epoch 4: val_loss improved from 0.10263 to 0.10256, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 0.1003 - mean_squared_error: 0.0191 - val_loss: 0.1026 - val_mean_squared_error: 0.0196\n",
      "Epoch 5/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1005 - mean_squared_error: 0.0191\n",
      "Epoch 5: val_loss improved from 0.10256 to 0.10244, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.1004 - mean_squared_error: 0.0191 - val_loss: 0.1024 - val_mean_squared_error: 0.0196\n",
      "Epoch 6/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0995 - mean_squared_error: 0.0189\n",
      "Epoch 6: val_loss improved from 0.10244 to 0.10230, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0998 - mean_squared_error: 0.0191 - val_loss: 0.1023 - val_mean_squared_error: 0.0195\n",
      "Epoch 7/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1007 - mean_squared_error: 0.0192\n",
      "Epoch 7: val_loss improved from 0.10230 to 0.10221, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.1007 - mean_squared_error: 0.0192 - val_loss: 0.1022 - val_mean_squared_error: 0.0195\n",
      "Epoch 8/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1012 - mean_squared_error: 0.0197\n",
      "Epoch 8: val_loss improved from 0.10221 to 0.10190, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.1008 - mean_squared_error: 0.0196 - val_loss: 0.1019 - val_mean_squared_error: 0.0195\n",
      "Epoch 9/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1002 - mean_squared_error: 0.0193\n",
      "Epoch 9: val_loss improved from 0.10190 to 0.10175, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.1001 - mean_squared_error: 0.0192 - val_loss: 0.1017 - val_mean_squared_error: 0.0194\n",
      "Epoch 10/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0979 - mean_squared_error: 0.0186\n",
      "Epoch 10: val_loss improved from 0.10175 to 0.10059, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.0986 - mean_squared_error: 0.0188 - val_loss: 0.1006 - val_mean_squared_error: 0.0192\n",
      "Epoch 11/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0987 - mean_squared_error: 0.0191\n",
      "Epoch 11: val_loss improved from 0.10059 to 0.09888, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 0.0991 - mean_squared_error: 0.0192 - val_loss: 0.0989 - val_mean_squared_error: 0.0189\n",
      "Epoch 12/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0980 - mean_squared_error: 0.0186\n",
      "Epoch 12: val_loss did not improve from 0.09888\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 0.0984 - mean_squared_error: 0.0188 - val_loss: 0.0995 - val_mean_squared_error: 0.0190\n",
      "Epoch 13/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0976 - mean_squared_error: 0.0185\n",
      "Epoch 13: val_loss improved from 0.09888 to 0.09859, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 0.0978 - mean_squared_error: 0.0185 - val_loss: 0.0986 - val_mean_squared_error: 0.0188\n",
      "Epoch 14/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0957 - mean_squared_error: 0.0178\n",
      "Epoch 14: val_loss improved from 0.09859 to 0.09071, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_0.hdf5\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 0.0964 - mean_squared_error: 0.0182 - val_loss: 0.0907 - val_mean_squared_error: 0.0171\n",
      "Epoch 15/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0979 - mean_squared_error: 0.0190\n",
      "Epoch 15: val_loss did not improve from 0.09071\n",
      "12/12 [==============================] - 1s 128ms/step - loss: 0.0980 - mean_squared_error: 0.0191 - val_loss: 0.0920 - val_mean_squared_error: 0.0174\n",
      "Epoch 16/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0974 - mean_squared_error: 0.0183\n",
      "Epoch 16: val_loss did not improve from 0.09071\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.0968 - mean_squared_error: 0.0180 - val_loss: 0.0952 - val_mean_squared_error: 0.0181\n",
      "Epoch 17/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0952 - mean_squared_error: 0.0177\n",
      "Epoch 17: val_loss did not improve from 0.09071\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.0957 - mean_squared_error: 0.0179 - val_loss: 0.0912 - val_mean_squared_error: 0.0172\n",
      "31/31 [==============================] - 1s 9ms/step\n",
      " ###0 fold : val mae 0.10###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1825 - mean_squared_error: 0.0894\n",
      "Epoch 1: val_loss improved from inf to 0.10328, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_1.hdf5\n",
      "12/12 [==============================] - 13s 168ms/step - loss: 0.1793 - mean_squared_error: 0.0868 - val_loss: 0.1033 - val_mean_squared_error: 0.0199\n",
      "Epoch 2/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1019 - mean_squared_error: 0.0198\n",
      "Epoch 2: val_loss did not improve from 0.10328\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.1021 - mean_squared_error: 0.0199 - val_loss: 0.1033 - val_mean_squared_error: 0.0199\n",
      "Epoch 3/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0997 - mean_squared_error: 0.0188\n",
      "Epoch 3: val_loss did not improve from 0.10328\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 0.1004 - mean_squared_error: 0.0190 - val_loss: 0.1033 - val_mean_squared_error: 0.0199\n",
      "Epoch 4/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1006 - mean_squared_error: 0.0193\n",
      "Epoch 4: val_loss did not improve from 0.10328\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.1007 - mean_squared_error: 0.0192 - val_loss: 0.1033 - val_mean_squared_error: 0.0199\n",
      "31/31 [==============================] - 1s 10ms/step\n",
      " ###1 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1876 - mean_squared_error: 0.0957\n",
      "Epoch 1: val_loss improved from inf to 0.09829, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_2.hdf5\n",
      "12/12 [==============================] - 17s 218ms/step - loss: 0.1850 - mean_squared_error: 0.0931 - val_loss: 0.0983 - val_mean_squared_error: 0.0185\n",
      "Epoch 2/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1016 - mean_squared_error: 0.0195\n",
      "Epoch 2: val_loss did not improve from 0.09829\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.1020 - mean_squared_error: 0.0196 - val_loss: 0.0983 - val_mean_squared_error: 0.0185\n",
      "Epoch 3/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1031 - mean_squared_error: 0.0201\n",
      "Epoch 3: val_loss did not improve from 0.09829\n",
      "12/12 [==============================] - 1s 127ms/step - loss: 0.1028 - mean_squared_error: 0.0200 - val_loss: 0.0983 - val_mean_squared_error: 0.0185\n",
      "Epoch 4/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1021 - mean_squared_error: 0.0194\n",
      "Epoch 4: val_loss did not improve from 0.09829\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 0.1019 - mean_squared_error: 0.0194 - val_loss: 0.0983 - val_mean_squared_error: 0.0185\n",
      "31/31 [==============================] - 1s 10ms/step\n",
      " ###2 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1836 - mean_squared_error: 0.0917\n",
      "Epoch 1: val_loss improved from inf to 0.10240, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 14s 169ms/step - loss: 0.1808 - mean_squared_error: 0.0892 - val_loss: 0.1024 - val_mean_squared_error: 0.0194\n",
      "Epoch 2/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1006 - mean_squared_error: 0.0192\n",
      "Epoch 2: val_loss did not improve from 0.10240\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.1005 - mean_squared_error: 0.0192 - val_loss: 0.1024 - val_mean_squared_error: 0.0194\n",
      "Epoch 3/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1003 - mean_squared_error: 0.0191\n",
      "Epoch 3: val_loss did not improve from 0.10240\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 0.1003 - mean_squared_error: 0.0192 - val_loss: 0.1024 - val_mean_squared_error: 0.0194\n",
      "Epoch 4/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1003 - mean_squared_error: 0.0191\n",
      "Epoch 4: val_loss improved from 0.10240 to 0.10226, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 0.1004 - mean_squared_error: 0.0191 - val_loss: 0.1023 - val_mean_squared_error: 0.0194\n",
      "Epoch 5/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1006 - mean_squared_error: 0.0193\n",
      "Epoch 5: val_loss improved from 0.10226 to 0.10212, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.1004 - mean_squared_error: 0.0193 - val_loss: 0.1021 - val_mean_squared_error: 0.0193\n",
      "Epoch 6/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1003 - mean_squared_error: 0.0193\n",
      "Epoch 6: val_loss improved from 0.10212 to 0.10197, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.1005 - mean_squared_error: 0.0193 - val_loss: 0.1020 - val_mean_squared_error: 0.0193\n",
      "Epoch 7/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0998 - mean_squared_error: 0.0191\n",
      "Epoch 7: val_loss improved from 0.10197 to 0.10182, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.1003 - mean_squared_error: 0.0193 - val_loss: 0.1018 - val_mean_squared_error: 0.0193\n",
      "Epoch 8/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1010 - mean_squared_error: 0.0194\n",
      "Epoch 8: val_loss did not improve from 0.10182\n",
      "12/12 [==============================] - 1s 128ms/step - loss: 0.1007 - mean_squared_error: 0.0192 - val_loss: 0.1021 - val_mean_squared_error: 0.0193\n",
      "Epoch 9/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1007 - mean_squared_error: 0.0194\n",
      "Epoch 9: val_loss did not improve from 0.10182\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.1008 - mean_squared_error: 0.0194 - val_loss: 0.1019 - val_mean_squared_error: 0.0193\n",
      "Epoch 10/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0998 - mean_squared_error: 0.0189\n",
      "Epoch 10: val_loss improved from 0.10182 to 0.10147, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 0.0999 - mean_squared_error: 0.0190 - val_loss: 0.1015 - val_mean_squared_error: 0.0192\n",
      "Epoch 11/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0996 - mean_squared_error: 0.0191\n",
      "Epoch 11: val_loss improved from 0.10147 to 0.10079, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.0997 - mean_squared_error: 0.0191 - val_loss: 0.1008 - val_mean_squared_error: 0.0191\n",
      "Epoch 12/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0998 - mean_squared_error: 0.0191\n",
      "Epoch 12: val_loss improved from 0.10079 to 0.09981, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 0.0999 - mean_squared_error: 0.0191 - val_loss: 0.0998 - val_mean_squared_error: 0.0189\n",
      "Epoch 13/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0998 - mean_squared_error: 0.0196\n",
      "Epoch 13: val_loss improved from 0.09981 to 0.09885, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 0.0999 - mean_squared_error: 0.0196 - val_loss: 0.0988 - val_mean_squared_error: 0.0187\n",
      "Epoch 14/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0980 - mean_squared_error: 0.0185\n",
      "Epoch 14: val_loss improved from 0.09885 to 0.09736, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0979 - mean_squared_error: 0.0185 - val_loss: 0.0974 - val_mean_squared_error: 0.0184\n",
      "Epoch 15/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0982 - mean_squared_error: 0.0189\n",
      "Epoch 15: val_loss improved from 0.09736 to 0.09679, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0981 - mean_squared_error: 0.0189 - val_loss: 0.0968 - val_mean_squared_error: 0.0183\n",
      "Epoch 16/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0974 - mean_squared_error: 0.0185\n",
      "Epoch 16: val_loss improved from 0.09679 to 0.09672, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.0977 - mean_squared_error: 0.0188 - val_loss: 0.0967 - val_mean_squared_error: 0.0182\n",
      "Epoch 17/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0967 - mean_squared_error: 0.0183\n",
      "Epoch 17: val_loss improved from 0.09672 to 0.09434, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0965 - mean_squared_error: 0.0182 - val_loss: 0.0943 - val_mean_squared_error: 0.0177\n",
      "Epoch 18/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0972 - mean_squared_error: 0.0186\n",
      "Epoch 18: val_loss improved from 0.09434 to 0.09031, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.0971 - mean_squared_error: 0.0185 - val_loss: 0.0903 - val_mean_squared_error: 0.0169\n",
      "Epoch 19/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0968 - mean_squared_error: 0.0187\n",
      "Epoch 19: val_loss did not improve from 0.09031\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 0.0968 - mean_squared_error: 0.0187 - val_loss: 0.0909 - val_mean_squared_error: 0.0170\n",
      "Epoch 20/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0951 - mean_squared_error: 0.0177\n",
      "Epoch 20: val_loss improved from 0.09031 to 0.08605, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 0.0951 - mean_squared_error: 0.0178 - val_loss: 0.0861 - val_mean_squared_error: 0.0159\n",
      "Epoch 21/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0937 - mean_squared_error: 0.0175\n",
      "Epoch 21: val_loss improved from 0.08605 to 0.07659, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.0937 - mean_squared_error: 0.0175 - val_loss: 0.0766 - val_mean_squared_error: 0.0134\n",
      "Epoch 22/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0917 - mean_squared_error: 0.0169\n",
      "Epoch 22: val_loss improved from 0.07659 to 0.07447, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0919 - mean_squared_error: 0.0169 - val_loss: 0.0745 - val_mean_squared_error: 0.0127\n",
      "Epoch 23/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0917 - mean_squared_error: 0.0167\n",
      "Epoch 23: val_loss improved from 0.07447 to 0.07077, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.0916 - mean_squared_error: 0.0166 - val_loss: 0.0708 - val_mean_squared_error: 0.0113\n",
      "Epoch 24/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0900 - mean_squared_error: 0.0161\n",
      "Epoch 24: val_loss improved from 0.07077 to 0.06859, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn4_filt64_size7_pool4_do0.5_tra4_head8_kdim64_fnn32/weights_3.hdf5\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 0.0899 - mean_squared_error: 0.0161 - val_loss: 0.0686 - val_mean_squared_error: 0.0106\n",
      "Epoch 25/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0855 - mean_squared_error: 0.0149\n",
      "Epoch 25: val_loss did not improve from 0.06859\n",
      "12/12 [==============================] - 1s 128ms/step - loss: 0.0855 - mean_squared_error: 0.0149 - val_loss: 0.0747 - val_mean_squared_error: 0.0128\n",
      "Epoch 26/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0802 - mean_squared_error: 0.0136\n",
      "Epoch 26: val_loss did not improve from 0.06859\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 0.0802 - mean_squared_error: 0.0136 - val_loss: 0.0760 - val_mean_squared_error: 0.0125\n",
      "Epoch 27/100\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0759 - mean_squared_error: 0.0120\n",
      "Epoch 27: val_loss did not improve from 0.06859\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.0759 - mean_squared_error: 0.0120 - val_loss: 0.0917 - val_mean_squared_error: 0.0170\n",
      "31/31 [==============================] - 1s 10ms/step\n",
      " ###3 fold : val mae 0.07###\n",
      "mae1.91+-0.28\n",
      "============================\n",
      "randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn2_filt32_size15_pool2_do0.1_tra5_head8_kdim128_fnn64\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 00:25:44.850537: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.96GiB (rounded to 6400000000)requested by op model/multi_head_attention/softmax/Softmax\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-09-22 00:25:44.850759: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-09-22 00:25:44.850781: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 783, Chunks in use: 783. 195.8KiB allocated for chunks. 195.8KiB in use in bin. 114.6KiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850793: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 265, Chunks in use: 264. 136.5KiB allocated for chunks. 136.0KiB in use in bin. 132.0KiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850805: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850816: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 108, Chunks in use: 108. 234.0KiB allocated for chunks. 234.0KiB in use in bin. 216.0KiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850828: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 130, Chunks in use: 130. 563.8KiB allocated for chunks. 563.8KiB in use in bin. 522.1KiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850839: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 104, Chunks in use: 104. 943.2KiB allocated for chunks. 943.2KiB in use in bin. 843.4KiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850850: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 92, Chunks in use: 92. 1.73MiB allocated for chunks. 1.73MiB in use in bin. 1.45MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850860: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 44, Chunks in use: 44. 1.76MiB allocated for chunks. 1.76MiB in use in bin. 1.39MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850870: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 18, Chunks in use: 18. 1.86MiB allocated for chunks. 1.86MiB in use in bin. 1.82MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850882: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 156, Chunks in use: 156. 21.28MiB allocated for chunks. 21.28MiB in use in bin. 19.31MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850894: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 1, Chunks in use: 0. 278.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850916: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 168, Chunks in use: 168. 89.03MiB allocated for chunks. 89.03MiB in use in bin. 84.00MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850926: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850936: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850946: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 5.12MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850956: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 2, Chunks in use: 0. 26.13MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850967: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 6, Chunks in use: 2. 129.46MiB allocated for chunks. 40.61MiB in use in bin. 39.06MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850978: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 9, Chunks in use: 6. 353.95MiB allocated for chunks. 246.22MiB in use in bin. 224.61MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.850990: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 4, Chunks in use: 4. 411.39MiB allocated for chunks. 411.39MiB in use in bin. 312.50MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.851001: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 6, Chunks in use: 4. 1.26GiB allocated for chunks. 897.07MiB in use in bin. 888.29MiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.851012: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 12, Chunks in use: 9. 19.16GiB allocated for chunks. 16.36GiB in use in bin. 16.36GiB client-requested in use in bin.\n",
      "2023-09-22 00:25:44.851022: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 5.96GiB was 256.00MiB, Chunk State: \n",
      "2023-09-22 00:25:44.851046: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 601.69MiB | Requested Size: 249.02MiB | in_use: 0 | bin_num: 20, prev:   Size: 667.88MiB | Requested Size: 667.88MiB | in_use: 1 | bin_num: -1, next:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1\n",
      "2023-09-22 00:25:44.851060: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 1013.88MiB | Requested Size: 512B | in_use: 0 | bin_num: 20, prev:   Size: 5.96GiB | Requested Size: 5.96GiB | in_use: 1 | bin_num: -1\n",
      "2023-09-22 00:25:44.851077: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 1.22GiB | Requested Size: 625.00MiB | in_use: 0 | bin_num: 20, prev:   Size: 5.96GiB | Requested Size: 5.96GiB | in_use: 1 | bin_num: -1, next:   Size: 5.96GiB | Requested Size: 5.96GiB | in_use: 1 | bin_num: -1\n",
      "2023-09-22 00:25:44.851085: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 23023583232\n",
      "2023-09-22 00:25:44.851097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000000 of size 1280 next 1\n",
      "2023-09-22 00:25:44.851107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000500 of size 256 next 2\n",
      "2023-09-22 00:25:44.851116: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000600 of size 256 next 3\n",
      "2023-09-22 00:25:44.851125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000700 of size 256 next 5\n",
      "2023-09-22 00:25:44.851133: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000800 of size 256 next 6\n",
      "2023-09-22 00:25:44.851166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000900 of size 256 next 4\n",
      "2023-09-22 00:25:44.851182: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000a00 of size 256 next 1219\n",
      "2023-09-22 00:25:44.851189: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000b00 of size 256 next 704\n",
      "2023-09-22 00:25:44.851197: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000c00 of size 256 next 953\n",
      "2023-09-22 00:25:44.851204: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000d00 of size 256 next 12\n",
      "2023-09-22 00:25:44.851211: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000e00 of size 256 next 13\n",
      "2023-09-22 00:25:44.851219: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8000f00 of size 256 next 14\n",
      "2023-09-22 00:25:44.851226: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8001000 of size 256 next 1438\n",
      "2023-09-22 00:25:44.851234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8001100 of size 256 next 1327\n",
      "2023-09-22 00:25:44.851241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8001200 of size 256 next 1516\n",
      "2023-09-22 00:25:44.851248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8001300 of size 256 next 9\n",
      "2023-09-22 00:25:44.851256: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8001400 of size 2048 next 369\n",
      "2023-09-22 00:25:44.851266: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8001c00 of size 2048 next 423\n",
      "2023-09-22 00:25:44.851274: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8002400 of size 512 next 72\n",
      "2023-09-22 00:25:44.851282: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8002600 of size 512 next 1543\n",
      "2023-09-22 00:25:44.851289: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8002800 of size 512 next 78\n",
      "2023-09-22 00:25:44.851296: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8002a00 of size 512 next 1211\n",
      "2023-09-22 00:25:44.851304: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8002c00 of size 512 next 561\n",
      "2023-09-22 00:25:44.851311: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8002e00 of size 512 next 556\n",
      "2023-09-22 00:25:44.851319: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8003000 of size 4352 next 28\n",
      "2023-09-22 00:25:44.851326: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004100 of size 256 next 29\n",
      "2023-09-22 00:25:44.851333: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004200 of size 256 next 30\n",
      "2023-09-22 00:25:44.851341: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004300 of size 256 next 1545\n",
      "2023-09-22 00:25:44.851348: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004400 of size 256 next 1578\n",
      "2023-09-22 00:25:44.851355: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004500 of size 256 next 189\n",
      "2023-09-22 00:25:44.851363: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004600 of size 256 next 42\n",
      "2023-09-22 00:25:44.851370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004700 of size 256 next 37\n",
      "2023-09-22 00:25:44.851378: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004800 of size 256 next 36\n",
      "2023-09-22 00:25:44.851385: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8004900 of size 2048 next 1985\n",
      "2023-09-22 00:25:44.851393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8005100 of size 2048 next 2091\n",
      "2023-09-22 00:25:44.851400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8005900 of size 2048 next 32\n",
      "2023-09-22 00:25:44.851407: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006100 of size 256 next 31\n",
      "2023-09-22 00:25:44.851415: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006200 of size 256 next 33\n",
      "2023-09-22 00:25:44.851422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006300 of size 256 next 1660\n",
      "2023-09-22 00:25:44.851430: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006400 of size 256 next 65\n",
      "2023-09-22 00:25:44.851437: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006500 of size 256 next 1416\n",
      "2023-09-22 00:25:44.851445: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006600 of size 256 next 1070\n",
      "2023-09-22 00:25:44.851452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006700 of size 256 next 1822\n",
      "2023-09-22 00:25:44.851466: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006800 of size 256 next 35\n",
      "2023-09-22 00:25:44.851474: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006900 of size 256 next 45\n",
      "2023-09-22 00:25:44.851481: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006a00 of size 256 next 48\n",
      "2023-09-22 00:25:44.851490: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006b00 of size 256 next 49\n",
      "2023-09-22 00:25:44.851498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8006c00 of size 77824 next 1280\n",
      "2023-09-22 00:25:44.851508: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8019c00 of size 4096 next 128\n",
      "2023-09-22 00:25:44.851534: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c801ac00 of size 4096 next 1135\n",
      "2023-09-22 00:25:44.851545: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c801bc00 of size 4096 next 1369\n",
      "2023-09-22 00:25:44.851555: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c801cc00 of size 4096 next 1653\n",
      "2023-09-22 00:25:44.851566: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c801dc00 of size 20480 next 991\n",
      "2023-09-22 00:25:44.851576: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8022c00 of size 4096 next 1387\n",
      "2023-09-22 00:25:44.851586: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8023c00 of size 4096 next 1325\n",
      "2023-09-22 00:25:44.851595: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8024c00 of size 4096 next 1321\n",
      "2023-09-22 00:25:44.851606: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8025c00 of size 4096 next 563\n",
      "2023-09-22 00:25:44.851614: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8026c00 of size 36608 next 105\n",
      "2023-09-22 00:25:44.851623: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c802fb00 of size 256 next 103\n",
      "2023-09-22 00:25:44.851631: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c802fc00 of size 256 next 104\n",
      "2023-09-22 00:25:44.851638: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c802fd00 of size 256 next 107\n",
      "2023-09-22 00:25:44.851647: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c802fe00 of size 256 next 110\n",
      "2023-09-22 00:25:44.851655: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c802ff00 of size 256 next 114\n",
      "2023-09-22 00:25:44.851662: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030000 of size 256 next 115\n",
      "2023-09-22 00:25:44.851671: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030100 of size 256 next 116\n",
      "2023-09-22 00:25:44.851682: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030200 of size 256 next 1418\n",
      "2023-09-22 00:25:44.851692: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030300 of size 256 next 583\n",
      "2023-09-22 00:25:44.851702: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030400 of size 256 next 828\n",
      "2023-09-22 00:25:44.851709: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030500 of size 256 next 1222\n",
      "2023-09-22 00:25:44.851718: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030600 of size 256 next 108\n",
      "2023-09-22 00:25:44.851726: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030700 of size 256 next 109\n",
      "2023-09-22 00:25:44.851734: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26c8030800 of size 700419840 next 694\n",
      "2023-09-22 00:25:44.851745: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c29700 of size 256 next 136\n",
      "2023-09-22 00:25:44.851752: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c29800 of size 256 next 487\n",
      "2023-09-22 00:25:44.851763: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c29900 of size 256 next 475\n",
      "2023-09-22 00:25:44.851773: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c29a00 of size 256 next 655\n",
      "2023-09-22 00:25:44.851781: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c29b00 of size 45056 next 963\n",
      "2023-09-22 00:25:44.851789: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c34b00 of size 256 next 1643\n",
      "2023-09-22 00:25:44.851798: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c34c00 of size 256 next 7\n",
      "2023-09-22 00:25:44.851808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c34d00 of size 256 next 577\n",
      "2023-09-22 00:25:44.851815: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c34e00 of size 256 next 195\n",
      "2023-09-22 00:25:44.851825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c34f00 of size 256 next 1982\n",
      "2023-09-22 00:25:44.851833: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35000 of size 256 next 1939\n",
      "2023-09-22 00:25:44.851840: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35100 of size 256 next 851\n",
      "2023-09-22 00:25:44.851851: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35200 of size 256 next 607\n",
      "2023-09-22 00:25:44.851861: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35300 of size 256 next 1248\n",
      "2023-09-22 00:25:44.851871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35400 of size 256 next 1053\n",
      "2023-09-22 00:25:44.851881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35500 of size 256 next 1171\n",
      "2023-09-22 00:25:44.851891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c35600 of size 8192 next 1878\n",
      "2023-09-22 00:25:44.851900: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c37600 of size 11264 next 421\n",
      "2023-09-22 00:25:44.851909: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3a200 of size 256 next 1265\n",
      "2023-09-22 00:25:44.851919: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3a300 of size 2048 next 1781\n",
      "2023-09-22 00:25:44.851928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3ab00 of size 3840 next 590\n",
      "2023-09-22 00:25:44.851938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3ba00 of size 256 next 730\n",
      "2023-09-22 00:25:44.851947: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3bb00 of size 256 next 672\n",
      "2023-09-22 00:25:44.851954: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3bc00 of size 256 next 1699\n",
      "2023-09-22 00:25:44.851964: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3bd00 of size 768 next 693\n",
      "2023-09-22 00:25:44.851975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c000 of size 256 next 437\n",
      "2023-09-22 00:25:44.851984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c100 of size 256 next 1216\n",
      "2023-09-22 00:25:44.851993: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c200 of size 256 next 97\n",
      "2023-09-22 00:25:44.852002: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c300 of size 256 next 1288\n",
      "2023-09-22 00:25:44.852011: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c400 of size 256 next 388\n",
      "2023-09-22 00:25:44.852021: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c500 of size 256 next 1669\n",
      "2023-09-22 00:25:44.852030: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c600 of size 512 next 1423\n",
      "2023-09-22 00:25:44.852039: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3c800 of size 768 next 162\n",
      "2023-09-22 00:25:44.852047: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3cb00 of size 512 next 1547\n",
      "2023-09-22 00:25:44.852054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3cd00 of size 512 next 874\n",
      "2023-09-22 00:25:44.852061: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3cf00 of size 512 next 978\n",
      "2023-09-22 00:25:44.852069: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d100 of size 512 next 1158\n",
      "2023-09-22 00:25:44.852076: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d300 of size 256 next 156\n",
      "2023-09-22 00:25:44.852085: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d400 of size 256 next 905\n",
      "2023-09-22 00:25:44.852095: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d500 of size 256 next 2025\n",
      "2023-09-22 00:25:44.852106: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d600 of size 256 next 2031\n",
      "2023-09-22 00:25:44.852115: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d700 of size 256 next 2067\n",
      "2023-09-22 00:25:44.852125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d800 of size 256 next 2072\n",
      "2023-09-22 00:25:44.852134: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3d900 of size 256 next 241\n",
      "2023-09-22 00:25:44.852144: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3da00 of size 256 next 967\n",
      "2023-09-22 00:25:44.852153: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3db00 of size 256 next 853\n",
      "2023-09-22 00:25:44.852162: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3dc00 of size 256 next 825\n",
      "2023-09-22 00:25:44.852172: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3dd00 of size 256 next 824\n",
      "2023-09-22 00:25:44.852181: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c3de00 of size 11776 next 1083\n",
      "2023-09-22 00:25:44.852191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c40c00 of size 256 next 216\n",
      "2023-09-22 00:25:44.852200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c40d00 of size 256 next 1444\n",
      "2023-09-22 00:25:44.852209: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c40e00 of size 256 next 383\n",
      "2023-09-22 00:25:44.852218: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c40f00 of size 256 next 1433\n",
      "2023-09-22 00:25:44.852227: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c41000 of size 512 next 1823\n",
      "2023-09-22 00:25:44.852237: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c41200 of size 512 next 1832\n",
      "2023-09-22 00:25:44.852246: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c41400 of size 4096 next 1810\n",
      "2023-09-22 00:25:44.852255: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c42400 of size 6656 next 1047\n",
      "2023-09-22 00:25:44.852265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c43e00 of size 2048 next 1596\n",
      "2023-09-22 00:25:44.852275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c44600 of size 3072 next 271\n",
      "2023-09-22 00:25:44.852284: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c45200 of size 10240 next 1017\n",
      "2023-09-22 00:25:44.852294: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c47a00 of size 16384 next 416\n",
      "2023-09-22 00:25:44.852303: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c4ba00 of size 38400 next 196\n",
      "2023-09-22 00:25:44.852312: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c55000 of size 4096 next 1004\n",
      "2023-09-22 00:25:44.852322: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c56000 of size 4096 next 721\n",
      "2023-09-22 00:25:44.852331: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c57000 of size 4096 next 600\n",
      "2023-09-22 00:25:44.852341: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c58000 of size 4096 next 410\n",
      "2023-09-22 00:25:44.852351: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c59000 of size 4096 next 2017\n",
      "2023-09-22 00:25:44.852359: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c5a000 of size 4096 next 375\n",
      "2023-09-22 00:25:44.852366: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c5b000 of size 4096 next 1364\n",
      "2023-09-22 00:25:44.852374: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c5c000 of size 7680 next 2150\n",
      "2023-09-22 00:25:44.852382: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c5de00 of size 13312 next 781\n",
      "2023-09-22 00:25:44.852393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c61200 of size 256 next 268\n",
      "2023-09-22 00:25:44.852403: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c61300 of size 8192 next 986\n",
      "2023-09-22 00:25:44.852413: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c63300 of size 8192 next 858\n",
      "2023-09-22 00:25:44.852422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c65300 of size 8192 next 219\n",
      "2023-09-22 00:25:44.852433: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c67300 of size 8448 next 1072\n",
      "2023-09-22 00:25:44.852442: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69400 of size 256 next 1329\n",
      "2023-09-22 00:25:44.852452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69500 of size 256 next 629\n",
      "2023-09-22 00:25:44.852461: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69600 of size 256 next 68\n",
      "2023-09-22 00:25:44.852470: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69700 of size 256 next 736\n",
      "2023-09-22 00:25:44.852480: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69800 of size 256 next 142\n",
      "2023-09-22 00:25:44.852489: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69900 of size 256 next 64\n",
      "2023-09-22 00:25:44.852500: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69a00 of size 512 next 253\n",
      "2023-09-22 00:25:44.852509: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69c00 of size 512 next 317\n",
      "2023-09-22 00:25:44.852519: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c69e00 of size 512 next 922\n",
      "2023-09-22 00:25:44.852529: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6a000 of size 512 next 227\n",
      "2023-09-22 00:25:44.852538: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6a200 of size 512 next 301\n",
      "2023-09-22 00:25:44.852548: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6a400 of size 512 next 1648\n",
      "2023-09-22 00:25:44.852557: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6a600 of size 256 next 432\n",
      "2023-09-22 00:25:44.852567: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6a700 of size 256 next 1397\n",
      "2023-09-22 00:25:44.852576: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6a800 of size 512 next 1314\n",
      "2023-09-22 00:25:44.852587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6aa00 of size 512 next 1154\n",
      "2023-09-22 00:25:44.852596: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6ac00 of size 512 next 1138\n",
      "2023-09-22 00:25:44.852605: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6ae00 of size 512 next 1429\n",
      "2023-09-22 00:25:44.852615: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6b000 of size 512 next 624\n",
      "2023-09-22 00:25:44.852624: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6b200 of size 4096 next 1507\n",
      "2023-09-22 00:25:44.852634: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6c200 of size 2048 next 1915\n",
      "2023-09-22 00:25:44.852643: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6ca00 of size 2560 next 242\n",
      "2023-09-22 00:25:44.852652: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6d400 of size 9216 next 1273\n",
      "2023-09-22 00:25:44.852662: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c6f800 of size 10752 next 1607\n",
      "2023-09-22 00:25:44.852670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c72200 of size 9216 next 1065\n",
      "2023-09-22 00:25:44.852679: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c74600 of size 9216 next 1361\n",
      "2023-09-22 00:25:44.852688: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c76a00 of size 10752 next 251\n",
      "2023-09-22 00:25:44.852699: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c79400 of size 17920 next 54\n",
      "2023-09-22 00:25:44.852708: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c7da00 of size 6656 next 1630\n",
      "2023-09-22 00:25:44.852718: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c7f400 of size 16384 next 677\n",
      "2023-09-22 00:25:44.852727: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c83400 of size 8192 next 498\n",
      "2023-09-22 00:25:44.852738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c85400 of size 8192 next 466\n",
      "2023-09-22 00:25:44.852748: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c87400 of size 22272 next 84\n",
      "2023-09-22 00:25:44.852758: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c8cb00 of size 256 next 760\n",
      "2023-09-22 00:25:44.852768: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f1c8cc00 of size 40960000 next 1024\n",
      "2023-09-22 00:25:44.852778: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26f439cc00 of size 122882048 next 2055\n",
      "2023-09-22 00:25:44.852789: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8cd400 of size 2048 next 2059\n",
      "2023-09-22 00:25:44.852799: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8cdc00 of size 2048 next 1720\n",
      "2023-09-22 00:25:44.852808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8ce400 of size 10240 next 1874\n",
      "2023-09-22 00:25:44.852817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8d0c00 of size 8192 next 1719\n",
      "2023-09-22 00:25:44.852828: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8d2c00 of size 8192 next 2068\n",
      "2023-09-22 00:25:44.852837: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8d4c00 of size 8448 next 2074\n",
      "2023-09-22 00:25:44.852847: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8d6d00 of size 131072 next 2077\n",
      "2023-09-22 00:25:44.852856: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb8f6d00 of size 237568 next 2083\n",
      "2023-09-22 00:25:44.852867: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fb930d00 of size 43515904 next 1309\n",
      "2023-09-22 00:25:44.852877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe2b0d00 of size 131072 next 623\n",
      "2023-09-22 00:25:44.852887: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe2d0d00 of size 131072 next 2146\n",
      "2023-09-22 00:25:44.852897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe2f0d00 of size 1048320 next 2090\n",
      "2023-09-22 00:25:44.852906: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe3f0c00 of size 8192 next 1956\n",
      "2023-09-22 00:25:44.852915: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe3f2c00 of size 8192 next 2089\n",
      "2023-09-22 00:25:44.852927: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe3f4c00 of size 8448 next 2095\n",
      "2023-09-22 00:25:44.852936: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe3f6d00 of size 131072 next 1778\n",
      "2023-09-22 00:25:44.852946: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe416d00 of size 237824 next 2116\n",
      "2023-09-22 00:25:44.852956: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe450e00 of size 256 next 2107\n",
      "2023-09-22 00:25:44.852965: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe450f00 of size 256 next 2108\n",
      "2023-09-22 00:25:44.852974: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe451000 of size 256 next 2109\n",
      "2023-09-22 00:25:44.852984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe451100 of size 131072 next 2101\n",
      "2023-09-22 00:25:44.852994: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f26fe471100 of size 37084672 next 154\n",
      "2023-09-22 00:25:44.853003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cef00 of size 256 next 1413\n",
      "2023-09-22 00:25:44.853012: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf000 of size 256 next 379\n",
      "2023-09-22 00:25:44.853022: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf100 of size 256 next 1486\n",
      "2023-09-22 00:25:44.853032: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf200 of size 256 next 159\n",
      "2023-09-22 00:25:44.853041: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf300 of size 256 next 1058\n",
      "2023-09-22 00:25:44.853050: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf400 of size 256 next 147\n",
      "2023-09-22 00:25:44.853059: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf500 of size 768 next 994\n",
      "2023-09-22 00:25:44.853069: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf800 of size 256 next 1424\n",
      "2023-09-22 00:25:44.853078: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cf900 of size 256 next 1029\n",
      "2023-09-22 00:25:44.853087: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cfa00 of size 768 next 574\n",
      "2023-09-22 00:25:44.853097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cfd00 of size 256 next 1408\n",
      "2023-09-22 00:25:44.853106: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cfe00 of size 256 next 1166\n",
      "2023-09-22 00:25:44.853115: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007cff00 of size 256 next 1654\n",
      "2023-09-22 00:25:44.853124: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007d0000 of size 512 next 1122\n",
      "2023-09-22 00:25:44.853134: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007d0200 of size 131072 next 1875\n",
      "2023-09-22 00:25:44.853141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27007f0200 of size 131072 next 1715\n",
      "2023-09-22 00:25:44.853150: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700810200 of size 131072 next 1877\n",
      "2023-09-22 00:25:44.853159: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700830200 of size 132608 next 1734\n",
      "2023-09-22 00:25:44.853169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700850800 of size 147968 next 298\n",
      "2023-09-22 00:25:44.853181: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700874a00 of size 45056 next 1484\n",
      "2023-09-22 00:25:44.853190: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270087fa00 of size 45056 next 73\n",
      "2023-09-22 00:25:44.853199: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270088aa00 of size 45056 next 1614\n",
      "2023-09-22 00:25:44.853209: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700895a00 of size 9216 next 272\n",
      "2023-09-22 00:25:44.853219: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700897e00 of size 9216 next 344\n",
      "2023-09-22 00:25:44.853228: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089a200 of size 256 next 313\n",
      "2023-09-22 00:25:44.853238: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089a300 of size 256 next 786\n",
      "2023-09-22 00:25:44.853247: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089a400 of size 2048 next 849\n",
      "2023-09-22 00:25:44.853257: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089ac00 of size 2048 next 243\n",
      "2023-09-22 00:25:44.853267: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089b400 of size 3584 next 528\n",
      "2023-09-22 00:25:44.853276: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089c200 of size 8192 next 586\n",
      "2023-09-22 00:25:44.853286: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270089e200 of size 13312 next 989\n",
      "2023-09-22 00:25:44.853295: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008a1600 of size 16384 next 1467\n",
      "2023-09-22 00:25:44.853306: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008a5600 of size 16896 next 306\n",
      "2023-09-22 00:25:44.853315: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008a9800 of size 4096 next 309\n",
      "2023-09-22 00:25:44.853325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008aa800 of size 4096 next 773\n",
      "2023-09-22 00:25:44.853334: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008ab800 of size 4096 next 614\n",
      "2023-09-22 00:25:44.853344: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008ac800 of size 4096 next 903\n",
      "2023-09-22 00:25:44.853353: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008ad800 of size 16384 next 350\n",
      "2023-09-22 00:25:44.853363: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008b1800 of size 30720 next 120\n",
      "2023-09-22 00:25:44.853372: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008b9000 of size 27136 next 862\n",
      "2023-09-22 00:25:44.853381: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008bfa00 of size 4096 next 346\n",
      "2023-09-22 00:25:44.853391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008c0a00 of size 4096 next 1224\n",
      "2023-09-22 00:25:44.853400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008c1a00 of size 4096 next 1523\n",
      "2023-09-22 00:25:44.853410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008c2a00 of size 16384 next 89\n",
      "2023-09-22 00:25:44.853419: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008c6a00 of size 16384 next 1259\n",
      "2023-09-22 00:25:44.853428: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008caa00 of size 16384 next 618\n",
      "2023-09-22 00:25:44.853438: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008cea00 of size 4096 next 1689\n",
      "2023-09-22 00:25:44.853447: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008cfa00 of size 4096 next 372\n",
      "2023-09-22 00:25:44.853455: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008d0a00 of size 4096 next 61\n",
      "2023-09-22 00:25:44.853464: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008d1a00 of size 8192 next 1144\n",
      "2023-09-22 00:25:44.853474: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008d3a00 of size 12288 next 613\n",
      "2023-09-22 00:25:44.853483: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008d6a00 of size 6912 next 1589\n",
      "2023-09-22 00:25:44.853492: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008d8500 of size 16128 next 867\n",
      "2023-09-22 00:25:44.853502: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008dc400 of size 70144 next 1107\n",
      "2023-09-22 00:25:44.853513: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27008ed600 of size 524288 next 157\n",
      "2023-09-22 00:25:44.853523: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270096d600 of size 131072 next 1711\n",
      "2023-09-22 00:25:44.853532: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270098d600 of size 237568 next 1887\n",
      "2023-09-22 00:25:44.853541: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27009c7600 of size 131072 next 1738\n",
      "2023-09-22 00:25:44.853551: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27009e7600 of size 237568 next 1899\n",
      "2023-09-22 00:25:44.853560: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a21600 of size 256 next 1901\n",
      "2023-09-22 00:25:44.853570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a21700 of size 2048 next 1895\n",
      "2023-09-22 00:25:44.853580: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a21f00 of size 2048 next 1741\n",
      "2023-09-22 00:25:44.853590: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a22700 of size 2048 next 1739\n",
      "2023-09-22 00:25:44.853599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a22f00 of size 2560 next 1757\n",
      "2023-09-22 00:25:44.853609: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a23900 of size 2048 next 2104\n",
      "2023-09-22 00:25:44.853618: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a24100 of size 2048 next 1968\n",
      "2023-09-22 00:25:44.853627: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a24900 of size 2048 next 598\n",
      "2023-09-22 00:25:44.853637: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a25100 of size 2048 next 1758\n",
      "2023-09-22 00:25:44.853646: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a25900 of size 8192 next 1907\n",
      "2023-09-22 00:25:44.853655: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a27900 of size 8192 next 1910\n",
      "2023-09-22 00:25:44.853665: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a29900 of size 8192 next 1900\n",
      "2023-09-22 00:25:44.853674: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2b900 of size 8192 next 1753\n",
      "2023-09-22 00:25:44.853684: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2d900 of size 2048 next 1716\n",
      "2023-09-22 00:25:44.853693: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2e100 of size 2048 next 1717\n",
      "2023-09-22 00:25:44.853702: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2e900 of size 256 next 2064\n",
      "2023-09-22 00:25:44.853711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2ea00 of size 256 next 619\n",
      "2023-09-22 00:25:44.853721: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2eb00 of size 256 next 2070\n",
      "2023-09-22 00:25:44.853728: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2ec00 of size 256 next 316\n",
      "2023-09-22 00:25:44.853736: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2ed00 of size 256 next 1889\n",
      "2023-09-22 00:25:44.853744: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2ee00 of size 256 next 2066\n",
      "2023-09-22 00:25:44.853754: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2ef00 of size 256 next 1830\n",
      "2023-09-22 00:25:44.853763: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2f000 of size 256 next 1873\n",
      "2023-09-22 00:25:44.853773: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2f100 of size 2048 next 1727\n",
      "2023-09-22 00:25:44.853783: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a2f900 of size 2048 next 531\n",
      "2023-09-22 00:25:44.853792: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a30100 of size 2048 next 1239\n",
      "2023-09-22 00:25:44.853801: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a30900 of size 2048 next 1019\n",
      "2023-09-22 00:25:44.853811: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a31100 of size 2048 next 1728\n",
      "2023-09-22 00:25:44.853820: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a31900 of size 8192 next 1882\n",
      "2023-09-22 00:25:44.853830: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a33900 of size 8448 next 1919\n",
      "2023-09-22 00:25:44.853839: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a35a00 of size 256 next 1762\n",
      "2023-09-22 00:25:44.853849: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a35b00 of size 256 next 1923\n",
      "2023-09-22 00:25:44.853858: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a35c00 of size 256 next 1924\n",
      "2023-09-22 00:25:44.853868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a35d00 of size 256 next 1925\n",
      "2023-09-22 00:25:44.853877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a35e00 of size 256 next 1926\n",
      "2023-09-22 00:25:44.853887: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a35f00 of size 8192 next 1789\n",
      "2023-09-22 00:25:44.853897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a37f00 of size 8192 next 1792\n",
      "2023-09-22 00:25:44.853906: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a39f00 of size 256 next 2106\n",
      "2023-09-22 00:25:44.853916: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a000 of size 256 next 1966\n",
      "2023-09-22 00:25:44.853925: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a100 of size 256 next 1978\n",
      "2023-09-22 00:25:44.853935: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a200 of size 256 next 2120\n",
      "2023-09-22 00:25:44.853945: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a300 of size 256 next 1962\n",
      "2023-09-22 00:25:44.853954: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a400 of size 256 next 2110\n",
      "2023-09-22 00:25:44.853964: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a500 of size 256 next 2105\n",
      "2023-09-22 00:25:44.853973: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a600 of size 256 next 2102\n",
      "2023-09-22 00:25:44.853983: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a700 of size 256 next 2098\n",
      "2023-09-22 00:25:44.853993: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3a800 of size 5888 next 1920\n",
      "2023-09-22 00:25:44.854002: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3bf00 of size 8192 next 1784\n",
      "2023-09-22 00:25:44.854012: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3df00 of size 8192 next 1928\n",
      "2023-09-22 00:25:44.854021: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a3ff00 of size 2048 next 1030\n",
      "2023-09-22 00:25:44.854030: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a40700 of size 2048 next 2042\n",
      "2023-09-22 00:25:44.854038: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a40f00 of size 2048 next 1712\n",
      "2023-09-22 00:25:44.854047: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a41700 of size 2048 next 1933\n",
      "2023-09-22 00:25:44.854056: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a41f00 of size 8192 next 1796\n",
      "2023-09-22 00:25:44.854065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a43f00 of size 8192 next 1800\n",
      "2023-09-22 00:25:44.854074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a45f00 of size 161536 next 667\n",
      "2023-09-22 00:25:44.854084: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700a6d600 of size 524288 next 615\n",
      "2023-09-22 00:25:44.854093: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700aed600 of size 131072 next 1955\n",
      "2023-09-22 00:25:44.854102: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700b0d600 of size 131072 next 1337\n",
      "2023-09-22 00:25:44.854112: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700b2d600 of size 131072 next 2034\n",
      "2023-09-22 00:25:44.854121: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700b4d600 of size 131072 next 772\n",
      "2023-09-22 00:25:44.854130: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700b6d600 of size 131072 next 1953\n",
      "2023-09-22 00:25:44.854140: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700b8d600 of size 131072 next 2084\n",
      "2023-09-22 00:25:44.854149: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700bad600 of size 131072 next 1770\n",
      "2023-09-22 00:25:44.854158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700bcd600 of size 131072 next 1850\n",
      "2023-09-22 00:25:44.854168: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700bed600 of size 131072 next 984\n",
      "2023-09-22 00:25:44.854177: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700c0d600 of size 131072 next 143\n",
      "2023-09-22 00:25:44.854186: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700c2d600 of size 131072 next 2141\n",
      "2023-09-22 00:25:44.854196: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700c4d600 of size 196608 next 336\n",
      "2023-09-22 00:25:44.854205: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700c7d600 of size 131072 next 1989\n",
      "2023-09-22 00:25:44.854214: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700c9d600 of size 131072 next 1230\n",
      "2023-09-22 00:25:44.854224: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700cbd600 of size 131072 next 1983\n",
      "2023-09-22 00:25:44.854234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700cdd600 of size 131072 next 578\n",
      "2023-09-22 00:25:44.854243: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700cfd600 of size 131072 next 1961\n",
      "2023-09-22 00:25:44.854253: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700d1d600 of size 131072 next 1209\n",
      "2023-09-22 00:25:44.854262: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700d3d600 of size 131072 next 2145\n",
      "2023-09-22 00:25:44.854271: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700d5d600 of size 131072 next 1940\n",
      "2023-09-22 00:25:44.854280: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700d7d600 of size 131072 next 2082\n",
      "2023-09-22 00:25:44.854290: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700d9d600 of size 131072 next 1852\n",
      "2023-09-22 00:25:44.854299: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700dbd600 of size 196608 next 597\n",
      "2023-09-22 00:25:44.854309: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700ded600 of size 524288 next 1683\n",
      "2023-09-22 00:25:44.854318: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700e6d600 of size 524288 next 111\n",
      "2023-09-22 00:25:44.854328: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700eed600 of size 524288 next 1146\n",
      "2023-09-22 00:25:44.854335: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700f6d600 of size 524288 next 1268\n",
      "2023-09-22 00:25:44.854344: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2700fed600 of size 524288 next 1267\n",
      "2023-09-22 00:25:44.854354: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270106d600 of size 524288 next 1557\n",
      "2023-09-22 00:25:44.854363: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27010ed600 of size 524288 next 1415\n",
      "2023-09-22 00:25:44.854373: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270116d600 of size 524288 next 335\n",
      "2023-09-22 00:25:44.854382: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27011ed600 of size 524288 next 288\n",
      "2023-09-22 00:25:44.854391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270126d600 of size 524288 next 1602\n",
      "2023-09-22 00:25:44.854400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27012ed600 of size 524288 next 1625\n",
      "2023-09-22 00:25:44.854410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270136d600 of size 524288 next 1632\n",
      "2023-09-22 00:25:44.854419: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27013ed600 of size 524288 next 234\n",
      "2023-09-22 00:25:44.854429: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270146d600 of size 524288 next 1220\n",
      "2023-09-22 00:25:44.854439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27014ed600 of size 524288 next 898\n",
      "2023-09-22 00:25:44.854449: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f270156d600 of size 524288 next 397\n",
      "2023-09-22 00:25:44.854458: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27015ed600 of size 994816 next 930\n",
      "2023-09-22 00:25:44.854467: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e0400 of size 256 next 718\n",
      "2023-09-22 00:25:44.854477: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e0500 of size 256 next 254\n",
      "2023-09-22 00:25:44.854486: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e0600 of size 2048 next 1864\n",
      "2023-09-22 00:25:44.854495: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e0e00 of size 2048 next 2003\n",
      "2023-09-22 00:25:44.854505: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e1600 of size 2816 next 1275\n",
      "2023-09-22 00:25:44.854514: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2100 of size 256 next 1277\n",
      "2023-09-22 00:25:44.854523: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2200 of size 256 next 1302\n",
      "2023-09-22 00:25:44.854533: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2300 of size 256 next 381\n",
      "2023-09-22 00:25:44.854543: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2400 of size 256 next 431\n",
      "2023-09-22 00:25:44.854552: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2500 of size 256 next 334\n",
      "2023-09-22 00:25:44.854561: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2600 of size 256 next 183\n",
      "2023-09-22 00:25:44.854571: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27016e2700 of size 20971520 next 2097\n",
      "2023-09-22 00:25:44.854580: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae2700 of size 8192 next 2007\n",
      "2023-09-22 00:25:44.854589: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae4700 of size 512 next 1086\n",
      "2023-09-22 00:25:44.854599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae4900 of size 512 next 106\n",
      "2023-09-22 00:25:44.854608: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae4b00 of size 512 next 1705\n",
      "2023-09-22 00:25:44.854617: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae4d00 of size 512 next 1380\n",
      "2023-09-22 00:25:44.854626: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae4f00 of size 512 next 1520\n",
      "2023-09-22 00:25:44.854635: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae5100 of size 512 next 323\n",
      "2023-09-22 00:25:44.854643: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae5300 of size 4096 next 1651\n",
      "2023-09-22 00:25:44.854652: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae6300 of size 4096 next 358\n",
      "2023-09-22 00:25:44.854662: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae7300 of size 4096 next 217\n",
      "2023-09-22 00:25:44.854671: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae8300 of size 4096 next 1347\n",
      "2023-09-22 00:25:44.854680: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702ae9300 of size 4096 next 1582\n",
      "2023-09-22 00:25:44.854690: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aea300 of size 4096 next 689\n",
      "2023-09-22 00:25:44.854699: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aeb300 of size 512 next 1672\n",
      "2023-09-22 00:25:44.854709: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aeb500 of size 512 next 203\n",
      "2023-09-22 00:25:44.854718: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aeb700 of size 512 next 1201\n",
      "2023-09-22 00:25:44.854727: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aeb900 of size 512 next 653\n",
      "2023-09-22 00:25:44.854737: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aebb00 of size 512 next 705\n",
      "2023-09-22 00:25:44.854746: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aebd00 of size 512 next 405\n",
      "2023-09-22 00:25:44.854756: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702aebf00 of size 32768 next 632\n",
      "2023-09-22 00:25:44.854768: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702af3f00 of size 32768 next 10\n",
      "2023-09-22 00:25:44.854777: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702afbf00 of size 256 next 1403\n",
      "2023-09-22 00:25:44.854785: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702afc000 of size 256 next 380\n",
      "2023-09-22 00:25:44.854795: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702afc100 of size 32768 next 1297\n",
      "2023-09-22 00:25:44.854805: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b04100 of size 32768 next 573\n",
      "2023-09-22 00:25:44.854814: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0c100 of size 512 next 140\n",
      "2023-09-22 00:25:44.854824: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0c300 of size 512 next 1619\n",
      "2023-09-22 00:25:44.854833: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0c500 of size 512 next 1703\n",
      "2023-09-22 00:25:44.854843: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0c700 of size 512 next 194\n",
      "2023-09-22 00:25:44.854853: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0c900 of size 512 next 158\n",
      "2023-09-22 00:25:44.854862: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0cb00 of size 512 next 163\n",
      "2023-09-22 00:25:44.854872: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b0cd00 of size 49152 next 2119\n",
      "2023-09-22 00:25:44.854881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b18d00 of size 8192 next 1976\n",
      "2023-09-22 00:25:44.854891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2702b1ad00 of size 8192 next 2118\n",
      "2023-09-22 00:25:44.854901: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2702b1cd00 of size 231158784 next 1188\n",
      "2023-09-22 00:25:44.854910: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2710790100 of size 256 next 1344\n",
      "2023-09-22 00:25:44.854920: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2710790200 of size 105512960 next 204\n",
      "2023-09-22 00:25:44.854929: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716c30200 of size 524288 next 218\n",
      "2023-09-22 00:25:44.854938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716cb0200 of size 524288 next 441\n",
      "2023-09-22 00:25:44.854948: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716d30200 of size 131072 next 449\n",
      "2023-09-22 00:25:44.854957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716d50200 of size 135168 next 899\n",
      "2023-09-22 00:25:44.854968: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716d71200 of size 131072 next 2099\n",
      "2023-09-22 00:25:44.854977: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716d91200 of size 153088 next 1963\n",
      "2023-09-22 00:25:44.854987: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716db6800 of size 8192 next 1964\n",
      "2023-09-22 00:25:44.854997: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716db8800 of size 8704 next 1967\n",
      "2023-09-22 00:25:44.855006: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbaa00 of size 8192 next 1969\n",
      "2023-09-22 00:25:44.855016: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbca00 of size 10240 next 1977\n",
      "2023-09-22 00:25:44.855025: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbf200 of size 256 next 2122\n",
      "2023-09-22 00:25:44.855035: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbf300 of size 256 next 2117\n",
      "2023-09-22 00:25:44.855044: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbf400 of size 256 next 2060\n",
      "2023-09-22 00:25:44.855053: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbf500 of size 256 next 1981\n",
      "2023-09-22 00:25:44.855063: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbf600 of size 2304 next 341\n",
      "2023-09-22 00:25:44.855072: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dbff00 of size 3584 next 2127\n",
      "2023-09-22 00:25:44.855081: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc0d00 of size 256 next 1710\n",
      "2023-09-22 00:25:44.855091: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc0e00 of size 256 next 2121\n",
      "2023-09-22 00:25:44.855099: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc0f00 of size 256 next 2123\n",
      "2023-09-22 00:25:44.855107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1000 of size 256 next 2009\n",
      "2023-09-22 00:25:44.855117: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1100 of size 256 next 2008\n",
      "2023-09-22 00:25:44.855125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1200 of size 256 next 1980\n",
      "2023-09-22 00:25:44.855132: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1300 of size 256 next 1974\n",
      "2023-09-22 00:25:44.855146: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1400 of size 256 next 1975\n",
      "2023-09-22 00:25:44.855154: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1500 of size 256 next 2010\n",
      "2023-09-22 00:25:44.855161: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1600 of size 256 next 1774\n",
      "2023-09-22 00:25:44.855172: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1700 of size 256 next 2015\n",
      "2023-09-22 00:25:44.855183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1800 of size 256 next 2144\n",
      "2023-09-22 00:25:44.855193: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1900 of size 256 next 1979\n",
      "2023-09-22 00:25:44.855203: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1a00 of size 256 next 2020\n",
      "2023-09-22 00:25:44.855213: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1b00 of size 256 next 2021\n",
      "2023-09-22 00:25:44.855224: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1c00 of size 256 next 2022\n",
      "2023-09-22 00:25:44.855234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1d00 of size 256 next 2023\n",
      "2023-09-22 00:25:44.855244: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1e00 of size 256 next 133\n",
      "2023-09-22 00:25:44.855253: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc1f00 of size 256 next 1818\n",
      "2023-09-22 00:25:44.855263: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2000 of size 256 next 2027\n",
      "2023-09-22 00:25:44.855272: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2100 of size 256 next 2028\n",
      "2023-09-22 00:25:44.855282: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2200 of size 256 next 2029\n",
      "2023-09-22 00:25:44.855291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2300 of size 256 next 2030\n",
      "2023-09-22 00:25:44.855301: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2400 of size 256 next 1817\n",
      "2023-09-22 00:25:44.855310: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2500 of size 256 next 1824\n",
      "2023-09-22 00:25:44.855320: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2600 of size 256 next 2035\n",
      "2023-09-22 00:25:44.855329: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2700 of size 256 next 2044\n",
      "2023-09-22 00:25:44.855339: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2800 of size 256 next 2046\n",
      "2023-09-22 00:25:44.855348: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2900 of size 256 next 2038\n",
      "2023-09-22 00:25:44.855358: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2a00 of size 256 next 2039\n",
      "2023-09-22 00:25:44.855367: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2b00 of size 256 next 2040\n",
      "2023-09-22 00:25:44.855377: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2c00 of size 256 next 2041\n",
      "2023-09-22 00:25:44.855386: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc2d00 of size 16384 next 1805\n",
      "2023-09-22 00:25:44.855395: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dc6d00 of size 30720 next 2013\n",
      "2023-09-22 00:25:44.855405: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dce500 of size 256 next 2014\n",
      "2023-09-22 00:25:44.855412: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dce600 of size 131072 next 1709\n",
      "2023-09-22 00:25:44.855421: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716dee600 of size 114688 next 1996\n",
      "2023-09-22 00:25:44.855431: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716e0a600 of size 154624 next 687\n",
      "2023-09-22 00:25:44.855442: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716e30200 of size 524288 next 1512\n",
      "2023-09-22 00:25:44.855452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716eb0200 of size 524288 next 744\n",
      "2023-09-22 00:25:44.855461: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716f30200 of size 131072 next 1007\n",
      "2023-09-22 00:25:44.855470: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716f50200 of size 131072 next 1076\n",
      "2023-09-22 00:25:44.855480: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716f70200 of size 131072 next 472\n",
      "2023-09-22 00:25:44.855489: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716f90200 of size 208896 next 146\n",
      "2023-09-22 00:25:44.855498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc3200 of size 4096 next 907\n",
      "2023-09-22 00:25:44.855508: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc4200 of size 8192 next 1316\n",
      "2023-09-22 00:25:44.855517: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc6200 of size 512 next 1650\n",
      "2023-09-22 00:25:44.855527: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc6400 of size 256 next 23\n",
      "2023-09-22 00:25:44.855537: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc6500 of size 256 next 627\n",
      "2023-09-22 00:25:44.855546: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc6600 of size 256 next 1141\n",
      "2023-09-22 00:25:44.855555: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc6700 of size 256 next 1390\n",
      "2023-09-22 00:25:44.855564: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc6800 of size 2048 next 462\n",
      "2023-09-22 00:25:44.855574: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc7000 of size 2048 next 1656\n",
      "2023-09-22 00:25:44.855583: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc7800 of size 2048 next 1639\n",
      "2023-09-22 00:25:44.855593: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8000 of size 512 next 181\n",
      "2023-09-22 00:25:44.855602: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8200 of size 512 next 67\n",
      "2023-09-22 00:25:44.855611: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8400 of size 512 next 1059\n",
      "2023-09-22 00:25:44.855621: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8600 of size 512 next 1085\n",
      "2023-09-22 00:25:44.855630: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8800 of size 512 next 446\n",
      "2023-09-22 00:25:44.855639: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8a00 of size 512 next 551\n",
      "2023-09-22 00:25:44.855649: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8c00 of size 512 next 965\n",
      "2023-09-22 00:25:44.855659: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc8e00 of size 512 next 1163\n",
      "2023-09-22 00:25:44.855668: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fc9000 of size 4608 next 1097\n",
      "2023-09-22 00:25:44.855678: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca200 of size 256 next 1052\n",
      "2023-09-22 00:25:44.855687: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca300 of size 256 next 1704\n",
      "2023-09-22 00:25:44.855696: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca400 of size 256 next 891\n",
      "2023-09-22 00:25:44.855705: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca500 of size 256 next 1001\n",
      "2023-09-22 00:25:44.855715: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca600 of size 256 next 1149\n",
      "2023-09-22 00:25:44.855722: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca700 of size 256 next 1571\n",
      "2023-09-22 00:25:44.855731: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca800 of size 256 next 1592\n",
      "2023-09-22 00:25:44.855741: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fca900 of size 256 next 635\n",
      "2023-09-22 00:25:44.855750: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcaa00 of size 256 next 492\n",
      "2023-09-22 00:25:44.855760: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcab00 of size 256 next 1046\n",
      "2023-09-22 00:25:44.855769: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcac00 of size 256 next 1342\n",
      "2023-09-22 00:25:44.855779: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcad00 of size 768 next 390\n",
      "2023-09-22 00:25:44.855788: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb000 of size 256 next 1358\n",
      "2023-09-22 00:25:44.855797: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb100 of size 256 next 890\n",
      "2023-09-22 00:25:44.855807: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb200 of size 256 next 879\n",
      "2023-09-22 00:25:44.855816: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb300 of size 256 next 1196\n",
      "2023-09-22 00:25:44.855825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb400 of size 512 next 697\n",
      "2023-09-22 00:25:44.855835: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb600 of size 256 next 848\n",
      "2023-09-22 00:25:44.855844: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb700 of size 256 next 457\n",
      "2023-09-22 00:25:44.855853: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcb800 of size 512 next 1119\n",
      "2023-09-22 00:25:44.855863: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcba00 of size 512 next 392\n",
      "2023-09-22 00:25:44.855872: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcbc00 of size 512 next 553\n",
      "2023-09-22 00:25:44.855882: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcbe00 of size 512 next 663\n",
      "2023-09-22 00:25:44.855891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcc000 of size 768 next 710\n",
      "2023-09-22 00:25:44.855900: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcc300 of size 256 next 683\n",
      "2023-09-22 00:25:44.855910: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcc400 of size 512 next 1451\n",
      "2023-09-22 00:25:44.855919: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcc600 of size 512 next 1626\n",
      "2023-09-22 00:25:44.855928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcc800 of size 512 next 1502\n",
      "2023-09-22 00:25:44.855938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcca00 of size 512 next 8\n",
      "2023-09-22 00:25:44.855947: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fccc00 of size 512 next 66\n",
      "2023-09-22 00:25:44.855956: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcce00 of size 512 next 648\n",
      "2023-09-22 00:25:44.855966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd000 of size 512 next 988\n",
      "2023-09-22 00:25:44.855975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd200 of size 256 next 1035\n",
      "2023-09-22 00:25:44.855985: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd300 of size 256 next 207\n",
      "2023-09-22 00:25:44.855994: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd400 of size 256 next 1525\n",
      "2023-09-22 00:25:44.856003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd500 of size 256 next 904\n",
      "2023-09-22 00:25:44.856013: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd600 of size 256 next 249\n",
      "2023-09-22 00:25:44.856023: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd700 of size 256 next 732\n",
      "2023-09-22 00:25:44.856030: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd800 of size 256 next 34\n",
      "2023-09-22 00:25:44.856040: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcd900 of size 256 next 606\n",
      "2023-09-22 00:25:44.856049: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcda00 of size 256 next 918\n",
      "2023-09-22 00:25:44.856059: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcdb00 of size 256 next 742\n",
      "2023-09-22 00:25:44.856068: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcdc00 of size 256 next 861\n",
      "2023-09-22 00:25:44.856078: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcdd00 of size 256 next 1509\n",
      "2023-09-22 00:25:44.856088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcde00 of size 256 next 1566\n",
      "2023-09-22 00:25:44.856097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fcdf00 of size 256 next 412\n",
      "2023-09-22 00:25:44.856107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fce000 of size 256 next 208\n",
      "2023-09-22 00:25:44.856117: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fce100 of size 256 next 279\n",
      "2023-09-22 00:25:44.856127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fce200 of size 32768 next 411\n",
      "2023-09-22 00:25:44.856136: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fd6200 of size 32768 next 152\n",
      "2023-09-22 00:25:44.856145: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fde200 of size 512 next 236\n",
      "2023-09-22 00:25:44.856155: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fde400 of size 512 next 634\n",
      "2023-09-22 00:25:44.856164: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fde600 of size 512 next 1402\n",
      "2023-09-22 00:25:44.856173: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fde800 of size 512 next 811\n",
      "2023-09-22 00:25:44.856183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fdea00 of size 512 next 764\n",
      "2023-09-22 00:25:44.856192: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fdec00 of size 512 next 537\n",
      "2023-09-22 00:25:44.856202: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fdee00 of size 4096 next 1528\n",
      "2023-09-22 00:25:44.856212: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fdfe00 of size 4096 next 1334\n",
      "2023-09-22 00:25:44.856221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe0e00 of size 4096 next 589\n",
      "2023-09-22 00:25:44.856231: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe1e00 of size 4096 next 445\n",
      "2023-09-22 00:25:44.856240: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe2e00 of size 4096 next 192\n",
      "2023-09-22 00:25:44.856249: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe3e00 of size 4096 next 1054\n",
      "2023-09-22 00:25:44.856259: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe4e00 of size 512 next 493\n",
      "2023-09-22 00:25:44.856268: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5000 of size 512 next 1232\n",
      "2023-09-22 00:25:44.856276: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5200 of size 512 next 1200\n",
      "2023-09-22 00:25:44.856283: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5400 of size 512 next 539\n",
      "2023-09-22 00:25:44.856292: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5600 of size 512 next 300\n",
      "2023-09-22 00:25:44.856302: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5800 of size 512 next 1213\n",
      "2023-09-22 00:25:44.856312: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5a00 of size 256 next 1159\n",
      "2023-09-22 00:25:44.856321: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5b00 of size 256 next 235\n",
      "2023-09-22 00:25:44.856331: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5c00 of size 512 next 1662\n",
      "2023-09-22 00:25:44.856338: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe5e00 of size 512 next 1306\n",
      "2023-09-22 00:25:44.856347: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6000 of size 512 next 659\n",
      "2023-09-22 00:25:44.856357: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6200 of size 512 next 1214\n",
      "2023-09-22 00:25:44.856366: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6400 of size 512 next 1258\n",
      "2023-09-22 00:25:44.856376: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6600 of size 512 next 87\n",
      "2023-09-22 00:25:44.856386: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6800 of size 512 next 728\n",
      "2023-09-22 00:25:44.856395: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6a00 of size 512 next 1362\n",
      "2023-09-22 00:25:44.856404: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6c00 of size 512 next 852\n",
      "2023-09-22 00:25:44.856414: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe6e00 of size 512 next 482\n",
      "2023-09-22 00:25:44.856423: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe7000 of size 256 next 625\n",
      "2023-09-22 00:25:44.856433: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe7100 of size 256 next 1377\n",
      "2023-09-22 00:25:44.856442: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe7200 of size 2048 next 1612\n",
      "2023-09-22 00:25:44.856451: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe7a00 of size 2048 next 113\n",
      "2023-09-22 00:25:44.856460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fe8200 of size 16384 next 1573\n",
      "2023-09-22 00:25:44.856470: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fec200 of size 8192 next 977\n",
      "2023-09-22 00:25:44.856479: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716fee200 of size 9216 next 1142\n",
      "2023-09-22 00:25:44.856488: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716ff0600 of size 256 next 139\n",
      "2023-09-22 00:25:44.856498: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2716ff0700 of size 63744 next 121\n",
      "2023-09-22 00:25:44.856507: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000000 of size 256 next 481\n",
      "2023-09-22 00:25:44.856517: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000100 of size 256 next 443\n",
      "2023-09-22 00:25:44.856526: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000200 of size 256 next 471\n",
      "2023-09-22 00:25:44.856536: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000300 of size 256 next 275\n",
      "2023-09-22 00:25:44.856546: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000400 of size 256 next 1519\n",
      "2023-09-22 00:25:44.856555: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000500 of size 256 next 1748\n",
      "2023-09-22 00:25:44.856564: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000600 of size 256 next 51\n",
      "2023-09-22 00:25:44.856574: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000700 of size 256 next 2130\n",
      "2023-09-22 00:25:44.856584: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000800 of size 256 next 616\n",
      "2023-09-22 00:25:44.856593: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000900 of size 256 next 1003\n",
      "2023-09-22 00:25:44.856603: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000a00 of size 256 next 324\n",
      "2023-09-22 00:25:44.856612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000b00 of size 256 next 1879\n",
      "2023-09-22 00:25:44.856622: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000c00 of size 256 next 1386\n",
      "2023-09-22 00:25:44.856631: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000d00 of size 256 next 311\n",
      "2023-09-22 00:25:44.856641: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000e00 of size 256 next 1498\n",
      "2023-09-22 00:25:44.856648: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717000f00 of size 256 next 396\n",
      "2023-09-22 00:25:44.856657: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717001000 of size 256 next 1223\n",
      "2023-09-22 00:25:44.856667: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717001100 of size 256 next 1680\n",
      "2023-09-22 00:25:44.856676: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717001200 of size 22272 next 731\n",
      "2023-09-22 00:25:44.856685: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006900 of size 256 next 1068\n",
      "2023-09-22 00:25:44.856695: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006a00 of size 256 next 593\n",
      "2023-09-22 00:25:44.856704: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006b00 of size 256 next 1021\n",
      "2023-09-22 00:25:44.856715: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006c00 of size 256 next 1697\n",
      "2023-09-22 00:25:44.856724: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006d00 of size 256 next 675\n",
      "2023-09-22 00:25:44.856734: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006e00 of size 256 next 1667\n",
      "2023-09-22 00:25:44.856743: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717006f00 of size 256 next 990\n",
      "2023-09-22 00:25:44.856752: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007000 of size 256 next 831\n",
      "2023-09-22 00:25:44.856762: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007100 of size 256 next 871\n",
      "2023-09-22 00:25:44.856771: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007200 of size 2304 next 333\n",
      "2023-09-22 00:25:44.856780: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007b00 of size 256 next 246\n",
      "2023-09-22 00:25:44.856790: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007c00 of size 256 next 1694\n",
      "2023-09-22 00:25:44.856799: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007d00 of size 512 next 232\n",
      "2023-09-22 00:25:44.856808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717007f00 of size 768 next 857\n",
      "2023-09-22 00:25:44.856818: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717008200 of size 256 next 671\n",
      "2023-09-22 00:25:44.856827: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717008300 of size 256 next 870\n",
      "2023-09-22 00:25:44.856836: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717008400 of size 256 next 549\n",
      "2023-09-22 00:25:44.856846: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717008500 of size 45056 next 1056\n",
      "2023-09-22 00:25:44.856855: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717013500 of size 32768 next 50\n",
      "2023-09-22 00:25:44.856864: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271701b500 of size 32768 next 581\n",
      "2023-09-22 00:25:44.856874: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717023500 of size 39680 next 394\n",
      "2023-09-22 00:25:44.856883: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271702d000 of size 4096 next 320\n",
      "2023-09-22 00:25:44.856892: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271702e000 of size 131072 next 171\n",
      "2023-09-22 00:25:44.856902: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271704e000 of size 131072 next 1535\n",
      "2023-09-22 00:25:44.856911: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271706e000 of size 16384 next 1880\n",
      "2023-09-22 00:25:44.856921: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717072000 of size 16384 next 1690\n",
      "2023-09-22 00:25:44.856930: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717076000 of size 21504 next 2019\n",
      "2023-09-22 00:25:44.856940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271707b400 of size 64512 next 2011\n",
      "2023-09-22 00:25:44.856949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271708b000 of size 8192 next 1341\n",
      "2023-09-22 00:25:44.856957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271708d000 of size 8704 next 567\n",
      "2023-09-22 00:25:44.856966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271708f200 of size 11776 next 1802\n",
      "2023-09-22 00:25:44.856976: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717092000 of size 123392 next 737\n",
      "2023-09-22 00:25:44.856985: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27170b0200 of size 524288 next 261\n",
      "2023-09-22 00:25:44.856995: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2717130200 of size 100270336 next 686\n",
      "2023-09-22 00:25:44.857007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d0d0300 of size 256 next 137\n",
      "2023-09-22 00:25:44.857017: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d0d0400 of size 256 next 1616\n",
      "2023-09-22 00:25:44.857027: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d0d0500 of size 524288 next 220\n",
      "2023-09-22 00:25:44.857036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d150500 of size 524288 next 96\n",
      "2023-09-22 00:25:44.857045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d1d0500 of size 524288 next 1117\n",
      "2023-09-22 00:25:44.857055: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d250500 of size 524288 next 987\n",
      "2023-09-22 00:25:44.857065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d2d0500 of size 524288 next 754\n",
      "2023-09-22 00:25:44.857074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d350500 of size 524288 next 260\n",
      "2023-09-22 00:25:44.857084: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d3d0500 of size 524288 next 1134\n",
      "2023-09-22 00:25:44.857094: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d450500 of size 524288 next 188\n",
      "2023-09-22 00:25:44.857103: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d4d0500 of size 524288 next 791\n",
      "2023-09-22 00:25:44.857113: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d550500 of size 524288 next 1330\n",
      "2023-09-22 00:25:44.857122: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d5d0500 of size 524288 next 1195\n",
      "2023-09-22 00:25:44.857131: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d650500 of size 524288 next 452\n",
      "2023-09-22 00:25:44.857141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d6d0500 of size 524288 next 1398\n",
      "2023-09-22 00:25:44.857150: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d750500 of size 524288 next 424\n",
      "2023-09-22 00:25:44.857159: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d7d0500 of size 524288 next 1500\n",
      "2023-09-22 00:25:44.857169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d850500 of size 524288 next 1537\n",
      "2023-09-22 00:25:44.857178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d8d0500 of size 524288 next 917\n",
      "2023-09-22 00:25:44.857188: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d950500 of size 131072 next 1737\n",
      "2023-09-22 00:25:44.857197: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d970500 of size 131072 next 1581\n",
      "2023-09-22 00:25:44.857206: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d990500 of size 131072 next 1876\n",
      "2023-09-22 00:25:44.857216: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d9b0500 of size 135168 next 1742\n",
      "2023-09-22 00:25:44.857225: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d9d1500 of size 176128 next 1136\n",
      "2023-09-22 00:25:44.857234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271d9fc500 of size 16384 next 273\n",
      "2023-09-22 00:25:44.857244: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271da00500 of size 16384 next 1207\n",
      "2023-09-22 00:25:44.857253: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271da04500 of size 131072 next 1745\n",
      "2023-09-22 00:25:44.857261: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271da24500 of size 167424 next 1764\n",
      "2023-09-22 00:25:44.857270: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271da4d300 of size 131072 next 1765\n",
      "2023-09-22 00:25:44.857279: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271da6d300 of size 135168 next 1768\n",
      "2023-09-22 00:25:44.857289: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271da8e300 of size 254464 next 847\n",
      "2023-09-22 00:25:44.857299: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dacc500 of size 524288 next 939\n",
      "2023-09-22 00:25:44.857308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271db4c500 of size 540672 next 1496\n",
      "2023-09-22 00:25:44.857317: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dbd0500 of size 524288 next 1635\n",
      "2023-09-22 00:25:44.857326: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dc50500 of size 524288 next 536\n",
      "2023-09-22 00:25:44.857336: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dcd0500 of size 524288 next 1636\n",
      "2023-09-22 00:25:44.857346: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dd50500 of size 524288 next 753\n",
      "2023-09-22 00:25:44.857355: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ddd0500 of size 114688 next 533\n",
      "2023-09-22 00:25:44.857365: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ddec500 of size 158720 next 337\n",
      "2023-09-22 00:25:44.857374: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de13100 of size 256 next 331\n",
      "2023-09-22 00:25:44.857383: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de13200 of size 256 next 193\n",
      "2023-09-22 00:25:44.857393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de13300 of size 8704 next 1673\n",
      "2023-09-22 00:25:44.857402: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de15500 of size 256 next 1124\n",
      "2023-09-22 00:25:44.857411: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de15600 of size 8448 next 451\n",
      "2023-09-22 00:25:44.857420: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de17700 of size 256 next 1089\n",
      "2023-09-22 00:25:44.857430: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de17800 of size 256 next 640\n",
      "2023-09-22 00:25:44.857439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de17900 of size 256 next 2164\n",
      "2023-09-22 00:25:44.857448: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de17a00 of size 256 next 2165\n",
      "2023-09-22 00:25:44.857458: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de17b00 of size 256 next 225\n",
      "2023-09-22 00:25:44.857468: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de17c00 of size 2048 next 1862\n",
      "2023-09-22 00:25:44.857475: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de18400 of size 2048 next 1854\n",
      "2023-09-22 00:25:44.857483: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de18c00 of size 256 next 354\n",
      "2023-09-22 00:25:44.857492: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de18d00 of size 256 next 1811\n",
      "2023-09-22 00:25:44.857501: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de18e00 of size 512 next 1554\n",
      "2023-09-22 00:25:44.857511: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19000 of size 512 next 674\n",
      "2023-09-22 00:25:44.857520: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19200 of size 512 next 1676\n",
      "2023-09-22 00:25:44.857530: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19400 of size 256 next 2094\n",
      "2023-09-22 00:25:44.857539: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19500 of size 256 next 1570\n",
      "2023-09-22 00:25:44.857549: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19600 of size 256 next 2093\n",
      "2023-09-22 00:25:44.857558: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19700 of size 256 next 2085\n",
      "2023-09-22 00:25:44.857568: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19800 of size 256 next 2088\n",
      "2023-09-22 00:25:44.857578: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19900 of size 256 next 1730\n",
      "2023-09-22 00:25:44.857587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19a00 of size 256 next 1005\n",
      "2023-09-22 00:25:44.857596: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19b00 of size 256 next 1549\n",
      "2023-09-22 00:25:44.857606: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de19c00 of size 57344 next 1534\n",
      "2023-09-22 00:25:44.857615: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de27c00 of size 256 next 1867\n",
      "2023-09-22 00:25:44.857625: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de27d00 of size 256 next 1722\n",
      "2023-09-22 00:25:44.857634: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de27e00 of size 256 next 608\n",
      "2023-09-22 00:25:44.857644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de27f00 of size 256 next 766\n",
      "2023-09-22 00:25:44.857653: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28000 of size 256 next 1354\n",
      "2023-09-22 00:25:44.857663: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28100 of size 256 next 1591\n",
      "2023-09-22 00:25:44.857673: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28200 of size 256 next 559\n",
      "2023-09-22 00:25:44.857682: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28300 of size 256 next 602\n",
      "2023-09-22 00:25:44.857692: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28400 of size 256 next 2051\n",
      "2023-09-22 00:25:44.857701: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28500 of size 256 next 1790\n",
      "2023-09-22 00:25:44.857710: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28600 of size 256 next 570\n",
      "2023-09-22 00:25:44.857720: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28700 of size 256 next 2047\n",
      "2023-09-22 00:25:44.857729: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28800 of size 256 next 2048\n",
      "2023-09-22 00:25:44.857738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28900 of size 256 next 1736\n",
      "2023-09-22 00:25:44.857748: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28a00 of size 256 next 2049\n",
      "2023-09-22 00:25:44.857757: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28b00 of size 256 next 1707\n",
      "2023-09-22 00:25:44.857766: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de28c00 of size 2048 next 1495\n",
      "2023-09-22 00:25:44.857776: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29400 of size 256 next 1724\n",
      "2023-09-22 00:25:44.857783: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29500 of size 256 next 1725\n",
      "2023-09-22 00:25:44.857792: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29600 of size 256 next 430\n",
      "2023-09-22 00:25:44.857801: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29700 of size 256 next 1885\n",
      "2023-09-22 00:25:44.857811: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29800 of size 256 next 1378\n",
      "2023-09-22 00:25:44.857821: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29900 of size 256 next 770\n",
      "2023-09-22 00:25:44.857830: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29a00 of size 256 next 1871\n",
      "2023-09-22 00:25:44.857840: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de29b00 of size 32768 next 1946\n",
      "2023-09-22 00:25:44.857849: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de31b00 of size 8192 next 2045\n",
      "2023-09-22 00:25:44.857859: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de33b00 of size 8192 next 863\n",
      "2023-09-22 00:25:44.857868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de35b00 of size 8192 next 1921\n",
      "2023-09-22 00:25:44.857877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de37b00 of size 8192 next 1898\n",
      "2023-09-22 00:25:44.857887: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de39b00 of size 11776 next 1944\n",
      "2023-09-22 00:25:44.857897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de3c900 of size 8192 next 2063\n",
      "2023-09-22 00:25:44.857905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de3e900 of size 4096 next 1404\n",
      "2023-09-22 00:25:44.857913: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de3f900 of size 4864 next 425\n",
      "2023-09-22 00:25:44.857920: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de40c00 of size 8192 next 1869\n",
      "2023-09-22 00:25:44.857928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de42c00 of size 8192 next 596\n",
      "2023-09-22 00:25:44.857935: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de44c00 of size 8192 next 768\n",
      "2023-09-22 00:25:44.857943: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de46c00 of size 15360 next 1723\n",
      "2023-09-22 00:25:44.857956: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de4a800 of size 8192 next 1881\n",
      "2023-09-22 00:25:44.857964: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de4c800 of size 15616 next 789\n",
      "2023-09-22 00:25:44.857971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271de50500 of size 524288 next 486\n",
      "2023-09-22 00:25:44.857978: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ded0500 of size 524288 next 1133\n",
      "2023-09-22 00:25:44.857986: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271df50500 of size 131072 next 998\n",
      "2023-09-22 00:25:44.857994: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271df70500 of size 131072 next 526\n",
      "2023-09-22 00:25:44.858001: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271df90500 of size 114688 next 1352\n",
      "2023-09-22 00:25:44.858009: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dfac500 of size 43008 next 1838\n",
      "2023-09-22 00:25:44.858016: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dfb6d00 of size 21504 next 1835\n",
      "2023-09-22 00:25:44.858024: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dfbc100 of size 21504 next 1836\n",
      "2023-09-22 00:25:44.858031: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dfc1500 of size 28672 next 661\n",
      "2023-09-22 00:25:44.858039: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dfc8500 of size 163840 next 797\n",
      "2023-09-22 00:25:44.858047: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271dff0500 of size 524288 next 1233\n",
      "2023-09-22 00:25:44.858054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e070500 of size 524288 next 966\n",
      "2023-09-22 00:25:44.858062: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e0f0500 of size 524288 next 200\n",
      "2023-09-22 00:25:44.858069: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e170500 of size 524288 next 187\n",
      "2023-09-22 00:25:44.858077: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e1f0500 of size 899584 next 961\n",
      "2023-09-22 00:25:44.858084: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2cbf00 of size 256 next 649\n",
      "2023-09-22 00:25:44.858092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2cc000 of size 256 next 804\n",
      "2023-09-22 00:25:44.858099: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2cc100 of size 8192 next 1143\n",
      "2023-09-22 00:25:44.858107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2ce100 of size 8192 next 936\n",
      "2023-09-22 00:25:44.858114: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2d0100 of size 8192 next 1947\n",
      "2023-09-22 00:25:44.858122: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2d2100 of size 8704 next 946\n",
      "2023-09-22 00:25:44.858129: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2d4300 of size 256 next 1343\n",
      "2023-09-22 00:25:44.858137: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2d4400 of size 131072 next 1954\n",
      "2023-09-22 00:25:44.858147: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e2f4400 of size 131072 next 86\n",
      "2023-09-22 00:25:44.858158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e314400 of size 167424 next 1335\n",
      "2023-09-22 00:25:44.858168: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e33d200 of size 131072 next 664\n",
      "2023-09-22 00:25:44.858178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e35d200 of size 135168 next 2128\n",
      "2023-09-22 00:25:44.858188: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e37e200 of size 131072 next 1932\n",
      "2023-09-22 00:25:44.858199: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e39e200 of size 135168 next 1970\n",
      "2023-09-22 00:25:44.858208: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e3bf200 of size 131072 next 417\n",
      "2023-09-22 00:25:44.858218: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e3df200 of size 135168 next 2100\n",
      "2023-09-22 00:25:44.858228: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e400200 of size 158208 next 1012\n",
      "2023-09-22 00:25:44.858239: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e426c00 of size 18432 next 1601\n",
      "2023-09-22 00:25:44.858249: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42b400 of size 4096 next 1820\n",
      "2023-09-22 00:25:44.858258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42c400 of size 512 next 1700\n",
      "2023-09-22 00:25:44.858267: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42c600 of size 512 next 1460\n",
      "2023-09-22 00:25:44.858277: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42c800 of size 512 next 1026\n",
      "2023-09-22 00:25:44.858286: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42ca00 of size 512 next 370\n",
      "2023-09-22 00:25:44.858296: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42cc00 of size 512 next 95\n",
      "2023-09-22 00:25:44.858306: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42ce00 of size 512 next 1965\n",
      "2023-09-22 00:25:44.858315: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e42d000 of size 32768 next 322\n",
      "2023-09-22 00:25:44.858325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e435000 of size 58368 next 985\n",
      "2023-09-22 00:25:44.858337: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e443400 of size 16896 next 637\n",
      "2023-09-22 00:25:44.858346: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e447600 of size 4096 next 756\n",
      "2023-09-22 00:25:44.858356: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e448600 of size 4096 next 1027\n",
      "2023-09-22 00:25:44.858363: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e449600 of size 4096 next 656\n",
      "2023-09-22 00:25:44.858372: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e44a600 of size 4608 next 160\n",
      "2023-09-22 00:25:44.858382: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e44b800 of size 38144 next 1312\n",
      "2023-09-22 00:25:44.858391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e454d00 of size 256 next 505\n",
      "2023-09-22 00:25:44.858400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e454e00 of size 524288 next 1228\n",
      "2023-09-22 00:25:44.858410: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e4d4e00 of size 524288 next 543\n",
      "2023-09-22 00:25:44.858420: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e554e00 of size 978432 next 174\n",
      "2023-09-22 00:25:44.858430: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e643c00 of size 256 next 628\n",
      "2023-09-22 00:25:44.858439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e643d00 of size 256 next 834\n",
      "2023-09-22 00:25:44.858449: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e643e00 of size 15616 next 290\n",
      "2023-09-22 00:25:44.858458: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e647b00 of size 256 next 947\n",
      "2023-09-22 00:25:44.858468: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e647c00 of size 541696 next 264\n",
      "2023-09-22 00:25:44.858477: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e6cc000 of size 512 next 1399\n",
      "2023-09-22 00:25:44.858486: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e6cc200 of size 512 next 186\n",
      "2023-09-22 00:25:44.858496: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e6cc400 of size 512 next 361\n",
      "2023-09-22 00:25:44.858505: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e6cc600 of size 512 next 673\n",
      "2023-09-22 00:25:44.858515: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e6cc800 of size 524288 next 102\n",
      "2023-09-22 00:25:44.858524: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e74c800 of size 524288 next 752\n",
      "2023-09-22 00:25:44.858534: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e7cc800 of size 524288 next 293\n",
      "2023-09-22 00:25:44.858544: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e84c800 of size 532480 next 823\n",
      "2023-09-22 00:25:44.858553: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e8ce800 of size 524288 next 503\n",
      "2023-09-22 00:25:44.858562: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e94e800 of size 524288 next 1572\n",
      "2023-09-22 00:25:44.858572: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271e9ce800 of size 532480 next 1585\n",
      "2023-09-22 00:25:44.858581: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ea50800 of size 568320 next 1588\n",
      "2023-09-22 00:25:44.858590: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadb400 of size 512 next 739\n",
      "2023-09-22 00:25:44.858599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadb600 of size 512 next 1010\n",
      "2023-09-22 00:25:44.858609: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadb800 of size 512 next 81\n",
      "2023-09-22 00:25:44.858618: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadba00 of size 512 next 248\n",
      "2023-09-22 00:25:44.858627: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadbc00 of size 512 next 960\n",
      "2023-09-22 00:25:44.858637: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadbe00 of size 512 next 944\n",
      "2023-09-22 00:25:44.858646: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadc000 of size 512 next 1235\n",
      "2023-09-22 00:25:44.858655: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadc200 of size 512 next 1530\n",
      "2023-09-22 00:25:44.858665: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadc400 of size 512 next 1160\n",
      "2023-09-22 00:25:44.858672: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadc600 of size 512 next 1191\n",
      "2023-09-22 00:25:44.858681: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadc800 of size 512 next 1514\n",
      "2023-09-22 00:25:44.858691: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadca00 of size 512 next 368\n",
      "2023-09-22 00:25:44.858700: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadcc00 of size 512 next 762\n",
      "2023-09-22 00:25:44.858710: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadce00 of size 512 next 605\n",
      "2023-09-22 00:25:44.858719: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadd000 of size 512 next 1270\n",
      "2023-09-22 00:25:44.858729: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadd200 of size 512 next 865\n",
      "2023-09-22 00:25:44.858738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadd400 of size 512 next 1176\n",
      "2023-09-22 00:25:44.858747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadd600 of size 512 next 1611\n",
      "2023-09-22 00:25:44.858757: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadd800 of size 512 next 962\n",
      "2023-09-22 00:25:44.858766: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadda00 of size 6656 next 1286\n",
      "2023-09-22 00:25:44.858775: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eadf400 of size 16384 next 843\n",
      "2023-09-22 00:25:44.858786: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eae3400 of size 16896 next 1376\n",
      "2023-09-22 00:25:44.858795: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eae7600 of size 20480 next 1504\n",
      "2023-09-22 00:25:44.858805: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eaec600 of size 256 next 729\n",
      "2023-09-22 00:25:44.858814: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eaec700 of size 256 next 1169\n",
      "2023-09-22 00:25:44.858824: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eaec800 of size 256 next 1132\n",
      "2023-09-22 00:25:44.858834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eaec900 of size 6144 next 1447\n",
      "2023-09-22 00:25:44.858844: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eaee100 of size 256 next 1284\n",
      "2023-09-22 00:25:44.858854: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eaee200 of size 655360 next 1539\n",
      "2023-09-22 00:25:44.858863: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eb8e200 of size 524288 next 1032\n",
      "2023-09-22 00:25:44.858873: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ec0e200 of size 917504 next 827\n",
      "2023-09-22 00:25:44.858885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ecee200 of size 524288 next 223\n",
      "2023-09-22 00:25:44.858894: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ed6e200 of size 524288 next 684\n",
      "2023-09-22 00:25:44.858902: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271edee200 of size 524288 next 996\n",
      "2023-09-22 00:25:44.858909: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ee6e200 of size 524288 next 119\n",
      "2023-09-22 00:25:44.858917: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271eeee200 of size 131072 next 1903\n",
      "2023-09-22 00:25:44.858924: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ef0e200 of size 131072 next 2058\n",
      "2023-09-22 00:25:44.858932: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ef2e200 of size 132608 next 1336\n",
      "2023-09-22 00:25:44.858943: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ef4e800 of size 131072 next 2075\n",
      "2023-09-22 00:25:44.858952: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ef6e800 of size 131072 next 1339\n",
      "2023-09-22 00:25:44.858962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271ef8e800 of size 131072 next 2037\n",
      "2023-09-22 00:25:44.858971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efae800 of size 138752 next 1365\n",
      "2023-09-22 00:25:44.858979: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd0600 of size 256 next 1678\n",
      "2023-09-22 00:25:44.858989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd0700 of size 256 next 485\n",
      "2023-09-22 00:25:44.858998: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd0800 of size 2048 next 2054\n",
      "2023-09-22 00:25:44.859008: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd1000 of size 2048 next 1779\n",
      "2023-09-22 00:25:44.859017: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd1800 of size 2048 next 1943\n",
      "2023-09-22 00:25:44.859026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2000 of size 256 next 1942\n",
      "2023-09-22 00:25:44.859036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2100 of size 256 next 2002\n",
      "2023-09-22 00:25:44.859045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2200 of size 256 next 1998\n",
      "2023-09-22 00:25:44.859055: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2300 of size 256 next 1995\n",
      "2023-09-22 00:25:44.859065: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2400 of size 256 next 1952\n",
      "2023-09-22 00:25:44.859074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2500 of size 256 next 1688\n",
      "2023-09-22 00:25:44.859084: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2600 of size 256 next 1993\n",
      "2023-09-22 00:25:44.859093: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2700 of size 256 next 339\n",
      "2023-09-22 00:25:44.859103: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd2800 of size 2048 next 2076\n",
      "2023-09-22 00:25:44.859112: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd3000 of size 2048 next 2114\n",
      "2023-09-22 00:25:44.859121: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd3800 of size 2048 next 276\n",
      "2023-09-22 00:25:44.859131: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd4000 of size 2048 next 1763\n",
      "2023-09-22 00:25:44.859158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd4800 of size 2048 next 1740\n",
      "2023-09-22 00:25:44.859169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5000 of size 2048 next 1153\n",
      "2023-09-22 00:25:44.859177: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5800 of size 256 next 1909\n",
      "2023-09-22 00:25:44.859185: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5900 of size 256 next 1769\n",
      "2023-09-22 00:25:44.859192: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5a00 of size 256 next 1911\n",
      "2023-09-22 00:25:44.859203: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5b00 of size 256 next 1761\n",
      "2023-09-22 00:25:44.859213: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5c00 of size 256 next 1904\n",
      "2023-09-22 00:25:44.859223: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5d00 of size 256 next 1755\n",
      "2023-09-22 00:25:44.859233: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5e00 of size 256 next 1756\n",
      "2023-09-22 00:25:44.859242: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd5f00 of size 256 next 1157\n",
      "2023-09-22 00:25:44.859252: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6000 of size 256 next 1945\n",
      "2023-09-22 00:25:44.859262: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6100 of size 256 next 2087\n",
      "2023-09-22 00:25:44.859271: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6200 of size 256 next 1760\n",
      "2023-09-22 00:25:44.859281: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6300 of size 256 next 2081\n",
      "2023-09-22 00:25:44.859290: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6400 of size 256 next 1743\n",
      "2023-09-22 00:25:44.859300: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6500 of size 256 next 488\n",
      "2023-09-22 00:25:44.859311: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6600 of size 256 next 2073\n",
      "2023-09-22 00:25:44.859321: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6700 of size 256 next 601\n",
      "2023-09-22 00:25:44.859332: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd6800 of size 2048 next 1714\n",
      "2023-09-22 00:25:44.859342: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd7000 of size 2048 next 1708\n",
      "2023-09-22 00:25:44.859351: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd7800 of size 2048 next 1861\n",
      "2023-09-22 00:25:44.859361: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd8000 of size 2048 next 1633\n",
      "2023-09-22 00:25:44.859370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd8800 of size 2048 next 1868\n",
      "2023-09-22 00:25:44.859380: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd9000 of size 2048 next 1918\n",
      "2023-09-22 00:25:44.859389: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efd9800 of size 2048 next 1941\n",
      "2023-09-22 00:25:44.859398: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efda000 of size 2048 next 1294\n",
      "2023-09-22 00:25:44.859408: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efda800 of size 256 next 1751\n",
      "2023-09-22 00:25:44.859419: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efda900 of size 256 next 1749\n",
      "2023-09-22 00:25:44.859426: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdaa00 of size 256 next 1897\n",
      "2023-09-22 00:25:44.859434: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdab00 of size 256 next 1908\n",
      "2023-09-22 00:25:44.859441: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdac00 of size 256 next 1893\n",
      "2023-09-22 00:25:44.859449: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdad00 of size 256 next 1733\n",
      "2023-09-22 00:25:44.859456: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdae00 of size 256 next 1891\n",
      "2023-09-22 00:25:44.859463: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdaf00 of size 256 next 1618\n",
      "2023-09-22 00:25:44.859471: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb000 of size 256 next 1906\n",
      "2023-09-22 00:25:44.859478: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb100 of size 256 next 1487\n",
      "2023-09-22 00:25:44.859486: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb200 of size 256 next 547\n",
      "2023-09-22 00:25:44.859494: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb300 of size 256 next 1782\n",
      "2023-09-22 00:25:44.859502: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb400 of size 256 next 1927\n",
      "2023-09-22 00:25:44.859509: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb500 of size 256 next 1987\n",
      "2023-09-22 00:25:44.859517: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb600 of size 256 next 2140\n",
      "2023-09-22 00:25:44.859524: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb700 of size 256 next 1522\n",
      "2023-09-22 00:25:44.859531: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb800 of size 256 next 1449\n",
      "2023-09-22 00:25:44.859539: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdb900 of size 256 next 2111\n",
      "2023-09-22 00:25:44.859546: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdba00 of size 256 next 1839\n",
      "2023-09-22 00:25:44.859554: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdbb00 of size 256 next 715\n",
      "2023-09-22 00:25:44.859561: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdbc00 of size 256 next 1844\n",
      "2023-09-22 00:25:44.859568: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdbd00 of size 256 next 1185\n",
      "2023-09-22 00:25:44.859576: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdbe00 of size 256 next 343\n",
      "2023-09-22 00:25:44.859583: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdbf00 of size 256 next 1732\n",
      "2023-09-22 00:25:44.859592: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdc000 of size 256 next 1759\n",
      "2023-09-22 00:25:44.859603: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdc100 of size 256 next 1210\n",
      "2023-09-22 00:25:44.859612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdc200 of size 256 next 55\n",
      "2023-09-22 00:25:44.859622: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdc300 of size 3328 next 1894\n",
      "2023-09-22 00:25:44.859632: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdd000 of size 2048 next 1746\n",
      "2023-09-22 00:25:44.859643: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdd800 of size 2048 next 1311\n",
      "2023-09-22 00:25:44.859654: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efde000 of size 2048 next 1189\n",
      "2023-09-22 00:25:44.859662: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efde800 of size 2048 next 1951\n",
      "2023-09-22 00:25:44.859671: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdf000 of size 2048 next 1735\n",
      "2023-09-22 00:25:44.859682: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efdf800 of size 2048 next 2043\n",
      "2023-09-22 00:25:44.859693: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe0000 of size 2048 next 1524\n",
      "2023-09-22 00:25:44.859704: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe0800 of size 2048 next 822\n",
      "2023-09-22 00:25:44.859712: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe1000 of size 2048 next 666\n",
      "2023-09-22 00:25:44.859718: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe1800 of size 2560 next 415\n",
      "2023-09-22 00:25:44.859723: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2200 of size 256 next 620\n",
      "2023-09-22 00:25:44.859728: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2300 of size 256 next 1172\n",
      "2023-09-22 00:25:44.859732: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2400 of size 256 next 1866\n",
      "2023-09-22 00:25:44.859736: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2500 of size 256 next 1393\n",
      "2023-09-22 00:25:44.859740: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2600 of size 256 next 1872\n",
      "2023-09-22 00:25:44.859743: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2700 of size 256 next 1548\n",
      "2023-09-22 00:25:44.859746: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe2800 of size 2048 next 1914\n",
      "2023-09-22 00:25:44.859750: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe3000 of size 2048 next 1913\n",
      "2023-09-22 00:25:44.859753: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe3800 of size 256 next 1888\n",
      "2023-09-22 00:25:44.859756: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe3900 of size 2048 next 1814\n",
      "2023-09-22 00:25:44.859760: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe4100 of size 256 next 1775\n",
      "2023-09-22 00:25:44.859763: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe4200 of size 2048 next 1772\n",
      "2023-09-22 00:25:44.859767: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe4a00 of size 2048 next 1773\n",
      "2023-09-22 00:25:44.859770: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5200 of size 2048 next 1767\n",
      "2023-09-22 00:25:44.859773: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5a00 of size 256 next 1104\n",
      "2023-09-22 00:25:44.859777: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5b00 of size 256 next 706\n",
      "2023-09-22 00:25:44.859780: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5c00 of size 256 next 2026\n",
      "2023-09-22 00:25:44.859783: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5d00 of size 256 next 1809\n",
      "2023-09-22 00:25:44.859787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5e00 of size 256 next 1808\n",
      "2023-09-22 00:25:44.859790: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe5f00 of size 256 next 1807\n",
      "2023-09-22 00:25:44.859794: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6000 of size 256 next 1806\n",
      "2023-09-22 00:25:44.859797: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6100 of size 256 next 1804\n",
      "2023-09-22 00:25:44.859800: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6200 of size 256 next 973\n",
      "2023-09-22 00:25:44.859804: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6300 of size 256 next 1752\n",
      "2023-09-22 00:25:44.859807: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6400 of size 256 next 2032\n",
      "2023-09-22 00:25:44.859810: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6500 of size 256 next 1841\n",
      "2023-09-22 00:25:44.859814: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6600 of size 256 next 1203\n",
      "2023-09-22 00:25:44.859818: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6700 of size 256 next 1513\n",
      "2023-09-22 00:25:44.859821: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6800 of size 256 next 1793\n",
      "2023-09-22 00:25:44.859824: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6900 of size 256 next 1922\n",
      "2023-09-22 00:25:44.859828: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6a00 of size 256 next 1786\n",
      "2023-09-22 00:25:44.859831: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6b00 of size 256 next 1787\n",
      "2023-09-22 00:25:44.859834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6c00 of size 256 next 1936\n",
      "2023-09-22 00:25:44.859838: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6d00 of size 256 next 1783\n",
      "2023-09-22 00:25:44.859841: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6e00 of size 256 next 403\n",
      "2023-09-22 00:25:44.859844: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe6f00 of size 256 next 1929\n",
      "2023-09-22 00:25:44.859848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe7000 of size 256 next 1917\n",
      "2023-09-22 00:25:44.859851: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe7100 of size 2048 next 1912\n",
      "2023-09-22 00:25:44.859854: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271efe7900 of size 114688 next 1819\n",
      "2023-09-22 00:25:44.859858: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f003900 of size 134912 next 1564\n",
      "2023-09-22 00:25:44.859862: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f024800 of size 180224 next 1174\n",
      "2023-09-22 00:25:44.859865: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f050800 of size 758272 next 749\n",
      "2023-09-22 00:25:44.859868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f109a00 of size 256 next 1346\n",
      "2023-09-22 00:25:44.859872: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f109b00 of size 256 next 1355\n",
      "2023-09-22 00:25:44.859875: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f109c00 of size 4096 next 355\n",
      "2023-09-22 00:25:44.859879: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f10ac00 of size 4096 next 294\n",
      "2023-09-22 00:25:44.859882: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f10bc00 of size 4096 next 1044\n",
      "2023-09-22 00:25:44.859885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f10cc00 of size 4096 next 393\n",
      "2023-09-22 00:25:44.859889: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f10dc00 of size 16384 next 1677\n",
      "2023-09-22 00:25:44.859892: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f111c00 of size 16384 next 795\n",
      "2023-09-22 00:25:44.859895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f115c00 of size 30720 next 1016\n",
      "2023-09-22 00:25:44.859899: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f11d400 of size 16384 next 1420\n",
      "2023-09-22 00:25:44.859902: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f121400 of size 16896 next 71\n",
      "2023-09-22 00:25:44.859905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f125600 of size 16384 next 1105\n",
      "2023-09-22 00:25:44.859909: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f129600 of size 16384 next 130\n",
      "2023-09-22 00:25:44.859912: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f12d600 of size 30720 next 165\n",
      "2023-09-22 00:25:44.859916: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f134e00 of size 16384 next 1183\n",
      "2023-09-22 00:25:44.859919: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f138e00 of size 16384 next 1409\n",
      "2023-09-22 00:25:44.859922: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f13ce00 of size 16896 next 803\n",
      "2023-09-22 00:25:44.859926: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f141000 of size 20480 next 1473\n",
      "2023-09-22 00:25:44.859929: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f146000 of size 256 next 975\n",
      "2023-09-22 00:25:44.859933: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f146100 of size 256 next 1634\n",
      "2023-09-22 00:25:44.859936: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f146200 of size 2048 next 1164\n",
      "2023-09-22 00:25:44.859939: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f146a00 of size 3840 next 1452\n",
      "2023-09-22 00:25:44.859943: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147900 of size 256 next 1401\n",
      "2023-09-22 00:25:44.859946: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147a00 of size 256 next 1443\n",
      "2023-09-22 00:25:44.859949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147b00 of size 256 next 652\n",
      "2023-09-22 00:25:44.859953: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147c00 of size 256 next 389\n",
      "2023-09-22 00:25:44.859956: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147d00 of size 256 next 11\n",
      "2023-09-22 00:25:44.859959: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147e00 of size 256 next 662\n",
      "2023-09-22 00:25:44.859963: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f147f00 of size 256 next 222\n",
      "2023-09-22 00:25:44.859966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148000 of size 256 next 940\n",
      "2023-09-22 00:25:44.859969: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148100 of size 256 next 1042\n",
      "2023-09-22 00:25:44.859973: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148200 of size 256 next 1675\n",
      "2023-09-22 00:25:44.859976: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148300 of size 256 next 168\n",
      "2023-09-22 00:25:44.859979: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148400 of size 256 next 1479\n",
      "2023-09-22 00:25:44.859983: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148500 of size 256 next 1645\n",
      "2023-09-22 00:25:44.859986: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148600 of size 256 next 1388\n",
      "2023-09-22 00:25:44.859989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148700 of size 256 next 1658\n",
      "2023-09-22 00:25:44.859993: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148800 of size 256 next 1098\n",
      "2023-09-22 00:25:44.859996: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148900 of size 256 next 807\n",
      "2023-09-22 00:25:44.859999: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148a00 of size 256 next 205\n",
      "2023-09-22 00:25:44.860003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148b00 of size 256 next 805\n",
      "2023-09-22 00:25:44.860006: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148c00 of size 256 next 1161\n",
      "2023-09-22 00:25:44.860009: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148d00 of size 256 next 1015\n",
      "2023-09-22 00:25:44.860013: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148e00 of size 256 next 1391\n",
      "2023-09-22 00:25:44.860016: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f148f00 of size 256 next 25\n",
      "2023-09-22 00:25:44.860020: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149000 of size 256 next 85\n",
      "2023-09-22 00:25:44.860023: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149100 of size 256 next 348\n",
      "2023-09-22 00:25:44.860026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149200 of size 256 next 327\n",
      "2023-09-22 00:25:44.860030: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149300 of size 256 next 395\n",
      "2023-09-22 00:25:44.860033: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149400 of size 512 next 1558\n",
      "2023-09-22 00:25:44.860036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149600 of size 256 next 454\n",
      "2023-09-22 00:25:44.860040: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149700 of size 256 next 1604\n",
      "2023-09-22 00:25:44.860043: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149800 of size 256 next 455\n",
      "2023-09-22 00:25:44.860047: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149900 of size 256 next 80\n",
      "2023-09-22 00:25:44.860050: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f149a00 of size 4096 next 402\n",
      "2023-09-22 00:25:44.860053: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f14aa00 of size 6144 next 82\n",
      "2023-09-22 00:25:44.860057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f14c200 of size 30976 next 1450\n",
      "2023-09-22 00:25:44.860060: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f153b00 of size 4096 next 201\n",
      "2023-09-22 00:25:44.860064: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f154b00 of size 6656 next 864\n",
      "2023-09-22 00:25:44.860067: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f156500 of size 512 next 491\n",
      "2023-09-22 00:25:44.860071: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f156700 of size 512 next 1291\n",
      "2023-09-22 00:25:44.860074: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f156900 of size 512 next 1272\n",
      "2023-09-22 00:25:44.860077: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f156b00 of size 512 next 1674\n",
      "2023-09-22 00:25:44.860081: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f156d00 of size 4096 next 850\n",
      "2023-09-22 00:25:44.860084: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f157d00 of size 4352 next 788\n",
      "2023-09-22 00:25:44.860087: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f158e00 of size 256 next 868\n",
      "2023-09-22 00:25:44.860091: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f158f00 of size 256 next 747\n",
      "2023-09-22 00:25:44.860094: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f159000 of size 4096 next 497\n",
      "2023-09-22 00:25:44.860097: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f15a000 of size 4096 next 948\n",
      "2023-09-22 00:25:44.860101: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f15b000 of size 4096 next 38\n",
      "2023-09-22 00:25:44.860104: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f15c000 of size 4096 next 855\n",
      "2023-09-22 00:25:44.860107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f15d000 of size 9216 next 91\n",
      "2023-09-22 00:25:44.860111: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f15f400 of size 9216 next 678\n",
      "2023-09-22 00:25:44.860114: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f161800 of size 9216 next 1094\n",
      "2023-09-22 00:25:44.860117: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f163c00 of size 9216 next 1621\n",
      "2023-09-22 00:25:44.860121: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f166000 of size 9216 next 1066\n",
      "2023-09-22 00:25:44.860124: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f168400 of size 17920 next 951\n",
      "2023-09-22 00:25:44.860127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f16ca00 of size 8192 next 515\n",
      "2023-09-22 00:25:44.860131: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f16ea00 of size 9216 next 1324\n",
      "2023-09-22 00:25:44.860134: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f170e00 of size 9216 next 1692\n",
      "2023-09-22 00:25:44.860138: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173200 of size 256 next 399\n",
      "2023-09-22 00:25:44.860141: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173300 of size 256 next 318\n",
      "2023-09-22 00:25:44.860144: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173400 of size 256 next 436\n",
      "2023-09-22 00:25:44.860148: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173500 of size 256 next 2159\n",
      "2023-09-22 00:25:44.860151: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173600 of size 256 next 2163\n",
      "2023-09-22 00:25:44.860154: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173700 of size 256 next 557\n",
      "2023-09-22 00:25:44.860158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173800 of size 256 next 1410\n",
      "2023-09-22 00:25:44.860161: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f173900 of size 5888 next 499\n",
      "2023-09-22 00:25:44.860164: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175000 of size 256 next 525\n",
      "2023-09-22 00:25:44.860168: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175100 of size 256 next 1293\n",
      "2023-09-22 00:25:44.860171: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175200 of size 256 next 2155\n",
      "2023-09-22 00:25:44.860174: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175300 of size 256 next 897\n",
      "2023-09-22 00:25:44.860178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175400 of size 256 next 2147\n",
      "2023-09-22 00:25:44.860181: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175500 of size 256 next 2151\n",
      "2023-09-22 00:25:44.860184: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175600 of size 256 next 2156\n",
      "2023-09-22 00:25:44.860188: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175700 of size 256 next 681\n",
      "2023-09-22 00:25:44.860191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175800 of size 256 next 267\n",
      "2023-09-22 00:25:44.860194: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175900 of size 256 next 1584\n",
      "2023-09-22 00:25:44.860198: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175a00 of size 256 next 778\n",
      "2023-09-22 00:25:44.860201: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175b00 of size 512 next 1078\n",
      "2023-09-22 00:25:44.860204: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175d00 of size 512 next 1652\n",
      "2023-09-22 00:25:44.860208: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f175f00 of size 256 next 910\n",
      "2023-09-22 00:25:44.860211: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176000 of size 256 next 315\n",
      "2023-09-22 00:25:44.860214: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176100 of size 256 next 751\n",
      "2023-09-22 00:25:44.860218: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176200 of size 256 next 1055\n",
      "2023-09-22 00:25:44.860221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176300 of size 512 next 792\n",
      "2023-09-22 00:25:44.860224: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176500 of size 256 next 1081\n",
      "2023-09-22 00:25:44.860228: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176600 of size 256 next 688\n",
      "2023-09-22 00:25:44.860231: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176700 of size 768 next 1356\n",
      "2023-09-22 00:25:44.860235: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176a00 of size 256 next 958\n",
      "2023-09-22 00:25:44.860238: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176b00 of size 256 next 540\n",
      "2023-09-22 00:25:44.860241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176c00 of size 256 next 1002\n",
      "2023-09-22 00:25:44.860245: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176d00 of size 256 next 1466\n",
      "2023-09-22 00:25:44.860248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176e00 of size 256 next 875\n",
      "2023-09-22 00:25:44.860251: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f176f00 of size 256 next 1426\n",
      "2023-09-22 00:25:44.860255: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177000 of size 256 next 1193\n",
      "2023-09-22 00:25:44.860258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177100 of size 256 next 1499\n",
      "2023-09-22 00:25:44.860262: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177200 of size 256 next 406\n",
      "2023-09-22 00:25:44.860265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177300 of size 256 next 949\n",
      "2023-09-22 00:25:44.860268: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177400 of size 256 next 1515\n",
      "2023-09-22 00:25:44.860272: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177500 of size 256 next 409\n",
      "2023-09-22 00:25:44.860275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177600 of size 256 next 1587\n",
      "2023-09-22 00:25:44.860278: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177700 of size 256 next 1118\n",
      "2023-09-22 00:25:44.860281: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177800 of size 256 next 1568\n",
      "2023-09-22 00:25:44.860285: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177900 of size 256 next 1333\n",
      "2023-09-22 00:25:44.860288: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177a00 of size 256 next 896\n",
      "2023-09-22 00:25:44.860291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177b00 of size 256 next 1093\n",
      "2023-09-22 00:25:44.860295: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177c00 of size 256 next 448\n",
      "2023-09-22 00:25:44.860298: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177d00 of size 256 next 1131\n",
      "2023-09-22 00:25:44.860301: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f177e00 of size 4096 next 407\n",
      "2023-09-22 00:25:44.860305: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f178e00 of size 4096 next 783\n",
      "2023-09-22 00:25:44.860308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f179e00 of size 7168 next 1238\n",
      "2023-09-22 00:25:44.860312: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f17ba00 of size 26624 next 1597\n",
      "2023-09-22 00:25:44.860317: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f182200 of size 4096 next 814\n",
      "2023-09-22 00:25:44.860320: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f183200 of size 4096 next 787\n",
      "2023-09-22 00:25:44.860324: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184200 of size 512 next 1435\n",
      "2023-09-22 00:25:44.860327: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184400 of size 512 next 352\n",
      "2023-09-22 00:25:44.860331: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184600 of size 512 next 626\n",
      "2023-09-22 00:25:44.860334: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184800 of size 512 next 62\n",
      "2023-09-22 00:25:44.860337: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184a00 of size 512 next 1419\n",
      "2023-09-22 00:25:44.860341: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184c00 of size 512 next 758\n",
      "2023-09-22 00:25:44.860344: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f184e00 of size 512 next 545\n",
      "2023-09-22 00:25:44.860347: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f185000 of size 512 next 1559\n",
      "2023-09-22 00:25:44.860351: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f185200 of size 4608 next 1521\n",
      "2023-09-22 00:25:44.860354: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f186400 of size 4096 next 651\n",
      "2023-09-22 00:25:44.860357: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f187400 of size 6144 next 1148\n",
      "2023-09-22 00:25:44.860361: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f188c00 of size 768 next 860\n",
      "2023-09-22 00:25:44.860364: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f188f00 of size 4096 next 473\n",
      "2023-09-22 00:25:44.860367: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f189f00 of size 512 next 1137\n",
      "2023-09-22 00:25:44.860371: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18a100 of size 512 next 976\n",
      "2023-09-22 00:25:44.860374: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18a300 of size 512 next 669\n",
      "2023-09-22 00:25:44.860377: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18a500 of size 512 next 504\n",
      "2023-09-22 00:25:44.860381: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18a700 of size 512 next 132\n",
      "2023-09-22 00:25:44.860384: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18a900 of size 512 next 1250\n",
      "2023-09-22 00:25:44.860387: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18ab00 of size 512 next 1454\n",
      "2023-09-22 00:25:44.860391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18ad00 of size 512 next 974\n",
      "2023-09-22 00:25:44.860394: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18af00 of size 512 next 603\n",
      "2023-09-22 00:25:44.860397: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b100 of size 512 next 240\n",
      "2023-09-22 00:25:44.860401: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b300 of size 256 next 382\n",
      "2023-09-22 00:25:44.860404: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b400 of size 256 next 1180\n",
      "2023-09-22 00:25:44.860407: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b500 of size 256 next 1247\n",
      "2023-09-22 00:25:44.860411: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b600 of size 256 next 2166\n",
      "2023-09-22 00:25:44.860414: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b700 of size 256 next 2112\n",
      "2023-09-22 00:25:44.860417: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b800 of size 256 next 308\n",
      "2023-09-22 00:25:44.860421: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18b900 of size 4096 next 376\n",
      "2023-09-22 00:25:44.860424: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18c900 of size 6144 next 780\n",
      "2023-09-22 00:25:44.860427: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18e100 of size 256 next 502\n",
      "2023-09-22 00:25:44.860431: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18e200 of size 256 next 117\n",
      "2023-09-22 00:25:44.860434: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18e300 of size 512 next 1363\n",
      "2023-09-22 00:25:44.860437: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f18e500 of size 16384 next 1009\n",
      "2023-09-22 00:25:44.860441: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f192500 of size 26624 next 579\n",
      "2023-09-22 00:25:44.860444: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f198d00 of size 2048 next 1785\n",
      "2023-09-22 00:25:44.860447: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f199500 of size 2048 next 2134\n",
      "2023-09-22 00:25:44.860451: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f199d00 of size 2816 next 1100\n",
      "2023-09-22 00:25:44.860454: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f19a800 of size 9472 next 155\n",
      "2023-09-22 00:25:44.860458: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f19cd00 of size 16384 next 1254\n",
      "2023-09-22 00:25:44.860461: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f271f1a0d00 of size 28672 next 41\n",
      "2023-09-22 00:25:44.860464: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f271f1a7d00 of size 30199808 next 501\n",
      "2023-09-22 00:25:44.860468: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2720e74d00 of size 655360 next 1501\n",
      "2023-09-22 00:25:44.860471: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2720f14d00 of size 34438912 next 237\n",
      "2023-09-22 00:25:44.860475: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2722fecc00 of size 524288 next 468\n",
      "2023-09-22 00:25:44.860478: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f272306cc00 of size 524288 next 884\n",
      "2023-09-22 00:25:44.860482: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27230ecc00 of size 114688 next 1373\n",
      "2023-09-22 00:25:44.860485: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2723108c00 of size 114688 next 1851\n",
      "2023-09-22 00:25:44.860489: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2723124c00 of size 147456 next 1115\n",
      "2023-09-22 00:25:44.860492: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2723148c00 of size 131072 next 1025\n",
      "2023-09-22 00:25:44.860495: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2723168c00 of size 147456 next 568\n",
      "2023-09-22 00:25:44.860499: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f272318cc00 of size 524288 next 228\n",
      "2023-09-22 00:25:44.860502: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f272320cc00 of size 557056 next 1405\n",
      "2023-09-22 00:25:44.860506: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2723294c00 of size 655360 next 1483\n",
      "2023-09-22 00:25:44.860509: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2723334c00 of size 655360 next 724\n",
      "2023-09-22 00:25:44.860513: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27233d4c00 of size 819200 next 1204\n",
      "2023-09-22 00:25:44.860516: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f272349cc00 of size 524288 next 1215\n",
      "2023-09-22 00:25:44.860520: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f272351cc00 of size 622592 next 1531\n",
      "2023-09-22 00:25:44.860523: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f27235b4c00 of size 17039360 next 558\n",
      "2023-09-22 00:25:44.860527: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27245f4c00 of size 256 next 398\n",
      "2023-09-22 00:25:44.860530: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27245f4d00 of size 256 next 1506\n",
      "2023-09-22 00:25:44.860533: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27245f4e00 of size 655360 next 798\n",
      "2023-09-22 00:25:44.860537: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724694e00 of size 524288 next 1533\n",
      "2023-09-22 00:25:44.860540: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724714e00 of size 524288 next 1295\n",
      "2023-09-22 00:25:44.860544: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724794e00 of size 524288 next 2149\n",
      "2023-09-22 00:25:44.860547: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724814e00 of size 524288 next 1074\n",
      "2023-09-22 00:25:44.860550: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724894e00 of size 524288 next 594\n",
      "2023-09-22 00:25:44.860554: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724914e00 of size 524288 next 1960\n",
      "2023-09-22 00:25:44.860557: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724994e00 of size 524288 next 622\n",
      "2023-09-22 00:25:44.860560: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724a14e00 of size 524288 next 2024\n",
      "2023-09-22 00:25:44.860564: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724a94e00 of size 524288 next 1022\n",
      "2023-09-22 00:25:44.860567: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724b14e00 of size 524288 next 585\n",
      "2023-09-22 00:25:44.860570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724b94e00 of size 655360 next 400\n",
      "2023-09-22 00:25:44.860574: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724c34e00 of size 524288 next 47\n",
      "2023-09-22 00:25:44.860577: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724cb4e00 of size 524288 next 233\n",
      "2023-09-22 00:25:44.860580: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724d34e00 of size 655360 next 680\n",
      "2023-09-22 00:25:44.860584: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724dd4e00 of size 16384 next 957\n",
      "2023-09-22 00:25:44.860587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724dd8e00 of size 25600 next 1305\n",
      "2023-09-22 00:25:44.860591: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724ddf200 of size 256 next 1655\n",
      "2023-09-22 00:25:44.860594: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724ddf300 of size 256 next 819\n",
      "2023-09-22 00:25:44.860597: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724ddf400 of size 256 next 894\n",
      "2023-09-22 00:25:44.860601: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724ddf500 of size 256 next 943\n",
      "2023-09-22 00:25:44.860604: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724ddf600 of size 30720 next 826\n",
      "2023-09-22 00:25:44.860607: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724de6e00 of size 16384 next 79\n",
      "2023-09-22 00:25:44.860611: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724deae00 of size 16384 next 1187\n",
      "2023-09-22 00:25:44.860614: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2724deee00 of size 233280000 next 696\n",
      "2023-09-22 00:25:44.860617: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2732c68000 of size 184584448 next 928\n",
      "2023-09-22 00:25:44.860621: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70900 of size 256 next 644\n",
      "2023-09-22 00:25:44.860624: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70a00 of size 256 next 522\n",
      "2023-09-22 00:25:44.860628: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70b00 of size 256 next 270\n",
      "2023-09-22 00:25:44.860631: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70c00 of size 256 next 523\n",
      "2023-09-22 00:25:44.860634: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70d00 of size 256 next 937\n",
      "2023-09-22 00:25:44.860638: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70e00 of size 256 next 456\n",
      "2023-09-22 00:25:44.860641: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc70f00 of size 256 next 796\n",
      "2023-09-22 00:25:44.860644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc71000 of size 4864 next 841\n",
      "2023-09-22 00:25:44.860648: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc72300 of size 256 next 842\n",
      "2023-09-22 00:25:44.860651: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f273dc72400 of size 231120128 next 263\n",
      "2023-09-22 00:25:44.860657: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f274b8dc100 of size 233520128 next 1281\n",
      "2023-09-22 00:25:44.860660: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f275978fd00 of size 242727936 next 1145\n",
      "2023-09-22 00:25:44.860664: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f0b900 of size 256 next 909\n",
      "2023-09-22 00:25:44.860667: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f0ba00 of size 256 next 920\n",
      "2023-09-22 00:25:44.860671: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f0bb00 of size 65536 next 1665\n",
      "2023-09-22 00:25:44.860674: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f1bb00 of size 4096 next 1245\n",
      "2023-09-22 00:25:44.860678: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f1cb00 of size 8192 next 806\n",
      "2023-09-22 00:25:44.860681: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f1eb00 of size 4608 next 244\n",
      "2023-09-22 00:25:44.860684: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f1fd00 of size 27904 next 1555\n",
      "2023-09-22 00:25:44.860688: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f26a00 of size 256 next 1289\n",
      "2023-09-22 00:25:44.860691: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f26b00 of size 256 next 1453\n",
      "2023-09-22 00:25:44.860694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f26c00 of size 256 next 521\n",
      "2023-09-22 00:25:44.860698: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f26d00 of size 2304 next 1264\n",
      "2023-09-22 00:25:44.860702: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f27600 of size 256 next 467\n",
      "2023-09-22 00:25:44.860705: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f27700 of size 10240 next 592\n",
      "2023-09-22 00:25:44.860708: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f29f00 of size 512 next 1276\n",
      "2023-09-22 00:25:44.860712: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f2a100 of size 512 next 1644\n",
      "2023-09-22 00:25:44.860716: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f2a300 of size 29696 next 1493\n",
      "2023-09-22 00:25:44.860720: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f31700 of size 256 next 427\n",
      "2023-09-22 00:25:44.860724: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f31800 of size 256 next 1536\n",
      "2023-09-22 00:25:44.860728: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f31900 of size 256 next 761\n",
      "2023-09-22 00:25:44.860731: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f31a00 of size 512 next 418\n",
      "2023-09-22 00:25:44.860735: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f31c00 of size 768 next 351\n",
      "2023-09-22 00:25:44.860739: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f31f00 of size 256 next 548\n",
      "2023-09-22 00:25:44.860744: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32000 of size 256 next 1303\n",
      "2023-09-22 00:25:44.860747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32100 of size 256 next 750\n",
      "2023-09-22 00:25:44.860750: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32200 of size 256 next 638\n",
      "2023-09-22 00:25:44.860754: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32300 of size 256 next 1071\n",
      "2023-09-22 00:25:44.860757: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32400 of size 256 next 1101\n",
      "2023-09-22 00:25:44.860760: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32500 of size 256 next 74\n",
      "2023-09-22 00:25:44.860764: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32600 of size 256 next 959\n",
      "2023-09-22 00:25:44.860767: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f32700 of size 3328 next 554\n",
      "2023-09-22 00:25:44.860771: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f33400 of size 256 next 27\n",
      "2023-09-22 00:25:44.860774: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f33500 of size 256 next 314\n",
      "2023-09-22 00:25:44.860777: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f33600 of size 3072 next 1812\n",
      "2023-09-22 00:25:44.860781: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34200 of size 256 next 1576\n",
      "2023-09-22 00:25:44.860784: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34300 of size 256 next 1754\n",
      "2023-09-22 00:25:44.860787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34400 of size 256 next 829\n",
      "2023-09-22 00:25:44.860791: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34500 of size 256 next 1950\n",
      "2023-09-22 00:25:44.860794: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34600 of size 256 next 2004\n",
      "2023-09-22 00:25:44.860798: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34700 of size 256 next 1729\n",
      "2023-09-22 00:25:44.860801: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34800 of size 256 next 1583\n",
      "2023-09-22 00:25:44.860804: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34900 of size 256 next 2005\n",
      "2023-09-22 00:25:44.860808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34a00 of size 256 next 374\n",
      "2023-09-22 00:25:44.860811: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34b00 of size 256 next 1992\n",
      "2023-09-22 00:25:44.860814: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34c00 of size 256 next 1599\n",
      "2023-09-22 00:25:44.860818: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34d00 of size 256 next 1109\n",
      "2023-09-22 00:25:44.860821: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34e00 of size 256 next 287\n",
      "2023-09-22 00:25:44.860824: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f34f00 of size 512 next 1205\n",
      "2023-09-22 00:25:44.860828: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35100 of size 256 next 1551\n",
      "2023-09-22 00:25:44.860831: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35200 of size 256 next 1179\n",
      "2023-09-22 00:25:44.860834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35300 of size 256 next 1381\n",
      "2023-09-22 00:25:44.860838: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35400 of size 256 next 2113\n",
      "2023-09-22 00:25:44.860841: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35500 of size 256 next 434\n",
      "2023-09-22 00:25:44.860845: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35600 of size 256 next 463\n",
      "2023-09-22 00:25:44.860848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35700 of size 256 next 2162\n",
      "2023-09-22 00:25:44.860851: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35800 of size 256 next 2136\n",
      "2023-09-22 00:25:44.860855: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35900 of size 256 next 1594\n",
      "2023-09-22 00:25:44.860858: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35a00 of size 256 next 1011\n",
      "2023-09-22 00:25:44.860861: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35b00 of size 512 next 670\n",
      "2023-09-22 00:25:44.860865: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35d00 of size 512 next 575\n",
      "2023-09-22 00:25:44.860868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f35f00 of size 768 next 1396\n",
      "2023-09-22 00:25:44.860871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f36200 of size 256 next 123\n",
      "2023-09-22 00:25:44.860875: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f36300 of size 256 next 342\n",
      "2023-09-22 00:25:44.860878: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f36400 of size 256 next 175\n",
      "2023-09-22 00:25:44.860881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f36500 of size 256 next 716\n",
      "2023-09-22 00:25:44.860885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f36600 of size 2560 next 1091\n",
      "2023-09-22 00:25:44.860888: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37000 of size 512 next 299\n",
      "2023-09-22 00:25:44.860891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37200 of size 256 next 1446\n",
      "2023-09-22 00:25:44.860895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37300 of size 256 next 169\n",
      "2023-09-22 00:25:44.860898: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37400 of size 256 next 560\n",
      "2023-09-22 00:25:44.860901: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37500 of size 256 next 1014\n",
      "2023-09-22 00:25:44.860905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37600 of size 256 next 2139\n",
      "2023-09-22 00:25:44.860908: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37700 of size 256 next 2057\n",
      "2023-09-22 00:25:44.860911: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37800 of size 256 next 16\n",
      "2023-09-22 00:25:44.860915: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37900 of size 256 next 1803\n",
      "2023-09-22 00:25:44.860918: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37a00 of size 256 next 611\n",
      "2023-09-22 00:25:44.860921: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37b00 of size 256 next 1542\n",
      "2023-09-22 00:25:44.860925: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37c00 of size 256 next 881\n",
      "2023-09-22 00:25:44.860928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37d00 of size 256 next 1351\n",
      "2023-09-22 00:25:44.860931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37e00 of size 256 next 357\n",
      "2023-09-22 00:25:44.860935: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f37f00 of size 13824 next 587\n",
      "2023-09-22 00:25:44.860938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f3b500 of size 512 next 178\n",
      "2023-09-22 00:25:44.860942: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f3b700 of size 8192 next 1693\n",
      "2023-09-22 00:25:44.860945: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f3d700 of size 4096 next 1432\n",
      "2023-09-22 00:25:44.860949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f3e700 of size 2048 next 99\n",
      "2023-09-22 00:25:44.860952: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f3ef00 of size 2048 next 1251\n",
      "2023-09-22 00:25:44.860955: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f3f700 of size 16384 next 979\n",
      "2023-09-22 00:25:44.860959: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f43700 of size 2048 next 1038\n",
      "2023-09-22 00:25:44.860962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f43f00 of size 2048 next 1147\n",
      "2023-09-22 00:25:44.860965: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f44700 of size 4096 next 1165\n",
      "2023-09-22 00:25:44.860969: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f45700 of size 25088 next 837\n",
      "2023-09-22 00:25:44.860972: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4b900 of size 4096 next 1155\n",
      "2023-09-22 00:25:44.860976: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4c900 of size 4096 next 129\n",
      "2023-09-22 00:25:44.860979: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4d900 of size 4096 next 1379\n",
      "2023-09-22 00:25:44.860982: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4e900 of size 512 next 931\n",
      "2023-09-22 00:25:44.860986: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4eb00 of size 512 next 1489\n",
      "2023-09-22 00:25:44.860989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4ed00 of size 512 next 385\n",
      "2023-09-22 00:25:44.860992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4ef00 of size 512 next 153\n",
      "2023-09-22 00:25:44.860996: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4f100 of size 512 next 295\n",
      "2023-09-22 00:25:44.860999: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4f300 of size 512 next 284\n",
      "2023-09-22 00:25:44.861003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4f500 of size 512 next 250\n",
      "2023-09-22 00:25:44.861006: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4f700 of size 512 next 321\n",
      "2023-09-22 00:25:44.861009: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f4f900 of size 16384 next 717\n",
      "2023-09-22 00:25:44.861013: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53900 of size 256 next 2152\n",
      "2023-09-22 00:25:44.861016: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53a00 of size 256 next 2160\n",
      "2023-09-22 00:25:44.861019: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53b00 of size 256 next 2148\n",
      "2023-09-22 00:25:44.861023: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53c00 of size 256 next 1283\n",
      "2023-09-22 00:25:44.861026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53d00 of size 256 next 257\n",
      "2023-09-22 00:25:44.861029: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53e00 of size 256 next 1458\n",
      "2023-09-22 00:25:44.861033: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f53f00 of size 256 next 1553\n",
      "2023-09-22 00:25:44.861036: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f54000 of size 256 next 1610\n",
      "2023-09-22 00:25:44.861040: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f54100 of size 2048 next 1092\n",
      "2023-09-22 00:25:44.861043: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f54900 of size 4096 next 1244\n",
      "2023-09-22 00:25:44.861046: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f55900 of size 4096 next 1127\n",
      "2023-09-22 00:25:44.861050: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f56900 of size 4096 next 90\n",
      "2023-09-22 00:25:44.861053: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f57900 of size 24064 next 775\n",
      "2023-09-22 00:25:44.861059: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f5d700 of size 45568 next 707\n",
      "2023-09-22 00:25:44.861063: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68900 of size 256 next 1062\n",
      "2023-09-22 00:25:44.861066: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68a00 of size 256 next 1463\n",
      "2023-09-22 00:25:44.861069: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68b00 of size 256 next 1031\n",
      "2023-09-22 00:25:44.861073: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68c00 of size 256 next 690\n",
      "2023-09-22 00:25:44.861076: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68d00 of size 256 next 2142\n",
      "2023-09-22 00:25:44.861079: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68e00 of size 256 next 1372\n",
      "2023-09-22 00:25:44.861083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f68f00 of size 256 next 1465\n",
      "2023-09-22 00:25:44.861086: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f69000 of size 256 next 63\n",
      "2023-09-22 00:25:44.861089: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f69100 of size 256 next 435\n",
      "2023-09-22 00:25:44.861093: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f69200 of size 2048 next 1750\n",
      "2023-09-22 00:25:44.861096: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f69a00 of size 2560 next 935\n",
      "2023-09-22 00:25:44.861100: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6a400 of size 256 next 1886\n",
      "2023-09-22 00:25:44.861103: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6a500 of size 256 next 1892\n",
      "2023-09-22 00:25:44.861107: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6a600 of size 256 next 1726\n",
      "2023-09-22 00:25:44.861110: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6a700 of size 256 next 258\n",
      "2023-09-22 00:25:44.861113: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6a800 of size 256 next 367\n",
      "2023-09-22 00:25:44.861117: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6a900 of size 256 next 1972\n",
      "2023-09-22 00:25:44.861120: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6aa00 of size 256 next 2018\n",
      "2023-09-22 00:25:44.861123: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6ab00 of size 256 next 259\n",
      "2023-09-22 00:25:44.861127: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6ac00 of size 256 next 1256\n",
      "2023-09-22 00:25:44.861130: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6ad00 of size 11520 next 1194\n",
      "2023-09-22 00:25:44.861134: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6da00 of size 512 next 1538\n",
      "2023-09-22 00:25:44.861137: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6dc00 of size 512 next 305\n",
      "2023-09-22 00:25:44.861140: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6de00 of size 512 next 838\n",
      "2023-09-22 00:25:44.861144: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6e000 of size 512 next 859\n",
      "2023-09-22 00:25:44.861147: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6e200 of size 512 next 420\n",
      "2023-09-22 00:25:44.861150: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6e400 of size 512 next 1073\n",
      "2023-09-22 00:25:44.861154: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6e600 of size 512 next 101\n",
      "2023-09-22 00:25:44.861157: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6e800 of size 512 next 1173\n",
      "2023-09-22 00:25:44.861160: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6ea00 of size 4096 next 1623\n",
      "2023-09-22 00:25:44.861164: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6fa00 of size 512 next 391\n",
      "2023-09-22 00:25:44.861167: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6fc00 of size 512 next 255\n",
      "2023-09-22 00:25:44.861170: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f6fe00 of size 512 next 926\n",
      "2023-09-22 00:25:44.861174: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70000 of size 512 next 347\n",
      "2023-09-22 00:25:44.861177: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70200 of size 512 next 442\n",
      "2023-09-22 00:25:44.861180: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70400 of size 512 next 535\n",
      "2023-09-22 00:25:44.861183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70600 of size 512 next 1691\n",
      "2023-09-22 00:25:44.861187: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70800 of size 512 next 919\n",
      "2023-09-22 00:25:44.861190: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70a00 of size 256 next 1048\n",
      "2023-09-22 00:25:44.861193: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70b00 of size 256 next 1079\n",
      "2023-09-22 00:25:44.861197: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70c00 of size 256 next 1184\n",
      "2023-09-22 00:25:44.861200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70d00 of size 256 next 460\n",
      "2023-09-22 00:25:44.861204: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70e00 of size 256 next 1615\n",
      "2023-09-22 00:25:44.861207: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f70f00 of size 256 next 514\n",
      "2023-09-22 00:25:44.861210: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f71000 of size 256 next 24\n",
      "2023-09-22 00:25:44.861214: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f71100 of size 256 next 94\n",
      "2023-09-22 00:25:44.861217: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f71200 of size 14848 next 743\n",
      "2023-09-22 00:25:44.861221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f74c00 of size 512 next 1481\n",
      "2023-09-22 00:25:44.861224: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f74e00 of size 512 next 291\n",
      "2023-09-22 00:25:44.861228: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f75000 of size 512 next 332\n",
      "2023-09-22 00:25:44.861231: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f75200 of size 2048 next 2069\n",
      "2023-09-22 00:25:44.861234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f75a00 of size 2048 next 1505\n",
      "2023-09-22 00:25:44.861238: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76200 of size 512 next 1279\n",
      "2023-09-22 00:25:44.861241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76400 of size 512 next 815\n",
      "2023-09-22 00:25:44.861244: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76600 of size 512 next 480\n",
      "2023-09-22 00:25:44.861248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76800 of size 512 next 691\n",
      "2023-09-22 00:25:44.861251: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76a00 of size 512 next 1412\n",
      "2023-09-22 00:25:44.861255: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76c00 of size 512 next 1225\n",
      "2023-09-22 00:25:44.861258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f76e00 of size 512 next 484\n",
      "2023-09-22 00:25:44.861261: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f77000 of size 512 next 266\n",
      "2023-09-22 00:25:44.861265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f77200 of size 44800 next 726\n",
      "2023-09-22 00:25:44.861268: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f82100 of size 256 next 1686\n",
      "2023-09-22 00:25:44.861272: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f82200 of size 45824 next 820\n",
      "2023-09-22 00:25:44.861275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f8d500 of size 18176 next 1231\n",
      "2023-09-22 00:25:44.861279: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f91c00 of size 256 next 1036\n",
      "2023-09-22 00:25:44.861282: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f91d00 of size 256 next 1477\n",
      "2023-09-22 00:25:44.861285: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f91e00 of size 256 next 1997\n",
      "2023-09-22 00:25:44.861289: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f91f00 of size 256 next 1567\n",
      "2023-09-22 00:25:44.861292: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f92000 of size 16384 next 1320\n",
      "2023-09-22 00:25:44.861295: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f96000 of size 16384 next 319\n",
      "2023-09-22 00:25:44.861299: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f9a000 of size 16384 next 20\n",
      "2023-09-22 00:25:44.861302: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767f9e000 of size 16896 next 955\n",
      "2023-09-22 00:25:44.861306: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fa2200 of size 16384 next 345\n",
      "2023-09-22 00:25:44.861309: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fa6200 of size 30720 next 518\n",
      "2023-09-22 00:25:44.861313: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fada00 of size 16384 next 438\n",
      "2023-09-22 00:25:44.861316: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fb1a00 of size 16384 next 429\n",
      "2023-09-22 00:25:44.861319: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fb5a00 of size 16896 next 630\n",
      "2023-09-22 00:25:44.861323: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fb9c00 of size 16384 next 1627\n",
      "2023-09-22 00:25:44.861326: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fbdc00 of size 30720 next 172\n",
      "2023-09-22 00:25:44.861329: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fc5400 of size 16384 next 433\n",
      "2023-09-22 00:25:44.861333: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fc9400 of size 16896 next 1482\n",
      "2023-09-22 00:25:44.861336: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fcd600 of size 16384 next 1128\n",
      "2023-09-22 00:25:44.861340: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fd1600 of size 20480 next 934\n",
      "2023-09-22 00:25:44.861343: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fd6600 of size 256 next 657\n",
      "2023-09-22 00:25:44.861346: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fd6700 of size 256 next 1628\n",
      "2023-09-22 00:25:44.861350: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fd6800 of size 512 next 665\n",
      "2023-09-22 00:25:44.861353: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fd6a00 of size 16384 next 161\n",
      "2023-09-22 00:25:44.861356: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fdaa00 of size 16896 next 1084\n",
      "2023-09-22 00:25:44.861360: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fdec00 of size 20224 next 941\n",
      "2023-09-22 00:25:44.861363: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fe3b00 of size 256 next 813\n",
      "2023-09-22 00:25:44.861366: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fe3c00 of size 256 next 902\n",
      "2023-09-22 00:25:44.861370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2767fe3d00 of size 131072 next 2132\n",
      "2023-09-22 00:25:44.861373: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768003d00 of size 131072 next 790\n",
      "2023-09-22 00:25:44.861377: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768023d00 of size 131072 next 1526\n",
      "2023-09-22 00:25:44.861380: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768043d00 of size 131072 next 2129\n",
      "2023-09-22 00:25:44.861383: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768063d00 of size 131072 next 532\n",
      "2023-09-22 00:25:44.861387: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768083d00 of size 131072 next 92\n",
      "2023-09-22 00:25:44.861390: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27680a3d00 of size 131072 next 595\n",
      "2023-09-22 00:25:44.861393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27680c3d00 of size 131072 next 2125\n",
      "2023-09-22 00:25:44.861397: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27680e3d00 of size 131072 next 2036\n",
      "2023-09-22 00:25:44.861400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768103d00 of size 166144 next 816\n",
      "2023-09-22 00:25:44.861405: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812c600 of size 256 next 506\n",
      "2023-09-22 00:25:44.861409: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812c700 of size 256 next 964\n",
      "2023-09-22 00:25:44.861412: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812c800 of size 256 next 1013\n",
      "2023-09-22 00:25:44.861415: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812c900 of size 256 next 2016\n",
      "2023-09-22 00:25:44.861419: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812ca00 of size 256 next 477\n",
      "2023-09-22 00:25:44.861422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812cb00 of size 256 next 1617\n",
      "2023-09-22 00:25:44.861425: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812cc00 of size 256 next 56\n",
      "2023-09-22 00:25:44.861428: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812cd00 of size 256 next 970\n",
      "2023-09-22 00:25:44.861432: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812ce00 of size 256 next 880\n",
      "2023-09-22 00:25:44.861435: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812cf00 of size 256 next 1986\n",
      "2023-09-22 00:25:44.861438: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812d000 of size 256 next 2092\n",
      "2023-09-22 00:25:44.861442: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812d100 of size 256 next 564\n",
      "2023-09-22 00:25:44.861445: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812d200 of size 256 next 2131\n",
      "2023-09-22 00:25:44.861448: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812d300 of size 256 next 283\n",
      "2023-09-22 00:25:44.861452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812d400 of size 256 next 906\n",
      "2023-09-22 00:25:44.861455: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276812d500 of size 524288 next 329\n",
      "2023-09-22 00:25:44.861458: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27681ad500 of size 524288 next 647\n",
      "2023-09-22 00:25:44.861462: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276822d500 of size 524288 next 723\n",
      "2023-09-22 00:25:44.861465: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27682ad500 of size 524288 next 513\n",
      "2023-09-22 00:25:44.861468: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276832d500 of size 524288 next 810\n",
      "2023-09-22 00:25:44.861472: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27683ad500 of size 656896 next 1121\n",
      "2023-09-22 00:25:44.861475: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276844db00 of size 524288 next 893\n",
      "2023-09-22 00:25:44.861479: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27684cdb00 of size 114688 next 1813\n",
      "2023-09-22 00:25:44.861482: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27684e9b00 of size 114688 next 2033\n",
      "2023-09-22 00:25:44.861485: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768505b00 of size 147456 next 1338\n",
      "2023-09-22 00:25:44.861489: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768529b00 of size 256256 next 458\n",
      "2023-09-22 00:25:44.861492: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768568400 of size 256 next 839\n",
      "2023-09-22 00:25:44.861495: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768568500 of size 524288 next 544\n",
      "2023-09-22 00:25:44.861499: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27685e8500 of size 524288 next 741\n",
      "2023-09-22 00:25:44.861502: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768668500 of size 524288 next 22\n",
      "2023-09-22 00:25:44.861505: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27686e8500 of size 524288 next 755\n",
      "2023-09-22 00:25:44.861509: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768768500 of size 524288 next 1606\n",
      "2023-09-22 00:25:44.861512: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27687e8500 of size 524288 next 1357\n",
      "2023-09-22 00:25:44.861515: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768868500 of size 114688 next 230\n",
      "2023-09-22 00:25:44.861519: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2768884500 of size 114688 next 1990\n",
      "2023-09-22 00:25:44.861522: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27688a0500 of size 114688 next 968\n",
      "2023-09-22 00:25:44.861525: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27688bc500 of size 114688 next 213\n",
      "2023-09-22 00:25:44.861529: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27688d8500 of size 209152 next 312\n",
      "2023-09-22 00:25:44.861532: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276890b600 of size 256 next 1468\n",
      "2023-09-22 00:25:44.861536: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276890b700 of size 21614592 next 1178\n",
      "2023-09-22 00:25:44.861539: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2769da8700 of size 524288 next 1345\n",
      "2023-09-22 00:25:44.861543: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2769e28700 of size 622592 next 1051\n",
      "2023-09-22 00:25:44.861546: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2769ec0700 of size 14296576 next 478\n",
      "2023-09-22 00:25:44.861550: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ac62d00 of size 256 next 1075\n",
      "2023-09-22 00:25:44.861553: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ac62e00 of size 8192 next 1747\n",
      "2023-09-22 00:25:44.861556: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ac64e00 of size 2048 next 1721\n",
      "2023-09-22 00:25:44.861560: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ac65600 of size 120832 next 1771\n",
      "2023-09-22 00:25:44.861563: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ac82e00 of size 131072 next 1916\n",
      "2023-09-22 00:25:44.861567: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aca2e00 of size 178176 next 1788\n",
      "2023-09-22 00:25:44.861570: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acce600 of size 4096 next 1569\n",
      "2023-09-22 00:25:44.861574: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276accf600 of size 6144 next 1797\n",
      "2023-09-22 00:25:44.861577: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd0e00 of size 256 next 1931\n",
      "2023-09-22 00:25:44.861581: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd0f00 of size 256 next 1799\n",
      "2023-09-22 00:25:44.861584: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd1000 of size 256 next 1794\n",
      "2023-09-22 00:25:44.861587: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd1100 of size 256 next 1801\n",
      "2023-09-22 00:25:44.861590: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd1200 of size 2048 next 2080\n",
      "2023-09-22 00:25:44.861594: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd1a00 of size 2048 next 269\n",
      "2023-09-22 00:25:44.861597: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2200 of size 2048 next 1825\n",
      "2023-09-22 00:25:44.861601: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2a00 of size 256 next 1937\n",
      "2023-09-22 00:25:44.861604: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2b00 of size 256 next 1828\n",
      "2023-09-22 00:25:44.861607: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2c00 of size 256 next 1934\n",
      "2023-09-22 00:25:44.861611: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2d00 of size 256 next 1826\n",
      "2023-09-22 00:25:44.861614: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2e00 of size 256 next 1827\n",
      "2023-09-22 00:25:44.861617: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd2f00 of size 256 next 1930\n",
      "2023-09-22 00:25:44.861620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3000 of size 256 next 1935\n",
      "2023-09-22 00:25:44.861624: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3100 of size 256 next 1795\n",
      "2023-09-22 00:25:44.861627: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3200 of size 256 next 1831\n",
      "2023-09-22 00:25:44.861630: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3300 of size 256 next 1833\n",
      "2023-09-22 00:25:44.861634: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3400 of size 256 next 1834\n",
      "2023-09-22 00:25:44.861637: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3500 of size 256 next 1332\n",
      "2023-09-22 00:25:44.861640: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3600 of size 256 next 1731\n",
      "2023-09-22 00:25:44.861644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3700 of size 256 next 1798\n",
      "2023-09-22 00:25:44.861647: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3800 of size 256 next 2126\n",
      "2023-09-22 00:25:44.861651: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3900 of size 256 next 2012\n",
      "2023-09-22 00:25:44.861654: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3a00 of size 256 next 1842\n",
      "2023-09-22 00:25:44.861657: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3b00 of size 256 next 1843\n",
      "2023-09-22 00:25:44.861661: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3c00 of size 256 next 2124\n",
      "2023-09-22 00:25:44.861664: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3d00 of size 256 next 1845\n",
      "2023-09-22 00:25:44.861667: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3e00 of size 256 next 1846\n",
      "2023-09-22 00:25:44.861671: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd3f00 of size 256 next 1847\n",
      "2023-09-22 00:25:44.861674: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4000 of size 256 next 1848\n",
      "2023-09-22 00:25:44.861677: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4100 of size 256 next 1849\n",
      "2023-09-22 00:25:44.861681: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4200 of size 256 next 1006\n",
      "2023-09-22 00:25:44.861684: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4300 of size 256 next 1853\n",
      "2023-09-22 00:25:44.861688: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4400 of size 256 next 1863\n",
      "2023-09-22 00:25:44.861691: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4500 of size 256 next 1870\n",
      "2023-09-22 00:25:44.861694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4600 of size 256 next 1856\n",
      "2023-09-22 00:25:44.861698: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4700 of size 256 next 1857\n",
      "2023-09-22 00:25:44.861701: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4800 of size 256 next 1858\n",
      "2023-09-22 00:25:44.861704: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4900 of size 256 next 1859\n",
      "2023-09-22 00:25:44.861708: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acd4a00 of size 131072 next 1860\n",
      "2023-09-22 00:25:44.861711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276acf4a00 of size 114688 next 1217\n",
      "2023-09-22 00:25:44.861714: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad10a00 of size 193792 next 93\n",
      "2023-09-22 00:25:44.861718: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad3ff00 of size 256 next 679\n",
      "2023-09-22 00:25:44.861721: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40000 of size 256 next 708\n",
      "2023-09-22 00:25:44.861724: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40100 of size 256 next 1441\n",
      "2023-09-22 00:25:44.861728: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40200 of size 256 next 1103\n",
      "2023-09-22 00:25:44.861731: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40300 of size 256 next 1384\n",
      "2023-09-22 00:25:44.861734: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40400 of size 256 next 474\n",
      "2023-09-22 00:25:44.861738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40500 of size 256 next 1061\n",
      "2023-09-22 00:25:44.861741: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40600 of size 768 next 1226\n",
      "2023-09-22 00:25:44.861744: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad40900 of size 131072 next 1462\n",
      "2023-09-22 00:25:44.861748: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad60900 of size 131072 next 1988\n",
      "2023-09-22 00:25:44.861751: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ad80900 of size 131072 next 1973\n",
      "2023-09-22 00:25:44.861754: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ada0900 of size 131072 next 1994\n",
      "2023-09-22 00:25:44.861758: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276adc0900 of size 131072 next 1815\n",
      "2023-09-22 00:25:44.861761: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ade0900 of size 131072 next 2137\n",
      "2023-09-22 00:25:44.861764: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae00900 of size 131072 next 769\n",
      "2023-09-22 00:25:44.861768: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae20900 of size 131072 next 1984\n",
      "2023-09-22 00:25:44.861771: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae40900 of size 131072 next 954\n",
      "2023-09-22 00:25:44.861774: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae60900 of size 131072 next 226\n",
      "2023-09-22 00:25:44.861778: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae80900 of size 32768 next 59\n",
      "2023-09-22 00:25:44.861782: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88900 of size 256 next 173\n",
      "2023-09-22 00:25:44.861785: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88a00 of size 256 next 1368\n",
      "2023-09-22 00:25:44.861788: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88b00 of size 256 next 224\n",
      "2023-09-22 00:25:44.861792: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88c00 of size 256 next 1108\n",
      "2023-09-22 00:25:44.861795: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88d00 of size 256 next 888\n",
      "2023-09-22 00:25:44.861798: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88e00 of size 256 next 245\n",
      "2023-09-22 00:25:44.861802: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae88f00 of size 256 next 1350\n",
      "2023-09-22 00:25:44.861805: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89000 of size 256 next 846\n",
      "2023-09-22 00:25:44.861808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89100 of size 256 next 1064\n",
      "2023-09-22 00:25:44.861811: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89200 of size 256 next 1427\n",
      "2023-09-22 00:25:44.861815: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89300 of size 256 next 774\n",
      "2023-09-22 00:25:44.861818: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89400 of size 256 next 1088\n",
      "2023-09-22 00:25:44.861821: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89500 of size 256 next 1023\n",
      "2023-09-22 00:25:44.861825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89600 of size 256 next 1359\n",
      "2023-09-22 00:25:44.861828: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89700 of size 256 next 373\n",
      "2023-09-22 00:25:44.861831: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89800 of size 256 next 1613\n",
      "2023-09-22 00:25:44.861835: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89900 of size 256 next 713\n",
      "2023-09-22 00:25:44.861838: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89a00 of size 256 next 980\n",
      "2023-09-22 00:25:44.861841: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89b00 of size 256 next 1459\n",
      "2023-09-22 00:25:44.861845: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89c00 of size 256 next 1374\n",
      "2023-09-22 00:25:44.861848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89d00 of size 256 next 177\n",
      "2023-09-22 00:25:44.861852: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89e00 of size 256 next 1018\n",
      "2023-09-22 00:25:44.861855: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae89f00 of size 256 next 459\n",
      "2023-09-22 00:25:44.861858: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a000 of size 256 next 1008\n",
      "2023-09-22 00:25:44.861862: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a100 of size 256 next 340\n",
      "2023-09-22 00:25:44.861865: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a200 of size 256 next 1313\n",
      "2023-09-22 00:25:44.861868: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a300 of size 256 next 1298\n",
      "2023-09-22 00:25:44.861872: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a400 of size 256 next 112\n",
      "2023-09-22 00:25:44.861875: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a500 of size 256 next 854\n",
      "2023-09-22 00:25:44.861878: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a600 of size 256 next 1261\n",
      "2023-09-22 00:25:44.861882: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a700 of size 512 next 759\n",
      "2023-09-22 00:25:44.861885: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8a900 of size 256 next 712\n",
      "2023-09-22 00:25:44.861888: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8aa00 of size 256 next 1706\n",
      "2023-09-22 00:25:44.861892: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8ab00 of size 256 next 1375\n",
      "2023-09-22 00:25:44.861895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8ac00 of size 256 next 889\n",
      "2023-09-22 00:25:44.861898: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8ad00 of size 256 next 124\n",
      "2023-09-22 00:25:44.861902: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8ae00 of size 256 next 997\n",
      "2023-09-22 00:25:44.861905: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8af00 of size 256 next 571\n",
      "2023-09-22 00:25:44.861908: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b000 of size 256 next 289\n",
      "2023-09-22 00:25:44.861912: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b100 of size 256 next 972\n",
      "2023-09-22 00:25:44.861915: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b200 of size 256 next 1488\n",
      "2023-09-22 00:25:44.861918: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b300 of size 256 next 1020\n",
      "2023-09-22 00:25:44.861921: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b400 of size 256 next 18\n",
      "2023-09-22 00:25:44.861925: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f276ae8b500 of size 512 next 646\n",
      "2023-09-22 00:25:44.861928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b700 of size 256 next 719\n",
      "2023-09-22 00:25:44.861931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b800 of size 256 next 1382\n",
      "2023-09-22 00:25:44.861935: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276ae8b900 of size 256 next 1492\n",
      "2023-09-22 00:25:44.861938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f276ae8ba00 of size 285440 next 1661\n",
      "2023-09-22 00:25:44.861941: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aed1500 of size 256 next 1129\n",
      "2023-09-22 00:25:44.861945: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aed1600 of size 256 next 43\n",
      "2023-09-22 00:25:44.861948: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aed1700 of size 2560 next 1461\n",
      "2023-09-22 00:25:44.861951: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aed2100 of size 131072 next 1257\n",
      "2023-09-22 00:25:44.861955: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aef2100 of size 131072 next 2103\n",
      "2023-09-22 00:25:44.861958: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276af12100 of size 131072 next 2143\n",
      "2023-09-22 00:25:44.861962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276af32100 of size 131072 next 1274\n",
      "2023-09-22 00:25:44.861965: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276af52100 of size 131072 next 911\n",
      "2023-09-22 00:25:44.861968: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276af72100 of size 131072 next 1780\n",
      "2023-09-22 00:25:44.861972: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276af92100 of size 131072 next 179\n",
      "2023-09-22 00:25:44.861975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276afb2100 of size 135424 next 1307\n",
      "2023-09-22 00:25:44.861980: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276afd3200 of size 256 next 530\n",
      "2023-09-22 00:25:44.861983: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276afd3300 of size 256 next 1126\n",
      "2023-09-22 00:25:44.861987: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276afd3400 of size 131072 next 1041\n",
      "2023-09-22 00:25:44.861990: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276aff3400 of size 131072 next 296\n",
      "2023-09-22 00:25:44.861993: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b013400 of size 131072 next 191\n",
      "2023-09-22 00:25:44.861997: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b033400 of size 131072 next 1958\n",
      "2023-09-22 00:25:44.862000: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b053400 of size 137728 next 1102\n",
      "2023-09-22 00:25:44.862004: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b074e00 of size 256 next 643\n",
      "2023-09-22 00:25:44.862007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b074f00 of size 512 next 1152\n",
      "2023-09-22 00:25:44.862010: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075100 of size 768 next 735\n",
      "2023-09-22 00:25:44.862014: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075400 of size 256 next 872\n",
      "2023-09-22 00:25:44.862017: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075500 of size 768 next 695\n",
      "2023-09-22 00:25:44.862021: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075800 of size 768 next 550\n",
      "2023-09-22 00:25:44.862024: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075b00 of size 256 next 1476\n",
      "2023-09-22 00:25:44.862027: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075c00 of size 256 next 1563\n",
      "2023-09-22 00:25:44.862031: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075d00 of size 256 next 490\n",
      "2023-09-22 00:25:44.862034: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075e00 of size 256 next 1243\n",
      "2023-09-22 00:25:44.862037: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b075f00 of size 12288 next 428\n",
      "2023-09-22 00:25:44.862041: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b078f00 of size 4096 next 722\n",
      "2023-09-22 00:25:44.862044: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b079f00 of size 4096 next 1241\n",
      "2023-09-22 00:25:44.862047: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b07af00 of size 4096 next 1430\n",
      "2023-09-22 00:25:44.862051: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b07bf00 of size 3840 next 1470\n",
      "2023-09-22 00:25:44.862054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b07ce00 of size 256 next 1434\n",
      "2023-09-22 00:25:44.862057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b07cf00 of size 20480 next 572\n",
      "2023-09-22 00:25:44.862061: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b081f00 of size 16384 next 325\n",
      "2023-09-22 00:25:44.862064: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b085f00 of size 16384 next 668\n",
      "2023-09-22 00:25:44.862068: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b089f00 of size 16384 next 198\n",
      "2023-09-22 00:25:44.862071: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b08df00 of size 16384 next 98\n",
      "2023-09-22 00:25:44.862075: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b091f00 of size 22016 next 779\n",
      "2023-09-22 00:25:44.862078: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b097500 of size 16384 next 1028\n",
      "2023-09-22 00:25:44.862081: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b09b500 of size 20736 next 912\n",
      "2023-09-22 00:25:44.862085: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b0a0600 of size 256 next 1475\n",
      "2023-09-22 00:25:44.862088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b0a0700 of size 256 next 202\n",
      "2023-09-22 00:25:44.862092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b0a0800 of size 131072 next 869\n",
      "2023-09-22 00:25:44.862095: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b0c0800 of size 131072 next 1855\n",
      "2023-09-22 00:25:44.862098: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b0e0800 of size 131072 next 604\n",
      "2023-09-22 00:25:44.862102: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b100800 of size 147200 next 924\n",
      "2023-09-22 00:25:44.862105: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b124700 of size 256 next 1510\n",
      "2023-09-22 00:25:44.862109: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b124800 of size 8192 next 1865\n",
      "2023-09-22 00:25:44.862112: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b126800 of size 8192 next 274\n",
      "2023-09-22 00:25:44.862115: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b128800 of size 8192 next 1959\n",
      "2023-09-22 00:25:44.862119: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b12a800 of size 8192 next 950\n",
      "2023-09-22 00:25:44.862122: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b12c800 of size 8448 next 1890\n",
      "2023-09-22 00:25:44.862125: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b12e900 of size 256 next 1883\n",
      "2023-09-22 00:25:44.862129: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b12ea00 of size 256 next 1884\n",
      "2023-09-22 00:25:44.862132: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b12eb00 of size 131072 next 1671\n",
      "2023-09-22 00:25:44.862135: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b14eb00 of size 131072 next 2078\n",
      "2023-09-22 00:25:44.862139: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b16eb00 of size 131072 next 2079\n",
      "2023-09-22 00:25:44.862142: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b18eb00 of size 135168 next 1713\n",
      "2023-09-22 00:25:44.862146: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b1afb00 of size 167424 next 1766\n",
      "2023-09-22 00:25:44.862149: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b1d8900 of size 131072 next 2096\n",
      "2023-09-22 00:25:44.862152: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b1f8900 of size 141312 next 856\n",
      "2023-09-22 00:25:44.862156: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b21b100 of size 55808 next 310\n",
      "2023-09-22 00:25:44.862159: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b228b00 of size 512 next 1206\n",
      "2023-09-22 00:25:44.862163: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b228d00 of size 256 next 281\n",
      "2023-09-22 00:25:44.862166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b228e00 of size 256 next 1151\n",
      "2023-09-22 00:25:44.862169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b228f00 of size 512 next 981\n",
      "2023-09-22 00:25:44.862173: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b229100 of size 8192 next 2158\n",
      "2023-09-22 00:25:44.862176: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22b100 of size 8192 next 1579\n",
      "2023-09-22 00:25:44.862179: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22d100 of size 256 next 1077\n",
      "2023-09-22 00:25:44.862183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22d200 of size 256 next 932\n",
      "2023-09-22 00:25:44.862186: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22d300 of size 256 next 993\n",
      "2023-09-22 00:25:44.862189: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22d400 of size 256 next 654\n",
      "2023-09-22 00:25:44.862193: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22d500 of size 256 next 356\n",
      "2023-09-22 00:25:44.862196: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22d600 of size 8192 next 2167\n",
      "2023-09-22 00:25:44.862199: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b22f600 of size 8192 next 1125\n",
      "2023-09-22 00:25:44.862203: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b231600 of size 12032 next 76\n",
      "2023-09-22 00:25:44.862206: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b234500 of size 512 next 776\n",
      "2023-09-22 00:25:44.862209: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b234700 of size 512 next 277\n",
      "2023-09-22 00:25:44.862213: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b234900 of size 512 next 364\n",
      "2023-09-22 00:25:44.862216: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b234b00 of size 512 next 1659\n",
      "2023-09-22 00:25:44.862219: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b234d00 of size 512 next 1603\n",
      "2023-09-22 00:25:44.862223: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b234f00 of size 512 next 1040\n",
      "2023-09-22 00:25:44.862226: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b235100 of size 512 next 408\n",
      "2023-09-22 00:25:44.862229: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b235300 of size 38912 next 542\n",
      "2023-09-22 00:25:44.862235: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b23eb00 of size 4096 next 599\n",
      "2023-09-22 00:25:44.862238: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b23fb00 of size 37376 next 845\n",
      "2023-09-22 00:25:44.862241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b248d00 of size 4096 next 901\n",
      "2023-09-22 00:25:44.862245: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b249d00 of size 4096 next 1478\n",
      "2023-09-22 00:25:44.862248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24ad00 of size 4096 next 209\n",
      "2023-09-22 00:25:44.862251: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24bd00 of size 512 next 877\n",
      "2023-09-22 00:25:44.862255: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24bf00 of size 512 next 479\n",
      "2023-09-22 00:25:44.862258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24c100 of size 512 next 1600\n",
      "2023-09-22 00:25:44.862261: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24c300 of size 512 next 444\n",
      "2023-09-22 00:25:44.862265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24c500 of size 512 next 1556\n",
      "2023-09-22 00:25:44.862268: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24c700 of size 512 next 1598\n",
      "2023-09-22 00:25:44.862272: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24c900 of size 256 next 19\n",
      "2023-09-22 00:25:44.862275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24ca00 of size 256 next 1641\n",
      "2023-09-22 00:25:44.862278: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24cb00 of size 256 next 286\n",
      "2023-09-22 00:25:44.862282: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24cc00 of size 256 next 210\n",
      "2023-09-22 00:25:44.862285: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b24cd00 of size 32768 next 2006\n",
      "2023-09-22 00:25:44.862288: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b254d00 of size 59904 next 1469\n",
      "2023-09-22 00:25:44.862292: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b263700 of size 4096 next 1266\n",
      "2023-09-22 00:25:44.862295: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b264700 of size 16384 next 2135\n",
      "2023-09-22 00:25:44.862298: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b268700 of size 16384 next 1186\n",
      "2023-09-22 00:25:44.862302: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b26c700 of size 4096 next 2157\n",
      "2023-09-22 00:25:44.862305: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b26d700 of size 4096 next 1608\n",
      "2023-09-22 00:25:44.862308: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b26e700 of size 4096 next 2000\n",
      "2023-09-22 00:25:44.862312: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b26f700 of size 4096 next 363\n",
      "2023-09-22 00:25:44.862315: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b270700 of size 512 next 1821\n",
      "2023-09-22 00:25:44.862318: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b270900 of size 512 next 818\n",
      "2023-09-22 00:25:44.862322: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b270b00 of size 512 next 1682\n",
      "2023-09-22 00:25:44.862325: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b270d00 of size 512 next 913\n",
      "2023-09-22 00:25:44.862328: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b270f00 of size 512 next 821\n",
      "2023-09-22 00:25:44.862332: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271100 of size 512 next 1971\n",
      "2023-09-22 00:25:44.862335: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271300 of size 256 next 2115\n",
      "2023-09-22 00:25:44.862338: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271400 of size 256 next 1360\n",
      "2023-09-22 00:25:44.862342: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271500 of size 512 next 307\n",
      "2023-09-22 00:25:44.862345: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271700 of size 512 next 1791\n",
      "2023-09-22 00:25:44.862349: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271900 of size 512 next 915\n",
      "2023-09-22 00:25:44.862352: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271b00 of size 512 next 2153\n",
      "2023-09-22 00:25:44.862355: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271d00 of size 512 next 2161\n",
      "2023-09-22 00:25:44.862359: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b271f00 of size 512 next 297\n",
      "2023-09-22 00:25:44.862362: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b272100 of size 4096 next 1718\n",
      "2023-09-22 00:25:44.862365: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b273100 of size 4096 next 541\n",
      "2023-09-22 00:25:44.862369: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b274100 of size 4096 next 1263\n",
      "2023-09-22 00:25:44.862372: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b275100 of size 4096 next 1586\n",
      "2023-09-22 00:25:44.862375: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b276100 of size 7168 next 239\n",
      "2023-09-22 00:25:44.862379: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b277d00 of size 524288 next 461\n",
      "2023-09-22 00:25:44.862382: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f276b2f7d00 of size 102706688 next 1455\n",
      "2023-09-22 00:25:44.862386: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714eab00 of size 256 next 1130\n",
      "2023-09-22 00:25:44.862389: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714eac00 of size 8960 next 365\n",
      "2023-09-22 00:25:44.862393: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714ecf00 of size 58880 next 46\n",
      "2023-09-22 00:25:44.862396: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714fb500 of size 4096 next 1060\n",
      "2023-09-22 00:25:44.862399: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714fc500 of size 4096 next 206\n",
      "2023-09-22 00:25:44.862403: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714fd500 of size 4096 next 1057\n",
      "2023-09-22 00:25:44.862406: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714fe500 of size 4608 next 17\n",
      "2023-09-22 00:25:44.862409: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27714ff700 of size 32768 next 658\n",
      "2023-09-22 00:25:44.862413: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771507700 of size 56832 next 1491\n",
      "2023-09-22 00:25:44.862416: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771515500 of size 45056 next 933\n",
      "2023-09-22 00:25:44.862420: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771520500 of size 45056 next 1679\n",
      "2023-09-22 00:25:44.862423: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277152b500 of size 524288 next 221\n",
      "2023-09-22 00:25:44.862426: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27715ab500 of size 524288 next 660\n",
      "2023-09-22 00:25:44.862430: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277162b500 of size 882176 next 982\n",
      "2023-09-22 00:25:44.862433: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771702b00 of size 512 next 1457\n",
      "2023-09-22 00:25:44.862437: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771702d00 of size 512 next 1957\n",
      "2023-09-22 00:25:44.862440: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771702f00 of size 512 next 2154\n",
      "2023-09-22 00:25:44.862444: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771703100 of size 512 next 1182\n",
      "2023-09-22 00:25:44.862447: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771703300 of size 4096 next 2001\n",
      "2023-09-22 00:25:44.862450: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771704300 of size 4096 next 1448\n",
      "2023-09-22 00:25:44.862454: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771705300 of size 4096 next 52\n",
      "2023-09-22 00:25:44.862457: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706300 of size 512 next 131\n",
      "2023-09-22 00:25:44.862460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706500 of size 512 next 887\n",
      "2023-09-22 00:25:44.862464: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706700 of size 512 next 353\n",
      "2023-09-22 00:25:44.862467: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706900 of size 256 next 170\n",
      "2023-09-22 00:25:44.862470: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706a00 of size 512 next 983\n",
      "2023-09-22 00:25:44.862474: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706c00 of size 512 next 1472\n",
      "2023-09-22 00:25:44.862477: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771706e00 of size 512 next 1902\n",
      "2023-09-22 00:25:44.862480: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707000 of size 256 next 386\n",
      "2023-09-22 00:25:44.862484: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707100 of size 256 next 135\n",
      "2023-09-22 00:25:44.862487: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707200 of size 256 next 1999\n",
      "2023-09-22 00:25:44.862490: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707300 of size 256 next 645\n",
      "2023-09-22 00:25:44.862494: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707400 of size 256 next 1113\n",
      "2023-09-22 00:25:44.862497: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707500 of size 256 next 2053\n",
      "2023-09-22 00:25:44.862500: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707600 of size 256 next 167\n",
      "2023-09-22 00:25:44.862504: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707700 of size 256 next 588\n",
      "2023-09-22 00:25:44.862507: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707800 of size 256 next 88\n",
      "2023-09-22 00:25:44.862510: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707900 of size 256 next 1236\n",
      "2023-09-22 00:25:44.862514: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707a00 of size 256 next 404\n",
      "2023-09-22 00:25:44.862517: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707b00 of size 256 next 1664\n",
      "2023-09-22 00:25:44.862520: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707c00 of size 256 next 1315\n",
      "2023-09-22 00:25:44.862524: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771707d00 of size 4096 next 1407\n",
      "2023-09-22 00:25:44.862527: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771708d00 of size 4608 next 1385\n",
      "2023-09-22 00:25:44.862530: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771709f00 of size 131072 next 1948\n",
      "2023-09-22 00:25:44.862534: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771729f00 of size 131072 next 1681\n",
      "2023-09-22 00:25:44.862537: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771749f00 of size 131072 next 1837\n",
      "2023-09-22 00:25:44.862540: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771769f00 of size 131072 next 1938\n",
      "2023-09-22 00:25:44.862544: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2771789f00 of size 131072 next 2056\n",
      "2023-09-22 00:25:44.862547: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27717a9f00 of size 131072 next 1949\n",
      "2023-09-22 00:25:44.862551: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27717c9f00 of size 136960 next 303\n",
      "2023-09-22 00:25:44.862556: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27717eb600 of size 768 next 1575\n",
      "2023-09-22 00:25:44.862559: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27717eb900 of size 256 next 214\n",
      "2023-09-22 00:25:44.862563: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27717eba00 of size 256 next 1428\n",
      "2023-09-22 00:25:44.862566: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f27717ebb00 of size 40632320 next 733\n",
      "2023-09-22 00:25:44.862569: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2773eabb00 of size 131072 next 2138\n",
      "2023-09-22 00:25:44.862573: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2773ecbb00 of size 196608 next 1208\n",
      "2023-09-22 00:25:44.862576: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2773efbb00 of size 37888256 next 1421\n",
      "2023-09-22 00:25:44.862579: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277631dc00 of size 256 next 1323\n",
      "2023-09-22 00:25:44.862583: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277631dd00 of size 131072 next 1896\n",
      "2023-09-22 00:25:44.862586: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277633dd00 of size 196608 next 1905\n",
      "2023-09-22 00:25:44.862589: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277636dd00 of size 131072 next 1776\n",
      "2023-09-22 00:25:44.862593: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277638dd00 of size 131072 next 1777\n",
      "2023-09-22 00:25:44.862596: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763add00 of size 131072 next 1561\n",
      "2023-09-22 00:25:44.862599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763cdd00 of size 131072 next 1744\n",
      "2023-09-22 00:25:44.862603: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763edd00 of size 32768 next 44\n",
      "2023-09-22 00:25:44.862606: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f5d00 of size 512 next 125\n",
      "2023-09-22 00:25:44.862610: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f5f00 of size 512 next 1471\n",
      "2023-09-22 00:25:44.862613: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f6100 of size 512 next 711\n",
      "2023-09-22 00:25:44.862616: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f6300 of size 4096 next 1595\n",
      "2023-09-22 00:25:44.862620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f7300 of size 4096 next 923\n",
      "2023-09-22 00:25:44.862623: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f8300 of size 4096 next 1140\n",
      "2023-09-22 00:25:44.862626: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763f9300 of size 4096 next 21\n",
      "2023-09-22 00:25:44.862630: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fa300 of size 4096 next 1120\n",
      "2023-09-22 00:25:44.862633: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fb300 of size 4096 next 1605\n",
      "2023-09-22 00:25:44.862636: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fc300 of size 512 next 1546\n",
      "2023-09-22 00:25:44.862640: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fc500 of size 512 next 765\n",
      "2023-09-22 00:25:44.862643: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fc700 of size 512 next 1039\n",
      "2023-09-22 00:25:44.862646: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fc900 of size 512 next 636\n",
      "2023-09-22 00:25:44.862650: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fcb00 of size 512 next 1550\n",
      "2023-09-22 00:25:44.862653: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fcd00 of size 512 next 1670\n",
      "2023-09-22 00:25:44.862656: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27763fcf00 of size 32768 next 992\n",
      "2023-09-22 00:25:44.862660: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776404f00 of size 32768 next 164\n",
      "2023-09-22 00:25:44.862663: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277640cf00 of size 256 next 927\n",
      "2023-09-22 00:25:44.862666: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277640d000 of size 256 next 100\n",
      "2023-09-22 00:25:44.862670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277640d100 of size 32768 next 782\n",
      "2023-09-22 00:25:44.862673: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776415100 of size 35840 next 830\n",
      "2023-09-22 00:25:44.862676: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277641dd00 of size 524288 next 1162\n",
      "2023-09-22 00:25:44.862680: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277649dd00 of size 8192 next 2086\n",
      "2023-09-22 00:25:44.862683: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277649fd00 of size 8192 next 2061\n",
      "2023-09-22 00:25:44.862687: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27764a1d00 of size 24576 next 2050\n",
      "2023-09-22 00:25:44.862690: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27764a7d00 of size 256 next 2052\n",
      "2023-09-22 00:25:44.862694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27764a7e00 of size 131072 next 1840\n",
      "2023-09-22 00:25:44.862697: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27764c7e00 of size 237824 next 2062\n",
      "2023-09-22 00:25:44.862700: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776501f00 of size 256 next 2071\n",
      "2023-09-22 00:25:44.862704: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776502000 of size 256 next 2065\n",
      "2023-09-22 00:25:44.862707: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776502100 of size 524288 next 149\n",
      "2023-09-22 00:25:44.862710: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776582100 of size 524288 next 508\n",
      "2023-09-22 00:25:44.862714: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776602100 of size 524288 next 527\n",
      "2023-09-22 00:25:44.862717: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776682100 of size 524288 next 1629\n",
      "2023-09-22 00:25:44.862721: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2776702100 of size 637952 next 1192\n",
      "2023-09-22 00:25:44.862724: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277679dd00 of size 16384 next 1638\n",
      "2023-09-22 00:25:44.862727: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a1d00 of size 20480 next 836\n",
      "2023-09-22 00:25:44.862731: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a6d00 of size 512 next 1411\n",
      "2023-09-22 00:25:44.862734: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a6f00 of size 7168 next 145\n",
      "2023-09-22 00:25:44.862737: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a8b00 of size 256 next 612\n",
      "2023-09-22 00:25:44.862741: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a8c00 of size 256 next 1991\n",
      "2023-09-22 00:25:44.862744: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a8d00 of size 256 next 1829\n",
      "2023-09-22 00:25:44.862747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a8e00 of size 256 next 1218\n",
      "2023-09-22 00:25:44.862751: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a8f00 of size 512 next 1414\n",
      "2023-09-22 00:25:44.862754: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9100 of size 512 next 895\n",
      "2023-09-22 00:25:44.862757: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9300 of size 512 next 122\n",
      "2023-09-22 00:25:44.862761: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9500 of size 512 next 942\n",
      "2023-09-22 00:25:44.862764: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9700 of size 512 next 185\n",
      "2023-09-22 00:25:44.862768: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9900 of size 256 next 1494\n",
      "2023-09-22 00:25:44.862771: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9a00 of size 512 next 1425\n",
      "2023-09-22 00:25:44.862774: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9c00 of size 512 next 470\n",
      "2023-09-22 00:25:44.862777: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767a9e00 of size 512 next 1371\n",
      "2023-09-22 00:25:44.862781: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa000 of size 256 next 2133\n",
      "2023-09-22 00:25:44.862784: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa100 of size 256 next 447\n",
      "2023-09-22 00:25:44.862787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa200 of size 256 next 1269\n",
      "2023-09-22 00:25:44.862791: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa300 of size 256 next 285\n",
      "2023-09-22 00:25:44.862794: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa400 of size 512 next 1422\n",
      "2023-09-22 00:25:44.862797: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa600 of size 256 next 844\n",
      "2023-09-22 00:25:44.862801: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa700 of size 256 next 1202\n",
      "2023-09-22 00:25:44.862804: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27767aa800 of size 256 next 1349\n",
      "2023-09-22 00:25:44.862807: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f27767aa900 of size 18535424 next 278\n",
      "2023-09-22 00:25:44.862811: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2777957d00 of size 655360 next 1440\n",
      "2023-09-22 00:25:44.862814: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f27779f7d00 of size 13107200 next 971\n",
      "2023-09-22 00:25:44.862817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778677d00 of size 524288 next 1439\n",
      "2023-09-22 00:25:44.862821: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27786f7d00 of size 524288 next 1480\n",
      "2023-09-22 00:25:44.862824: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778777d00 of size 524288 next 1497\n",
      "2023-09-22 00:25:44.862828: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27787f7d00 of size 524288 next 702\n",
      "2023-09-22 00:25:44.862831: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778877d00 of size 524288 next 1400\n",
      "2023-09-22 00:25:44.862834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27788f7d00 of size 524288 next 60\n",
      "2023-09-22 00:25:44.862838: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778977d00 of size 524288 next 510\n",
      "2023-09-22 00:25:44.862841: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27789f7d00 of size 524288 next 1816\n",
      "2023-09-22 00:25:44.862844: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778a77d00 of size 524288 next 1253\n",
      "2023-09-22 00:25:44.862848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778af7d00 of size 524288 next 1069\n",
      "2023-09-22 00:25:44.862851: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778b77d00 of size 524288 next 1348\n",
      "2023-09-22 00:25:44.862854: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778bf7d00 of size 524288 next 1529\n",
      "2023-09-22 00:25:44.862858: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778c77d00 of size 524288 next 745\n",
      "2023-09-22 00:25:44.862861: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778cf7d00 of size 524288 next 1123\n",
      "2023-09-22 00:25:44.862864: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778d77d00 of size 524288 next 256\n",
      "2023-09-22 00:25:44.862867: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778df7d00 of size 524288 next 176\n",
      "2023-09-22 00:25:44.862871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778e77d00 of size 524288 next 500\n",
      "2023-09-22 00:25:44.862874: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778ef7d00 of size 524288 next 238\n",
      "2023-09-22 00:25:44.862877: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778f77d00 of size 524288 next 1684\n",
      "2023-09-22 00:25:44.862881: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2778ff7d00 of size 524288 next 1112\n",
      "2023-09-22 00:25:44.862884: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779077d00 of size 524288 next 414\n",
      "2023-09-22 00:25:44.862887: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27790f7d00 of size 524288 next 682\n",
      "2023-09-22 00:25:44.862891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779177d00 of size 524288 next 1696\n",
      "2023-09-22 00:25:44.862894: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27791f7d00 of size 524288 next 1252\n",
      "2023-09-22 00:25:44.862897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779277d00 of size 524288 next 1668\n",
      "2023-09-22 00:25:44.862901: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27792f7d00 of size 524288 next 1304\n",
      "2023-09-22 00:25:44.862904: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779377d00 of size 647680 next 151\n",
      "2023-09-22 00:25:44.862907: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779415f00 of size 524288 next 916\n",
      "2023-09-22 00:25:44.862911: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779495f00 of size 956928 next 1067\n",
      "2023-09-22 00:25:44.862914: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277957f900 of size 256 next 703\n",
      "2023-09-22 00:25:44.862918: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277957fa00 of size 524288 next 925\n",
      "2023-09-22 00:25:44.862921: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27795ffa00 of size 524288 next 832\n",
      "2023-09-22 00:25:44.862924: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277967fa00 of size 524288 next 1114\n",
      "2023-09-22 00:25:44.862928: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27796ffa00 of size 524288 next 126\n",
      "2023-09-22 00:25:44.862931: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277977fa00 of size 524288 next 1508\n",
      "2023-09-22 00:25:44.862934: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27797ffa00 of size 524288 next 166\n",
      "2023-09-22 00:25:44.862938: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277987fa00 of size 524288 next 1177\n",
      "2023-09-22 00:25:44.862941: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27798ffa00 of size 524288 next 633\n",
      "2023-09-22 00:25:44.862944: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277997fa00 of size 524288 next 835\n",
      "2023-09-22 00:25:44.862948: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f27799ffa00 of size 5365760 next 1111\n",
      "2023-09-22 00:25:44.862951: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779f1da00 of size 256 next 83\n",
      "2023-09-22 00:25:44.862955: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779f1db00 of size 256 next 802\n",
      "2023-09-22 00:25:44.862958: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779f1dc00 of size 256 next 360\n",
      "2023-09-22 00:25:44.862961: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779f1dd00 of size 256 next 1045\n",
      "2023-09-22 00:25:44.862964: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2779f1de00 of size 256 next 1383\n",
      "2023-09-22 00:25:44.862968: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2779f1df00 of size 27392768 next 1532\n",
      "2023-09-22 00:25:44.862971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277b93da00 of size 524288 next 995\n",
      "2023-09-22 00:25:44.862975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277b9bda00 of size 524288 next 1168\n",
      "2023-09-22 00:25:44.862978: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277ba3da00 of size 524288 next 833\n",
      "2023-09-22 00:25:44.862981: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277babda00 of size 524288 next 426\n",
      "2023-09-22 00:25:44.862985: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277bb3da00 of size 40960000 next 580\n",
      "2023-09-22 00:25:44.862988: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f277e24da00 of size 40960000 next 1000\n",
      "2023-09-22 00:25:44.862991: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f278095da00 of size 54701312 next 1370\n",
      "2023-09-22 00:25:44.862995: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2783d88700 of size 256 next 1442\n",
      "2023-09-22 00:25:44.862998: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2783d88800 of size 256 next 812\n",
      "2023-09-22 00:25:44.863002: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2783d88900 of size 700560128 next 349\n",
      "2023-09-22 00:25:44.863005: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27ad9a3c00 of size 700320000 next 900\n",
      "2023-09-22 00:25:44.863011: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f27d7584500 of size 630916352 next 1609\n",
      "2023-09-22 00:25:44.863014: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27fcf34a00 of size 256 next 727\n",
      "2023-09-22 00:25:44.863017: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27fcf34b00 of size 256 next 1647\n",
      "2023-09-22 00:25:44.863021: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27fcf34c00 of size 256 next 794\n",
      "2023-09-22 00:25:44.863024: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f27fcf34d00 of size 700320000 next 650\n",
      "2023-09-22 00:25:44.863028: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2826b15600 of size 655360000 next 1540\n",
      "2023-09-22 00:25:44.863031: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f284dc15600 of size 655360000 next 785\n",
      "2023-09-22 00:25:44.863034: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2874d15600 of size 655360000 next 1622\n",
      "2023-09-22 00:25:44.863038: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f289be15600 of size 6400000000 next 1642\n",
      "2023-09-22 00:25:44.863041: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2a19599600 of size 1310720000 next 282\n",
      "2023-09-22 00:25:44.863045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2a67799600 of size 6400000000 next 784\n",
      "2023-09-22 00:25:44.863048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2be4f1d600 of size 1063135744 next 18446744073709551615\n",
      "2023-09-22 00:25:44.863051: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-09-22 00:25:44.863057: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 783 Chunks of size 256 totalling 195.8KiB\n",
      "2023-09-22 00:25:44.863061: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 248 Chunks of size 512 totalling 124.0KiB\n",
      "2023-09-22 00:25:44.863065: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 16 Chunks of size 768 totalling 12.0KiB\n",
      "2023-09-22 00:25:44.863069: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-09-22 00:25:44.863073: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 88 Chunks of size 2048 totalling 176.0KiB\n",
      "2023-09-22 00:25:44.863077: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 2304 totalling 6.8KiB\n",
      "2023-09-22 00:25:44.863081: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 2560 totalling 15.0KiB\n",
      "2023-09-22 00:25:44.863086: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 2816 totalling 5.5KiB\n",
      "2023-09-22 00:25:44.863090: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 3072 totalling 6.0KiB\n",
      "2023-09-22 00:25:44.863093: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 3328 totalling 6.5KiB\n",
      "2023-09-22 00:25:44.863097: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 3584 totalling 7.0KiB\n",
      "2023-09-22 00:25:44.863101: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 3840 totalling 11.2KiB\n",
      "2023-09-22 00:25:44.863106: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 104 Chunks of size 4096 totalling 416.0KiB\n",
      "2023-09-22 00:25:44.863110: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 4352 totalling 8.5KiB\n",
      "2023-09-22 00:25:44.863114: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 4608 totalling 27.0KiB\n",
      "2023-09-22 00:25:44.863117: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 4864 totalling 9.5KiB\n",
      "2023-09-22 00:25:44.863121: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 5888 totalling 11.5KiB\n",
      "2023-09-22 00:25:44.863125: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 5 Chunks of size 6144 totalling 30.0KiB\n",
      "2023-09-22 00:25:44.863129: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 6656 totalling 26.0KiB\n",
      "2023-09-22 00:25:44.863133: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6912 totalling 6.8KiB\n",
      "2023-09-22 00:25:44.863137: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 7168 totalling 21.0KiB\n",
      "2023-09-22 00:25:44.863144: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 7680 totalling 7.5KiB\n",
      "2023-09-22 00:25:44.863148: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 57 Chunks of size 8192 totalling 456.0KiB\n",
      "2023-09-22 00:25:44.863152: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 8448 totalling 49.5KiB\n",
      "2023-09-22 00:25:44.863156: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 8704 totalling 34.0KiB\n",
      "2023-09-22 00:25:44.863160: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8960 totalling 8.8KiB\n",
      "2023-09-22 00:25:44.863164: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 13 Chunks of size 9216 totalling 117.0KiB\n",
      "2023-09-22 00:25:44.863168: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 9472 totalling 9.2KiB\n",
      "2023-09-22 00:25:44.863180: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 10240 totalling 40.0KiB\n",
      "2023-09-22 00:25:44.863184: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 10752 totalling 21.0KiB\n",
      "2023-09-22 00:25:44.863188: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11264 totalling 11.0KiB\n",
      "2023-09-22 00:25:44.863192: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 11520 totalling 11.2KiB\n",
      "2023-09-22 00:25:44.863196: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 11776 totalling 34.5KiB\n",
      "2023-09-22 00:25:44.863200: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 12032 totalling 11.8KiB\n",
      "2023-09-22 00:25:44.863204: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 12288 totalling 24.0KiB\n",
      "2023-09-22 00:25:44.863208: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 13312 totalling 26.0KiB\n",
      "2023-09-22 00:25:44.863212: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13824 totalling 13.5KiB\n",
      "2023-09-22 00:25:44.863216: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 14848 totalling 14.5KiB\n",
      "2023-09-22 00:25:44.863220: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15360 totalling 15.0KiB\n",
      "2023-09-22 00:25:44.863223: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 15616 totalling 30.5KiB\n",
      "2023-09-22 00:25:44.863227: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16128 totalling 15.8KiB\n",
      "2023-09-22 00:25:44.863231: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 46 Chunks of size 16384 totalling 736.0KiB\n",
      "2023-09-22 00:25:44.863235: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 9 Chunks of size 16896 totalling 148.5KiB\n",
      "2023-09-22 00:25:44.863239: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 17920 totalling 35.0KiB\n",
      "2023-09-22 00:25:44.863243: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 18176 totalling 17.8KiB\n",
      "2023-09-22 00:25:44.863247: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 18432 totalling 18.0KiB\n",
      "2023-09-22 00:25:44.863251: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 20224 totalling 19.8KiB\n",
      "2023-09-22 00:25:44.863256: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 6 Chunks of size 20480 totalling 120.0KiB\n",
      "2023-09-22 00:25:44.863260: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 20736 totalling 20.2KiB\n",
      "2023-09-22 00:25:44.863264: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 21504 totalling 63.0KiB\n",
      "2023-09-22 00:25:44.863268: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 22016 totalling 21.5KiB\n",
      "2023-09-22 00:25:44.863271: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 22272 totalling 43.5KiB\n",
      "2023-09-22 00:25:44.863275: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 24064 totalling 23.5KiB\n",
      "2023-09-22 00:25:44.863279: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 24576 totalling 24.0KiB\n",
      "2023-09-22 00:25:44.863283: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 25088 totalling 24.5KiB\n",
      "2023-09-22 00:25:44.863287: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 25600 totalling 25.0KiB\n",
      "2023-09-22 00:25:44.863291: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 26624 totalling 52.0KiB\n",
      "2023-09-22 00:25:44.863295: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 27136 totalling 26.5KiB\n",
      "2023-09-22 00:25:44.863299: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 27904 totalling 27.2KiB\n",
      "2023-09-22 00:25:44.863303: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 28672 totalling 56.0KiB\n",
      "2023-09-22 00:25:44.863307: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 29696 totalling 29.0KiB\n",
      "2023-09-22 00:25:44.863311: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 7 Chunks of size 30720 totalling 210.0KiB\n",
      "2023-09-22 00:25:44.863315: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 30976 totalling 30.2KiB\n",
      "2023-09-22 00:25:44.863319: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 17 Chunks of size 32768 totalling 544.0KiB\n",
      "2023-09-22 00:25:44.863323: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 35840 totalling 35.0KiB\n",
      "2023-09-22 00:25:44.863327: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 36608 totalling 35.8KiB\n",
      "2023-09-22 00:25:44.863331: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 37376 totalling 36.5KiB\n",
      "2023-09-22 00:25:44.863335: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 38144 totalling 37.2KiB\n",
      "2023-09-22 00:25:44.863339: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 38400 totalling 37.5KiB\n",
      "2023-09-22 00:25:44.863343: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 38912 totalling 38.0KiB\n",
      "2023-09-22 00:25:44.863347: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 39680 totalling 38.8KiB\n",
      "2023-09-22 00:25:44.863351: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 43008 totalling 42.0KiB\n",
      "2023-09-22 00:25:44.863355: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 44800 totalling 43.8KiB\n",
      "2023-09-22 00:25:44.863359: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 7 Chunks of size 45056 totalling 308.0KiB\n",
      "2023-09-22 00:25:44.863363: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 45568 totalling 44.5KiB\n",
      "2023-09-22 00:25:44.863367: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 45824 totalling 44.8KiB\n",
      "2023-09-22 00:25:44.863371: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 49152 totalling 48.0KiB\n",
      "2023-09-22 00:25:44.863375: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 55808 totalling 54.5KiB\n",
      "2023-09-22 00:25:44.863379: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 56832 totalling 55.5KiB\n",
      "2023-09-22 00:25:44.863382: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 57344 totalling 56.0KiB\n",
      "2023-09-22 00:25:44.863386: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 58368 totalling 57.0KiB\n",
      "2023-09-22 00:25:44.863391: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 58880 totalling 57.5KiB\n",
      "2023-09-22 00:25:44.863394: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 59904 totalling 58.5KiB\n",
      "2023-09-22 00:25:44.863398: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 63744 totalling 62.2KiB\n",
      "2023-09-22 00:25:44.863402: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 64512 totalling 63.0KiB\n",
      "2023-09-22 00:25:44.863406: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 65536 totalling 64.0KiB\n",
      "2023-09-22 00:25:44.863410: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 70144 totalling 68.5KiB\n",
      "2023-09-22 00:25:44.863414: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 77824 totalling 76.0KiB\n",
      "2023-09-22 00:25:44.863418: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 13 Chunks of size 114688 totalling 1.42MiB\n",
      "2023-09-22 00:25:44.863422: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 120832 totalling 118.0KiB\n",
      "2023-09-22 00:25:44.863426: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 123392 totalling 120.5KiB\n",
      "2023-09-22 00:25:44.863430: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 109 Chunks of size 131072 totalling 13.62MiB\n",
      "2023-09-22 00:25:44.863434: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 132608 totalling 259.0KiB\n",
      "2023-09-22 00:25:44.863438: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 134912 totalling 131.8KiB\n",
      "2023-09-22 00:25:44.863442: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 7 Chunks of size 135168 totalling 924.0KiB\n",
      "2023-09-22 00:25:44.863446: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 135424 totalling 132.2KiB\n",
      "2023-09-22 00:25:44.863450: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 136960 totalling 133.8KiB\n",
      "2023-09-22 00:25:44.863454: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 137728 totalling 134.5KiB\n",
      "2023-09-22 00:25:44.863458: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 138752 totalling 135.5KiB\n",
      "2023-09-22 00:25:44.863462: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 141312 totalling 138.0KiB\n",
      "2023-09-22 00:25:44.863466: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 147200 totalling 143.8KiB\n",
      "2023-09-22 00:25:44.863470: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 147456 totalling 432.0KiB\n",
      "2023-09-22 00:25:44.863474: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 147968 totalling 144.5KiB\n",
      "2023-09-22 00:25:44.863478: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 153088 totalling 149.5KiB\n",
      "2023-09-22 00:25:44.863482: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 154624 totalling 151.0KiB\n",
      "2023-09-22 00:25:44.863486: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 158208 totalling 154.5KiB\n",
      "2023-09-22 00:25:44.863490: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 158720 totalling 155.0KiB\n",
      "2023-09-22 00:25:44.863494: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 161536 totalling 157.8KiB\n",
      "2023-09-22 00:25:44.863498: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 163840 totalling 160.0KiB\n",
      "2023-09-22 00:25:44.863502: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 166144 totalling 162.2KiB\n",
      "2023-09-22 00:25:44.863506: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 167424 totalling 490.5KiB\n",
      "2023-09-22 00:25:44.863510: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 176128 totalling 172.0KiB\n",
      "2023-09-22 00:25:44.863514: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 178176 totalling 174.0KiB\n",
      "2023-09-22 00:25:44.863518: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 180224 totalling 176.0KiB\n",
      "2023-09-22 00:25:44.863522: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 193792 totalling 189.2KiB\n",
      "2023-09-22 00:25:44.863526: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 196608 totalling 768.0KiB\n",
      "2023-09-22 00:25:44.863530: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 208896 totalling 204.0KiB\n",
      "2023-09-22 00:25:44.863534: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 209152 totalling 204.2KiB\n",
      "2023-09-22 00:25:44.863538: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 237568 totalling 696.0KiB\n",
      "2023-09-22 00:25:44.863542: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 237824 totalling 464.5KiB\n",
      "2023-09-22 00:25:44.863546: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 254464 totalling 248.5KiB\n",
      "2023-09-22 00:25:44.863550: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 256256 totalling 250.2KiB\n",
      "2023-09-22 00:25:44.863554: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 140 Chunks of size 524288 totalling 70.00MiB\n",
      "2023-09-22 00:25:44.863558: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 532480 totalling 1.02MiB\n",
      "2023-09-22 00:25:44.863562: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 540672 totalling 528.0KiB\n",
      "2023-09-22 00:25:44.863566: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 541696 totalling 529.0KiB\n",
      "2023-09-22 00:25:44.863570: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 557056 totalling 544.0KiB\n",
      "2023-09-22 00:25:44.863574: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 568320 totalling 555.0KiB\n",
      "2023-09-22 00:25:44.863578: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 622592 totalling 1.19MiB\n",
      "2023-09-22 00:25:44.863582: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 637952 totalling 623.0KiB\n",
      "2023-09-22 00:25:44.863586: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 647680 totalling 632.5KiB\n",
      "2023-09-22 00:25:44.863589: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 8 Chunks of size 655360 totalling 5.00MiB\n",
      "2023-09-22 00:25:44.863594: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 656896 totalling 641.5KiB\n",
      "2023-09-22 00:25:44.863598: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 758272 totalling 740.5KiB\n",
      "2023-09-22 00:25:44.863602: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 819200 totalling 800.0KiB\n",
      "2023-09-22 00:25:44.863606: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 882176 totalling 861.5KiB\n",
      "2023-09-22 00:25:44.863610: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 899584 totalling 878.5KiB\n",
      "2023-09-22 00:25:44.863614: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 917504 totalling 896.0KiB\n",
      "2023-09-22 00:25:44.863618: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 956928 totalling 934.5KiB\n",
      "2023-09-22 00:25:44.863622: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 978432 totalling 955.5KiB\n",
      "2023-09-22 00:25:44.863625: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 994816 totalling 971.5KiB\n",
      "2023-09-22 00:25:44.863629: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1048320 totalling 1023.8KiB\n",
      "2023-09-22 00:25:44.863633: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 20971520 totalling 20.00MiB\n",
      "2023-09-22 00:25:44.863637: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 21614592 totalling 20.61MiB\n",
      "2023-09-22 00:25:44.863641: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 37084672 totalling 35.37MiB\n",
      "2023-09-22 00:25:44.863646: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 40960000 totalling 117.19MiB\n",
      "2023-09-22 00:25:44.863650: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 43515904 totalling 41.50MiB\n",
      "2023-09-22 00:25:44.863654: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 54701312 totalling 52.17MiB\n",
      "2023-09-22 00:25:44.863658: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 100270336 totalling 95.62MiB\n",
      "2023-09-22 00:25:44.863662: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 102706688 totalling 97.95MiB\n",
      "2023-09-22 00:25:44.863669: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 105512960 totalling 100.62MiB\n",
      "2023-09-22 00:25:44.863674: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 122882048 totalling 117.19MiB\n",
      "2023-09-22 00:25:44.863680: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 231120128 totalling 220.41MiB\n",
      "2023-09-22 00:25:44.863686: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 233280000 totalling 222.47MiB\n",
      "2023-09-22 00:25:44.863690: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 233520128 totalling 222.70MiB\n",
      "2023-09-22 00:25:44.863696: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 242727936 totalling 231.48MiB\n",
      "2023-09-22 00:25:44.863702: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 655360000 totalling 1.83GiB\n",
      "2023-09-22 00:25:44.863707: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 700320000 totalling 1.30GiB\n",
      "2023-09-22 00:25:44.863712: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 700419840 totalling 667.97MiB\n",
      "2023-09-22 00:25:44.863716: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 700560128 totalling 668.11MiB\n",
      "2023-09-22 00:25:44.863721: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 6400000000 totalling 11.92GiB\n",
      "2023-09-22 00:25:44.863726: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 18.03GiB\n",
      "2023-09-22 00:25:44.863730: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 23023583232 memory_limit_: 23023583232 available bytes: 0 curr_region_allocation_bytes_: 46047166464\n",
      "2023-09-22 00:25:44.863738: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                     23023583232\n",
      "InUse:                     19363885568\n",
      "MaxInUse:                  20674605568\n",
      "NumAllocs:                    10681409\n",
      "MaxAllocSize:               6400000000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-09-22 00:25:44.863780: W tensorflow/tsl/framework/bfc_allocator.cc:492] ********************__****************************************_____*****************************____\n",
      "2023-09-22 00:25:44.863801: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at softmax_op_gpu.cu.cc:222 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[128,8,1250,1250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph execution error:\n",
      "\n",
      "Detected at node 'model/multi_head_attention/softmax/Softmax' defined at (most recent call last):\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "      app.start()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "      handle._run()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "      await result\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"/tmp/ipykernel_2868052/11483105.py\", line 121, in <module>\n",
      "      hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3},\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/training.py\", line 1650, in fit\n",
      "      tmp_logs = self.train_function(iterator)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step\n",
      "      outputs = model.train_step(data)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n",
      "      y_pred = self(x, training=True)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/training.py\", line 561, in __call__\n",
      "      return super().__call__(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/functional.py\", line 511, in call\n",
      "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n",
      "      outputs = node.layer(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/layers/attention/multi_head_attention.py\", line 595, in call\n",
      "      attention_output, attention_scores = self._compute_attention(\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/layers/attention/multi_head_attention.py\", line 526, in _compute_attention\n",
      "      attention_scores = self._masked_softmax(\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/layers/attention/multi_head_attention.py\", line 492, in _masked_softmax\n",
      "      return self._softmax(attention_scores, attention_mask)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/layers/activation/softmax.py\", line 103, in call\n",
      "      return backend.softmax(inputs, axis=self.axis[0])\n",
      "    File \"/opt/anaconda-3-2020.02/envs/hskim/lib/python3.8/site-packages/keras/backend.py\", line 5416, in softmax\n",
      "      return tf.nn.softmax(x, axis=axis)\n",
      "Node: 'model/multi_head_attention/softmax/Softmax'\n",
      "OOM when allocating tensor with shape[128,8,1250,1250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node model/multi_head_attention/softmax/Softmax}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [Op:__inference_train_function_1108157]\n",
      "============================\n",
      "randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1217 - mean_squared_error: 0.0380\n",
      "Epoch 1: val_loss improved from inf to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 21s 334ms/step - loss: 0.1217 - mean_squared_error: 0.0380 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 2: val_loss did not improve from 0.10280\n",
      "23/23 [==============================] - 6s 281ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 3: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 299ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 4: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 5: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 6: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 7: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 8: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 292ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 9: val_loss improved from 0.10280 to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 291ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 10: val_loss improved from 0.10280 to 0.10279, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 11: val_loss improved from 0.10279 to 0.10279, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 291ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 12: val_loss improved from 0.10279 to 0.10278, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1014 - mean_squared_error: 0.0193\n",
      "Epoch 13: val_loss improved from 0.10278 to 0.10273, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1014 - mean_squared_error: 0.0193 - val_loss: 0.1027 - val_mean_squared_error: 0.0196\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 14: val_loss did not improve from 0.10273\n",
      "23/23 [==============================] - 6s 282ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1027 - val_mean_squared_error: 0.0196\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1014 - mean_squared_error: 0.0193\n",
      "Epoch 15: val_loss improved from 0.10273 to 0.10271, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_0.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1014 - mean_squared_error: 0.0193 - val_loss: 0.1027 - val_mean_squared_error: 0.0196\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1018 - mean_squared_error: 0.0196\n",
      "Epoch 16: val_loss did not improve from 0.10271\n",
      "23/23 [==============================] - 6s 282ms/step - loss: 0.1018 - mean_squared_error: 0.0196 - val_loss: 0.1027 - val_mean_squared_error: 0.0196\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 17: val_loss did not improve from 0.10271\n",
      "23/23 [==============================] - 6s 282ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 18: val_loss did not improve from 0.10271\n",
      "23/23 [==============================] - 6s 282ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "31/31 [==============================] - 2s 29ms/step\n",
      " ###0 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1210 - mean_squared_error: 0.0368\n",
      "Epoch 1: val_loss improved from inf to 0.10339, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_1.hdf5\n",
      "23/23 [==============================] - 20s 320ms/step - loss: 0.1210 - mean_squared_error: 0.0368 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 2: val_loss did not improve from 0.10339\n",
      "23/23 [==============================] - 6s 278ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 3: val_loss did not improve from 0.10339\n",
      "23/23 [==============================] - 6s 278ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 4: val_loss did not improve from 0.10339\n",
      "23/23 [==============================] - 6s 279ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "31/31 [==============================] - 2s 29ms/step\n",
      " ###1 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1211 - mean_squared_error: 0.0366\n",
      "Epoch 1: val_loss improved from inf to 0.09845, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 24s 339ms/step - loss: 0.1211 - mean_squared_error: 0.0366 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 2: val_loss improved from 0.09845 to 0.09845, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 288ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 3: val_loss improved from 0.09845 to 0.09845, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 4: val_loss improved from 0.09845 to 0.09845, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 291ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 5: val_loss improved from 0.09845 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 288ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 6: val_loss improved from 0.09844 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 7: val_loss improved from 0.09844 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 288ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 8: val_loss improved from 0.09844 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 9: val_loss improved from 0.09844 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 10: val_loss improved from 0.09844 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 11: val_loss improved from 0.09844 to 0.09844, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 12: val_loss improved from 0.09844 to 0.09843, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_2.hdf5\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 13: val_loss did not improve from 0.09843\n",
      "23/23 [==============================] - 6s 284ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 14: val_loss did not improve from 0.09843\n",
      "23/23 [==============================] - 6s 282ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 15: val_loss did not improve from 0.09843\n",
      "23/23 [==============================] - 6s 282ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "31/31 [==============================] - 1s 29ms/step\n",
      " ###2 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1237 - mean_squared_error: 0.0403\n",
      "Epoch 1: val_loss improved from inf to 0.10258, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch128_cnn4_filt16_size9_pool2_do0.5_tra5_head8_kdim128_fnn64/weights_3.hdf5\n",
      "23/23 [==============================] - 21s 348ms/step - loss: 0.1237 - mean_squared_error: 0.0403 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0194\n",
      "Epoch 2: val_loss did not improve from 0.10258\n",
      "23/23 [==============================] - 6s 280ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0194\n",
      "Epoch 3: val_loss did not improve from 0.10258\n",
      "23/23 [==============================] - 6s 280ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0194\n",
      "Epoch 4: val_loss did not improve from 0.10258\n",
      "23/23 [==============================] - 6s 280ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "31/31 [==============================] - 2s 29ms/step\n",
      " ###3 fold : val mae 0.11###\n",
      "mae2.15+-0.00\n",
      "============================\n",
      "randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn2_filt64_size19_pool5_do0.1_tra3_head8_kdim128_fnn64\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1571 - mean_squared_error: 0.0682\n",
      "Epoch 1: val_loss improved from inf to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn2_filt64_size19_pool5_do0.1_tra3_head8_kdim128_fnn64/weights_0.hdf5\n",
      "12/12 [==============================] - 12s 325ms/step - loss: 0.1571 - mean_squared_error: 0.0682 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 2: val_loss did not improve from 0.10280\n",
      "12/12 [==============================] - 3s 260ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 3: val_loss did not improve from 0.10280\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 4: val_loss did not improve from 0.10280\n",
      "12/12 [==============================] - 3s 216ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "31/31 [==============================] - 1s 14ms/step\n",
      " ###0 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1550 - mean_squared_error: 0.0666\n",
      "Epoch 1: val_loss improved from inf to 0.10339, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn2_filt64_size19_pool5_do0.1_tra3_head8_kdim128_fnn64/weights_1.hdf5\n",
      "12/12 [==============================] - 14s 286ms/step - loss: 0.1550 - mean_squared_error: 0.0666 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 2: val_loss did not improve from 0.10339\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 3: val_loss did not improve from 0.10339\n",
      "12/12 [==============================] - 3s 219ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 4: val_loss did not improve from 0.10339\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      " ###1 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1553 - mean_squared_error: 0.0653\n",
      "Epoch 1: val_loss improved from inf to 0.09845, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn2_filt64_size19_pool5_do0.1_tra3_head8_kdim128_fnn64/weights_2.hdf5\n",
      "12/12 [==============================] - 12s 310ms/step - loss: 0.1553 - mean_squared_error: 0.0653 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 2: val_loss did not improve from 0.09845\n",
      "12/12 [==============================] - 3s 239ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 3: val_loss did not improve from 0.09845\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1029 - mean_squared_error: 0.0197\n",
      "Epoch 4: val_loss did not improve from 0.09845\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "31/31 [==============================] - 1s 14ms/step\n",
      " ###2 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1570 - mean_squared_error: 0.0682\n",
      "Epoch 1: val_loss improved from inf to 0.10258, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch256_cnn2_filt64_size19_pool5_do0.1_tra3_head8_kdim128_fnn64/weights_3.hdf5\n",
      "12/12 [==============================] - 12s 331ms/step - loss: 0.1570 - mean_squared_error: 0.0682 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0194\n",
      "Epoch 2: val_loss did not improve from 0.10258\n",
      "12/12 [==============================] - 3s 238ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0194\n",
      "Epoch 3: val_loss did not improve from 0.10258\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0194\n",
      "Epoch 4: val_loss did not improve from 0.10258\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      " ###3 fold : val mae 0.11###\n",
      "mae2.15+-0.00\n",
      "============================\n",
      "randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1088 - mean_squared_error: 0.0242\n",
      "Epoch 1: val_loss improved from inf to 0.10280, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64/weights_0.hdf5\n",
      "46/46 [==============================] - 17s 76ms/step - loss: 0.1087 - mean_squared_error: 0.0242 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1014 - mean_squared_error: 0.0193\n",
      "Epoch 2: val_loss did not improve from 0.10280\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 3/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0193\n",
      "Epoch 3: val_loss did not improve from 0.10280\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "Epoch 4/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1012 - mean_squared_error: 0.0192\n",
      "Epoch 4: val_loss did not improve from 0.10280\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.1015 - mean_squared_error: 0.0193 - val_loss: 0.1028 - val_mean_squared_error: 0.0196\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      " ###0 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1076 - mean_squared_error: 0.0233\n",
      "Epoch 1: val_loss improved from inf to 0.10339, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64/weights_1.hdf5\n",
      "46/46 [==============================] - 16s 68ms/step - loss: 0.1073 - mean_squared_error: 0.0231 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1012 - mean_squared_error: 0.0192\n",
      "Epoch 2: val_loss did not improve from 0.10339\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0192\n",
      "Epoch 3: val_loss did not improve from 0.10339\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "Epoch 4/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.0193\n",
      "Epoch 4: val_loss did not improve from 0.10339\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.1013 - mean_squared_error: 0.0192 - val_loss: 0.1034 - val_mean_squared_error: 0.0200\n",
      "31/31 [==============================] - 1s 12ms/step\n",
      " ###1 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1105 - mean_squared_error: 0.0250\n",
      "Epoch 1: val_loss improved from inf to 0.09845, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64/weights_2.hdf5\n",
      "46/46 [==============================] - 20s 86ms/step - loss: 0.1104 - mean_squared_error: 0.0249 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1032 - mean_squared_error: 0.0198\n",
      "Epoch 2: val_loss did not improve from 0.09845\n",
      "46/46 [==============================] - 3s 59ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 3/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1030 - mean_squared_error: 0.0197\n",
      "Epoch 3: val_loss did not improve from 0.09845\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "Epoch 4/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1032 - mean_squared_error: 0.0198\n",
      "Epoch 4: val_loss did not improve from 0.09845\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1029 - mean_squared_error: 0.0197 - val_loss: 0.0984 - val_mean_squared_error: 0.0185\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      " ###2 fold : val mae 0.11###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1078 - mean_squared_error: 0.0235\n",
      "Epoch 1: val_loss improved from inf to 0.10258, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64/weights_3.hdf5\n",
      "46/46 [==============================] - 16s 70ms/step - loss: 0.1076 - mean_squared_error: 0.0234 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 2/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0193\n",
      "Epoch 2: val_loss did not improve from 0.10258\n",
      "46/46 [==============================] - 3s 55ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 3/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1016 - mean_squared_error: 0.0194\n",
      "Epoch 3: val_loss did not improve from 0.10258\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "Epoch 4/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1013 - mean_squared_error: 0.0193\n",
      "Epoch 4: val_loss did not improve from 0.10258\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1015 - mean_squared_error: 0.0194 - val_loss: 0.1026 - val_mean_squared_error: 0.0194\n",
      "31/31 [==============================] - 1s 12ms/step\n",
      " ###3 fold : val mae 0.11###\n",
      "mae2.15+-0.00\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 39] Directory not empty: 'randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64' -> 'randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/mae2.15+-0.00_max0__batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_mae\u001b[38;5;241m*\u001b[39mSCALE_Y\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_mae\u001b[38;5;241m*\u001b[39mSCALE_Y\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mopen\u001b[39m(odir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/model.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite(model\u001b[38;5;241m.\u001b[39mto_json())\n\u001b[0;32m--> 160\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43modir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrootdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mae\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmean_mae\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mSCALE_Y\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m+-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstd_mae\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mSCALE_Y\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_max\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmax_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m__\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43modir_f\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 39] Directory not empty: 'randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64' -> 'randomSearch/SRATE500_ECG-bandpass_z-norm_child_male_train/CNN+transformer_age%20(sigmoid)_loss(mae)-nodecay_4fold_500cases/mae2.15+-0.00_max0__batch64_cnn3_filt32_size15_pool5_do0.1_tra5_head8_kdim256_fnn64'"
     ]
    }
   ],
   "source": [
    "random.seed(98)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, LayerNormalization, Dense, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Input, concatenate, multiply, dot, MultiHeadAttention\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\n",
    "SRATE = 500  # in hz\n",
    "SEGLEN = 10 * SRATE  # samples\n",
    "#BATCH_SIZE = 256\n",
    "MAX_CASES = 500\n",
    "nfold = 4\n",
    "\n",
    "hyperparameters = {\n",
    "    \"nfilt\" : [16, 32, 64],\n",
    "    'nhead' : [2, 4, 8],\n",
    "    'kdim': [16, 64, 128, 256],\n",
    "    \"fnode\" : [32, 64, 128],\n",
    "    \"clayer\" : [2, 3, 4],\n",
    "    \"tlayer\" : [3, 4, 5],\n",
    "    \"droprate\" : [0.1, 0.2, 0.5],\n",
    "    \"filtsize\" : [5, 7, 9, 11, 15, 19],\n",
    "    'poolsize' : [2, 4, 5],\n",
    "    'batch_size': [64, 128, 256]\n",
    "}\n",
    "keys, values = zip(*hyperparameters.items())\n",
    "permutations_dicts = it.product(*values)\n",
    "permutations_dicts = list(permutations_dicts)\n",
    "random.shuffle(permutations_dicts)\n",
    "for nfilt, nhead, kdim, fnode, clayer, tlayer, droprate, filtsize, poolsize, batch_size in permutations_dicts:\n",
    "    test_start = time.time()\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    rootdir = f'randomSearch/{hyper_path}/CNN+transformer_age%{SCALE_Y}(sigmoid)_loss(mae)-nodecay_{nfold}fold_{MAX_CASES}cases'\n",
    "    odir_f = 'batch{}_cnn{}_filt{}_size{}_pool{}_do{}'.format(batch_size, clayer, nfilt, filtsize, poolsize, droprate)\n",
    "    odir_f += '_tra{}_head{}_kdim{}_fnn{}'.format(tlayer, nhead, kdim, fnode)\n",
    "    \n",
    "    if not os.path.exists(rootdir):\n",
    "        os.mkdir(rootdir)\n",
    "    \n",
    "    odir = rootdir+'/'+odir_f\n",
    "    print(\"============================\")\n",
    "    print(odir)\n",
    "    print(\"============================\")\n",
    "\n",
    "    # cnn-transformer\n",
    "    # https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "    out = inp = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    \n",
    "    # out = Conv1D(filters=kdim, kernel_size=filtsize, padding='same')(out)\n",
    "\n",
    "    # conv 여러층    \n",
    "    for i in range(clayer):\n",
    "        out = Conv1D(filters=nfilt, kernel_size=filtsize, padding='same', activation=None)(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)\n",
    "        out = MaxPooling1D(poolsize, padding='same')(out)\n",
    "    out = Dense(kdim)(out)  # 마지막 차원이 nfilt 인데 kdim 으로 바꿔야 transformer block을 쌓을 수 있다.\n",
    "    for i in range(tlayer):  # transformer\n",
    "        attn_output = MultiHeadAttention(num_heads=nhead, key_dim=kdim, attention_axes=[1,])(out, out)\n",
    "        attn_output = Dropout(droprate)(attn_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(out + attn_output)  # sum and norm\n",
    "        ffn_output = tf.keras.Sequential([Dense(fnode, activation=\"relu\"), Dense(kdim)])(out1)\n",
    "        out2 = Dropout(droprate)(ffn_output)\n",
    "        out = LayerNormalization(epsilon=1e-6)(out1 + out2)  # sum and norm\n",
    "    out = GlobalMaxPooling1D()(out)\n",
    "\n",
    "    if droprate:\n",
    "        out = Dropout(droprate)(out)\n",
    "    out = Dense(fnode)(out)\n",
    "    if droprate:\n",
    "        out = Dropout(droprate)(out)\n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "    \n",
    "    \n",
    "    # 4-fold cv\n",
    "    kfold = KFold(nfold)\n",
    "    tprs, aucs, prs = [], [], []\n",
    "    test_rmse, test_mae = [], []\n",
    "    f1_scores, thvals = [], []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    mean_recall = np.linspace(0,1,100)\n",
    "\n",
    "    switch = 0\n",
    "    caseids_train = np.unique(c_train)\n",
    "    for fold, (c_cv_trains_mask, c_cv_test_mask) in enumerate(kfold.split(caseids_train)):\n",
    "        c_cv_trains = caseids_train[c_cv_trains_mask]\n",
    "\n",
    "        cv_train_mask = np.isin(c_train, c_cv_trains)\n",
    "        cv_val_mask = ~cv_train_mask\n",
    "\n",
    "        X_train = x_train[cv_train_mask]\n",
    "        X_val = x_train[cv_val_mask]\n",
    "\n",
    "        Y_train = y_train[cv_train_mask]\n",
    "        Y_val = y_train[cv_val_mask]\n",
    "\n",
    "\n",
    "        # model 학습\n",
    "        try:\n",
    "            # learning scheduler\n",
    "            def step_decay(epoch):\n",
    "                start = 1e-3\n",
    "                drop = 0.1\n",
    "                epochs_drop = 10\n",
    "                lr = start * (drop ** np.floor((epoch)/epochs_drop))\n",
    "                return lr\n",
    "            lr_scheduler = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "            weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "            #model = multi_gpu_model(model, gpus=4)\n",
    "            model.compile(loss='mae', optimizer=Adam(lr=lr_scheduler, weight_decay=None), metrics=['mean_squared_error'])\n",
    "            hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "\n",
    "            model.load_weights(weightcache)\n",
    "            y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "            # MAE 계산\n",
    "            model_err = metrics.MeanAbsoluteError()\n",
    "            model_err.update_state(y_test, y_pred)\n",
    "            mae_val = model_err.result().numpy()\n",
    "            test_mae.append(mae_val)\n",
    "\n",
    "\n",
    "            print(f' ###{fold} fold : val mae {mae_val:.2f}###')\n",
    "            tf.keras.backend.clear_session()\n",
    "            model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            switch = 1\n",
    "            shutil.rmtree(odir)\n",
    "            break\n",
    "    ###\n",
    "    if switch:\n",
    "        switch = 0\n",
    "        tf.keras.backend.clear_session()\n",
    "        continue\n",
    "\n",
    "\n",
    "    mean_mae = np.mean(test_mae)\n",
    "    std_mae = np.std(test_mae)\n",
    "\n",
    "    max_idx = test_mae.index(min(test_mae))\n",
    "\n",
    "\n",
    "    print(f'mae{mean_mae*SCALE_Y:.2f}+-{std_mae*SCALE_Y:.2f}')\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    os.rename(odir, rootdir+f'/mae{mean_mae*SCALE_Y:.2f}+-{std_mae*SCALE_Y:.2f}_max{max_idx}__{odir_f}')\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a244db0-6867-47e5-9b07-96b09bce64c6",
   "metadata": {},
   "source": [
    "## BRNO team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8d4ef9-1a30-4d96-bbda-be88bc331b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13390,) (13390, 55) (13390, 5000, 12) (13390,)\n"
     ]
    }
   ],
   "source": [
    "SRATE = 500\n",
    "ECG_FILT = 'bandpass'\n",
    "TRAIN = 'train'\n",
    "ADULT = 'adult'\n",
    "NORM = 'z-norm'\n",
    "GENDER = 'male'\n",
    "\n",
    "SMALL  = ''\n",
    "\n",
    "\n",
    "hyper_path = f'SRATE{SRATE}_ECG-{ECG_FILT}_{NORM}_{ADULT}_{GENDER}_{TRAIN}'\n",
    "FILENAME = f'dataset/{ADULT}_train_{GENDER}{SMALL}.npz'\n",
    "with np.load(open(FILENAME, 'rb'), allow_pickle=True) as f:\n",
    "    caseid = f['caseid']\n",
    "    x_feature, x_signal, y = f['X_feature'], f['X_signal'], f['y']\n",
    "\n",
    "    \n",
    "x_signal = np.delete(x_signal, 5559, axis=0)\n",
    "x_feature = np.delete(x_feature, 5559, axis=0)\n",
    "y = np.delete(y, 5559, axis=0)\n",
    "caseid = np.delete(caseid, 5559, axis=0)        \n",
    "    \n",
    "x_train = x_signal\n",
    "print(caseid.shape, x_feature.shape, x_signal.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae30cdd-aaf9-4812-a6a1-960c4cb54bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder\n",
    "nfold = 4  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"randomSearch/{hyper_path}/CNN_4layers_{nfold}fold_test{ntest}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ea9aeb-e60d-46d0-b1d1-3e2c4de5313b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 00:02:07.423584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8904\n",
      "2023-09-21 00:02:07.884944: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-21 00:02:09.361620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-21 00:02:09.374769: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1e17c2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-21 00:02:09.374806: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2023-09-21 00:02:09.383599: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-21 00:02:09.448974: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-21 00:02:09.502453: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 0s - loss: 22.0945 - mean_squared_error: 1465.9642\n",
      "Epoch 1: val_loss improved from inf to 159.81340, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 31s 517ms/step - loss: 22.0945 - mean_squared_error: 1465.9642 - val_loss: 159.8134 - val_mean_squared_error: 48110.7031\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 11.9311 - mean_squared_error: 223.9284\n",
      "Epoch 2: val_loss improved from 159.81340 to 24.37708, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 14s 428ms/step - loss: 11.9311 - mean_squared_error: 223.9284 - val_loss: 24.3771 - val_mean_squared_error: 732.1544\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.4996 - mean_squared_error: 178.0850\n",
      "Epoch 3: val_loss improved from 24.37708 to 20.85607, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 10.4996 - mean_squared_error: 178.0850 - val_loss: 20.8561 - val_mean_squared_error: 558.2950\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.3857 - mean_squared_error: 173.9679\n",
      "Epoch 4: val_loss improved from 20.85607 to 19.30862, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 10.3857 - mean_squared_error: 173.9679 - val_loss: 19.3086 - val_mean_squared_error: 489.7553\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.7241 - mean_squared_error: 183.0186\n",
      "Epoch 5: val_loss improved from 19.30862 to 14.47594, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 14s 434ms/step - loss: 10.7241 - mean_squared_error: 183.0186 - val_loss: 14.4759 - val_mean_squared_error: 324.8720\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.2727 - mean_squared_error: 169.2604\n",
      "Epoch 6: val_loss improved from 14.47594 to 13.42676, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 14s 436ms/step - loss: 10.2727 - mean_squared_error: 169.2604 - val_loss: 13.4268 - val_mean_squared_error: 271.7940\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.0683 - mean_squared_error: 164.8087\n",
      "Epoch 7: val_loss improved from 13.42676 to 9.48572, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_0.hdf5\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 10.0683 - mean_squared_error: 164.8087 - val_loss: 9.4857 - val_mean_squared_error: 148.4078\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.3650 - mean_squared_error: 173.8412\n",
      "Epoch 8: val_loss did not improve from 9.48572\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 10.3650 - mean_squared_error: 173.8412 - val_loss: 9.4892 - val_mean_squared_error: 151.1754\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.7057 - mean_squared_error: 153.2769\n",
      "Epoch 9: val_loss did not improve from 9.48572\n",
      "32/32 [==============================] - 14s 433ms/step - loss: 9.7057 - mean_squared_error: 153.2769 - val_loss: 9.7794 - val_mean_squared_error: 161.5688\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.8419 - mean_squared_error: 157.5357\n",
      "Epoch 10: val_loss did not improve from 9.48572\n",
      "32/32 [==============================] - 14s 433ms/step - loss: 9.8419 - mean_squared_error: 157.5357 - val_loss: 9.9136 - val_mean_squared_error: 156.3069\n",
      "next step\n",
      "105/105 [==============================] - 3s 21ms/step\n",
      "mae cal\n",
      "rmse cal\n",
      " ###0 fold : val mae 9.37, val rmse 11.89###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 22.0408 - mean_squared_error: 1379.5649\n",
      "Epoch 1: val_loss improved from inf to 37.47163, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_1.hdf5\n",
      "32/32 [==============================] - 24s 475ms/step - loss: 22.0408 - mean_squared_error: 1379.5649 - val_loss: 37.4716 - val_mean_squared_error: 1910.1138\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 11.5303 - mean_squared_error: 210.6608\n",
      "Epoch 2: val_loss improved from 37.47163 to 18.31047, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_1.hdf5\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 11.5303 - mean_squared_error: 210.6608 - val_loss: 18.3105 - val_mean_squared_error: 447.1162\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.6726 - mean_squared_error: 183.8148\n",
      "Epoch 3: val_loss did not improve from 18.31047\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 10.6726 - mean_squared_error: 183.8148 - val_loss: 18.9981 - val_mean_squared_error: 469.9930\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.6244 - mean_squared_error: 180.1600\n",
      "Epoch 4: val_loss improved from 18.31047 to 15.02311, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_1.hdf5\n",
      "32/32 [==============================] - 14s 439ms/step - loss: 10.6244 - mean_squared_error: 180.1600 - val_loss: 15.0231 - val_mean_squared_error: 325.0357\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.0382 - mean_squared_error: 162.7797\n",
      "Epoch 5: val_loss did not improve from 15.02311\n",
      "32/32 [==============================] - 14s 434ms/step - loss: 10.0382 - mean_squared_error: 162.7797 - val_loss: 25.6416 - val_mean_squared_error: 789.4448\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.0380 - mean_squared_error: 163.1478\n",
      "Epoch 6: val_loss did not improve from 15.02311\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 10.0380 - mean_squared_error: 163.1478 - val_loss: 21.4477 - val_mean_squared_error: 573.9794\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.1346 - mean_squared_error: 168.0141\n",
      "Epoch 7: val_loss improved from 15.02311 to 12.72197, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_1.hdf5\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 10.1346 - mean_squared_error: 168.0141 - val_loss: 12.7220 - val_mean_squared_error: 241.4668\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.2610 - mean_squared_error: 168.5274\n",
      "Epoch 8: val_loss improved from 12.72197 to 10.33238, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_1.hdf5\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 10.2610 - mean_squared_error: 168.5274 - val_loss: 10.3324 - val_mean_squared_error: 163.7815\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.7916 - mean_squared_error: 156.3753\n",
      "Epoch 9: val_loss did not improve from 10.33238\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 9.7916 - mean_squared_error: 156.3753 - val_loss: 19.9656 - val_mean_squared_error: 518.5924\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.7357 - mean_squared_error: 155.1782\n",
      "Epoch 10: val_loss did not improve from 10.33238\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 9.7357 - mean_squared_error: 155.1782 - val_loss: 21.3487 - val_mean_squared_error: 594.5479\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.8983 - mean_squared_error: 158.5344\n",
      "Epoch 11: val_loss improved from 10.33238 to 10.32244, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_1.hdf5\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 9.8983 - mean_squared_error: 158.5344 - val_loss: 10.3224 - val_mean_squared_error: 159.4894\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.1354 - mean_squared_error: 165.6032\n",
      "Epoch 12: val_loss did not improve from 10.32244\n",
      "32/32 [==============================] - 14s 435ms/step - loss: 10.1354 - mean_squared_error: 165.6032 - val_loss: 11.2838 - val_mean_squared_error: 189.2157\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.7925 - mean_squared_error: 155.1853\n",
      "Epoch 13: val_loss did not improve from 10.32244\n",
      "32/32 [==============================] - 14s 437ms/step - loss: 9.7925 - mean_squared_error: 155.1853 - val_loss: 23.0090 - val_mean_squared_error: 653.9166\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.5738 - mean_squared_error: 149.3044\n",
      "Epoch 14: val_loss did not improve from 10.32244\n",
      "32/32 [==============================] - 14s 453ms/step - loss: 9.5738 - mean_squared_error: 149.3044 - val_loss: 17.3221 - val_mean_squared_error: 416.5386\n",
      "next step\n",
      "105/105 [==============================] - 2s 19ms/step\n",
      "mae cal\n",
      "rmse cal\n",
      " ###1 fold : val mae 10.08, val rmse 12.44###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 22.2764 - mean_squared_error: 1410.8519\n",
      "Epoch 1: val_loss improved from inf to 88.39001, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_2.hdf5\n",
      "32/32 [==============================] - 25s 498ms/step - loss: 22.2764 - mean_squared_error: 1410.8519 - val_loss: 88.3900 - val_mean_squared_error: 10888.9648\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 11.5786 - mean_squared_error: 209.5145\n",
      "Epoch 2: val_loss improved from 88.39001 to 20.21701, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_2.hdf5\n",
      "32/32 [==============================] - 14s 437ms/step - loss: 11.5786 - mean_squared_error: 209.5145 - val_loss: 20.2170 - val_mean_squared_error: 522.8632\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.6558 - mean_squared_error: 181.6925\n",
      "Epoch 3: val_loss improved from 20.21701 to 15.96322, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_2.hdf5\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 10.6558 - mean_squared_error: 181.6925 - val_loss: 15.9632 - val_mean_squared_error: 426.8172\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.8471 - mean_squared_error: 187.1044\n",
      "Epoch 4: val_loss improved from 15.96322 to 9.51101, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_2.hdf5\n",
      "32/32 [==============================] - 14s 438ms/step - loss: 10.8471 - mean_squared_error: 187.1044 - val_loss: 9.5110 - val_mean_squared_error: 144.9320\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.1698 - mean_squared_error: 166.3857\n",
      "Epoch 5: val_loss did not improve from 9.51101\n",
      "32/32 [==============================] - 14s 434ms/step - loss: 10.1698 - mean_squared_error: 166.3857 - val_loss: 18.1134 - val_mean_squared_error: 435.5876\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.2896 - mean_squared_error: 171.1633\n",
      "Epoch 6: val_loss did not improve from 9.51101\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 10.2896 - mean_squared_error: 171.1633 - val_loss: 11.4195 - val_mean_squared_error: 189.2100\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.9312 - mean_squared_error: 158.2728\n",
      "Epoch 7: val_loss did not improve from 9.51101\n",
      "32/32 [==============================] - 14s 435ms/step - loss: 9.9312 - mean_squared_error: 158.2728 - val_loss: 11.1555 - val_mean_squared_error: 189.6322\n",
      "next step\n",
      "105/105 [==============================] - 3s 20ms/step\n",
      "mae cal\n",
      "rmse cal\n",
      " ###2 fold : val mae 9.63, val rmse 12.38###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 21.6108 - mean_squared_error: 1296.2366\n",
      "Epoch 1: val_loss improved from inf to 376.34180, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_3.hdf5\n",
      "32/32 [==============================] - 24s 467ms/step - loss: 21.6108 - mean_squared_error: 1296.2366 - val_loss: 376.3418 - val_mean_squared_error: 154868.8750\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 11.3763 - mean_squared_error: 207.7018\n",
      "Epoch 2: val_loss improved from 376.34180 to 14.93443, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_3.hdf5\n",
      "32/32 [==============================] - 14s 437ms/step - loss: 11.3763 - mean_squared_error: 207.7018 - val_loss: 14.9344 - val_mean_squared_error: 392.3897\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.9248 - mean_squared_error: 189.9506\n",
      "Epoch 3: val_loss improved from 14.93443 to 12.91850, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_3.hdf5\n",
      "32/32 [==============================] - 14s 439ms/step - loss: 10.9248 - mean_squared_error: 189.9506 - val_loss: 12.9185 - val_mean_squared_error: 232.3557\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.9482 - mean_squared_error: 193.7075\n",
      "Epoch 4: val_loss did not improve from 12.91850\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 10.9482 - mean_squared_error: 193.7075 - val_loss: 18.1698 - val_mean_squared_error: 431.6687\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.3223 - mean_squared_error: 169.7018\n",
      "Epoch 5: val_loss did not improve from 12.91850\n",
      "32/32 [==============================] - 14s 434ms/step - loss: 10.3223 - mean_squared_error: 169.7018 - val_loss: 14.3406 - val_mean_squared_error: 297.4778\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.1040 - mean_squared_error: 165.7218\n",
      "Epoch 6: val_loss improved from 12.91850 to 9.19022, saving model to randomSearch/SRATE500_ECG-bandpass_z-norm_adult_male_train/CNN_4layers_4fold_test500/BRNO_same/weights_3.hdf5\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 10.1040 - mean_squared_error: 165.7218 - val_loss: 9.1902 - val_mean_squared_error: 132.3475\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.7862 - mean_squared_error: 156.9995\n",
      "Epoch 7: val_loss did not improve from 9.19022\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 9.7862 - mean_squared_error: 156.9995 - val_loss: 9.3595 - val_mean_squared_error: 136.5694\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.7414 - mean_squared_error: 185.2043\n",
      "Epoch 8: val_loss did not improve from 9.19022\n",
      "32/32 [==============================] - 14s 435ms/step - loss: 10.7414 - mean_squared_error: 185.2043 - val_loss: 12.3507 - val_mean_squared_error: 235.7708\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 10.9974 - mean_squared_error: 193.1167\n",
      "Epoch 9: val_loss did not improve from 9.19022\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 10.9974 - mean_squared_error: 193.1167 - val_loss: 10.2773 - val_mean_squared_error: 160.8263\n",
      "next step\n",
      "105/105 [==============================] - 2s 19ms/step\n",
      "mae cal\n",
      "rmse cal\n",
      " ###3 fold : val mae 9.69, val rmse 12.19###\n",
      "mae9.69+-0.25_rmse12.23+-0.22\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import MultiHeadAttention, GlobalMaxPool1D, Concatenate, LeakyReLU, AveragePooling1D\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "odir_f = 'BRNO_same'\n",
    "odir = rootdir + '/' + odir_f\n",
    "if not os.path.exists(odir):\n",
    "    os.mkdir(odir)\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# BUILD MODEL\n",
    "# ==================================================\n",
    "# input layer\n",
    "inp_sig = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "out = inp_sig\n",
    "\n",
    "# first conv1d layer\n",
    "out = Conv1D(filters=256, kernel_size=15, strides=2, padding='same', activation=None)(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = LeakyReLU(alpha=0.01)(out)\n",
    "\n",
    "# stacked residual blocks\n",
    "for i in range(1, 6):\n",
    "    res = out\n",
    "\n",
    "    out = Conv1D(filters=256, kernel_size=9, strides=2, padding='same', activation=None)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = LeakyReLU(alpha=0.01)(out)\n",
    "    out = Conv1D(filters=256, kernel_size=9, strides=1, padding='same', activation=None)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = LeakyReLU(alpha=0.01)(out)\n",
    "\n",
    "    res = AveragePooling1D(pool_size=2, padding='same')(res)\n",
    "    res = Conv1D(filters=256, kernel_size=1, strides=1, padding='same', activation=None)(res)\n",
    "\n",
    "    out = res + out\n",
    "\n",
    "# multi-head attention\n",
    "out = Dropout(0.5)(out)\n",
    "out = MultiHeadAttention(num_heads=8, key_dim=256)(out, out)\n",
    "out = GlobalMaxPool1D()(out)\n",
    "\n",
    "# concat with X_feature\n",
    "inp_feat = Input(shape=(x_feature.shape[1]))\n",
    "out = Concatenate(axis=1)([out, inp_feat])\n",
    "\n",
    "# final dense layer\n",
    "out = Dropout(0.5)(out)\n",
    "out = Dense(256, activation=None)(out)\n",
    "#out = BatchNormalization()(out)\n",
    "#out = LeakyReLU(alpha=0.01)(out)\n",
    "\n",
    "#out = Dropout(dropout_fc)(out)\n",
    "#out = Dense(dense_node, activation=None)(out)\n",
    "#out = LeakyReLU(alpha=0.01)(out)\n",
    "out = Dense(1, activation='linear')(out)\n",
    "\n",
    "# build model\n",
    "model = Model(inputs=[inp_sig, inp_feat], outputs=[out])\n",
    "model.save_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "# ==================================================\n",
    "# K FOLD CROSS VALIDATION\n",
    "# ==================================================\n",
    "kfold = KFold(n_splits=nfold)\n",
    "tprs, aucs, prs = [], [], []\n",
    "test_rmse, test_mae = [], []\n",
    "f1_scores, thvals = [], []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "mean_recall = np.linspace(0,1,100)\n",
    "\n",
    "switch = 0\n",
    "caseids_train = np.unique(caseid)\n",
    "for fold, (c_cv_trains_mask, c_cv_test_mask) in enumerate(kfold.split(caseids_train)):\n",
    "    c_cv_trains = caseids_train[c_cv_trains_mask]\n",
    "    random.shuffle(c_cv_trains)\n",
    "\n",
    "    ncv = int(len(c_cv_trains)*0.8)\n",
    "    c_cv_train = c_cv_trains[:ncv]\n",
    "    c_cv_val = c_cv_trains[ncv:]\n",
    "\n",
    "    cv_train_mask = np.isin(caseid, c_cv_train)\n",
    "    cv_val_mask = np.isin(caseid, c_cv_val)\n",
    "    cv_test_mask = np.isin(caseid, caseids_train[c_cv_test_mask])\n",
    "\n",
    "    X_train_signal = x_train[cv_train_mask]\n",
    "    X_train_feature = x_feature[cv_train_mask]\n",
    "\n",
    "    X_val_signal = x_train[cv_val_mask]\n",
    "    X_val_feature = x_feature[cv_val_mask]\n",
    "\n",
    "    X_test_signal = x_train[cv_test_mask]\n",
    "    X_test_feature = x_feature[cv_test_mask]\n",
    "\n",
    "    Y_train = y[cv_train_mask]\n",
    "    Y_val = y[cv_val_mask]\n",
    "    Y_test = y[cv_test_mask]\n",
    "\n",
    "\n",
    "    # ==================================================\n",
    "    # TRAIN MODEL\n",
    "    # ==================================================\n",
    "\n",
    "    def step_decay(epoch):\n",
    "        start = 1e-3\n",
    "        drop = 0.1\n",
    "        epochs_drop = 10\n",
    "        lr = start * (drop ** np.floor((epoch)/epochs_drop))\n",
    "        return lr\n",
    "    lr_scheduler = LearningRateScheduler(step_decay, verbose=1)\n",
    "    \n",
    "    weightcache = f\"{odir}/weights_{fold}.hdf5\"\n",
    "    model.compile(loss='mean_absolute_error', optimizer=Adam(lr=lr_scheduler, weight_decay=1e-4), metrics=['mean_squared_error'])\n",
    "    hist = model.fit([X_train_signal, X_train_feature], Y_train, validation_data=([X_val_signal, X_val_feature], Y_val), epochs=100, batch_size=256,\n",
    "                     callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                     EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "\n",
    "    print('next step')\n",
    "    model.load_weights(weightcache)\n",
    "    y_pred = model.predict([X_test_signal, X_test_feature]).flatten()\n",
    "\n",
    "    print('mae cal')\n",
    "    # MAE 계산\n",
    "\n",
    "    model_err = metrics.MeanAbsoluteError()\n",
    "    model_err.update_state(Y_test, y_pred)\n",
    "    mae_val = model_err.result().numpy()\n",
    "    test_mae.append(mae_val)\n",
    "    print('rmse cal')\n",
    "    # RMSE 계산\n",
    "    model_err = metrics.RootMeanSquaredError()\n",
    "    model_err.update_state(Y_test, y_pred)\n",
    "    rmse_val = model_err.result().numpy()\n",
    "    test_rmse.append(rmse_val)\n",
    "\n",
    "\n",
    "    print(f' ###{fold} fold : val mae {mae_val:.2f}, val rmse {rmse_val:.2f}###')\n",
    "    tf.keras.backend.clear_session()\n",
    "    model.load_weights(f'{odir}/initial_weights.hdf5')\n",
    "\n",
    "\n",
    "# RMSE 계산\n",
    "mean_rmse = np.mean(test_rmse)\n",
    "std_rmse = np.std(test_rmse)\n",
    "\n",
    "mean_mae = np.mean(test_mae)\n",
    "std_mae = np.std(test_mae)\n",
    "\n",
    "max_idx = test_mae.index(min(test_mae))\n",
    "\n",
    "\n",
    "print(f'mae{mean_mae:.2f}+-{std_mae:.2f}_rmse{mean_rmse:.2f}+-{std_rmse:.2f}')\n",
    "open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "os.rename(odir, rootdir+f'/mae{mean_mae:.2f}+-{std_mae:.2f}_rmse{mean_rmse:.2f}+-{std_rmse:.2f}_max{max_idx}__{odir_f}')\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452be6b-4fc3-4793-9745-c977431c6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    start = 1e-3\n",
    "    drop = 0.1\n",
    "    epochs_drop = 20\n",
    "    lr = start * (drop ** np.floor((epoch)/epochs_drop))\n",
    "    return lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hskim",
   "language": "python",
   "name": "hskim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
